import pandas as pd
import numpy as np
import os
import joblib
import warnings
import lightgbm as lgb
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import roc_auc_score, roc_curve, log_loss
import matplotlib.pyplot as plt
import platform

# æ·±åº¦å­¦ä¹ ç›¸å…³å¯¼å…¥æ£€æŸ¥
HAS_TORCH = False
try:
    import torch
    HAS_TORCH = True
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…PyTorchï¼Œå°†è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸å…³åŠŸèƒ½")

warnings.filterwarnings('ignore')

# ä»…åœ¨Windowsç³»ç»Ÿä¸Šè®¾ç½®ä¸­æ–‡å­—ä½“
if platform.system() == 'Windows':
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # Windows å¾®è½¯é›…é»‘
    plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

class BaseModelProcessor:
    def __init__(self, model_dir="output", config_dir="config"):
        self.model_dir = model_dir
        self.config_dir = config_dir
        self.feature_config = None
        self.category_features = None
        self.continuous_features = None
        self.train_feature_names = None
        self.lr_feature_names = None
        self.gbdt_model = None
        self.lr_model = None
        self.dl_model = None
        
    def load_feature_config(self):
        """åŠ è½½ç‰¹å¾é…ç½®"""
        try:
            # åŠ è½½ç‰¹å¾é…ç½®æ–‡ä»¶
            features_path = os.path.join(self.config_dir, "features.csv")
            self.feature_config = pd.read_csv(features_path)
            print(f"âœ… ç‰¹å¾é…ç½®å·²åŠ è½½: {features_path}")
            
            # åˆ†ç¦»ç±»åˆ«ç‰¹å¾å’Œè¿ç»­ç‰¹å¾
            self.category_features = self.feature_config[
                self.feature_config['feature_type'] == 'category'
            ]['feature_name'].tolist()
            
            self.continuous_features = self.feature_config[
                self.feature_config['feature_type'] == 'continuous'
            ]['feature_name'].tolist()
            
            # ç§»é™¤Idç‰¹å¾
            if 'Id' in self.continuous_features:
                self.continuous_features.remove('Id')
            
            print(f"ğŸ“Š ç‰¹å¾é…ç½®: {len(self.continuous_features)} ä¸ªè¿ç»­ç‰¹å¾, {len(self.category_features)} ä¸ªç±»åˆ«ç‰¹å¾")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½ç‰¹å¾é…ç½®æ—¶å‡ºé”™: {e}")
            return False
    
    def load_models(self, model_type="gbdt_lr"):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            if model_type == "gbdt_lr":
                # åŠ è½½GBDTæ¨¡å‹
                gbdt_model_path = os.path.join(self.model_dir, "gbdt_model.pkl")
                self.gbdt_model = joblib.load(gbdt_model_path)
                print(f"âœ… GBDTæ¨¡å‹å·²åŠ è½½: {gbdt_model_path}")
                
                # åŠ è½½LRæ¨¡å‹
                lr_model_path = os.path.join(self.model_dir, "lr_model.pkl")
                self.lr_model = joblib.load(lr_model_path)
                print(f"âœ… LRæ¨¡å‹å·²åŠ è½½: {lr_model_path}")
                
                # è·å–LRæ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾åç§°
                self.lr_feature_names = self.lr_model.feature_names_in_.tolist()
                print(f"âœ… LRæ¨¡å‹ç‰¹å¾åç§°å·²åŠ è½½: {len(self.lr_feature_names)} ä¸ªç‰¹å¾")
                
                # åŠ è½½è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
                train_features_path = os.path.join(self.model_dir, "train_feature_names.csv")
                train_features_df = pd.read_csv(train_features_path)
                self.train_feature_names = train_features_df['feature'].tolist()
                print(f"âœ… è®­ç»ƒæ—¶ç‰¹å¾åç§°å·²åŠ è½½: {len(self.train_feature_names)} ä¸ªç‰¹å¾")
            elif model_type == "dl":
                if not HAS_TORCH:
                    print("âŒ æœªå®‰è£…PyTorchï¼Œæ— æ³•åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹")
                    return False
                    
                # åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹ä¿¡æ¯
                model_info_path = os.path.join(self.model_dir, "dl_model_info.csv")
                if os.path.exists(model_info_path):
                    model_info = pd.read_csv(model_info_path).iloc[0]
                    self.dl_model_info = model_info.to_dict()
                    print(f"âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹ä¿¡æ¯å·²åŠ è½½: {model_info_path}")
                else:
                    print(f"âš ï¸ æ·±åº¦å­¦ä¹ æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {model_info_path}")
                    return False
                
                # åŠ è½½è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
                train_features_path = os.path.join(self.model_dir, "train_feature_names.csv")
                if os.path.exists(train_features_path):
                    train_features_df = pd.read_csv(train_features_path)
                    self.train_feature_names = train_features_df['feature'].tolist()
                    print(f"âœ… è®­ç»ƒæ—¶ç‰¹å¾åç§°å·²åŠ è½½: {len(self.train_feature_names)} ä¸ªç‰¹å¾")
                else:
                    print(f"âš ï¸ è®­ç»ƒæ—¶ç‰¹å¾åç§°æ–‡ä»¶ä¸å­˜åœ¨: {train_features_path}")
                    return False
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return False
            
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def save_models(self, gbdt_model=None, lr_model=None, category_feature=None, continuous_feature=None, 
                   dl_model=None, dl_model_info=None):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            # ä¿å­˜GBDTæ¨¡å‹
            if gbdt_model is not None:
                joblib.dump(gbdt_model, os.path.join(self.model_dir, "gbdt_model.pkl"))
            
            # ä¿å­˜LRæ¨¡å‹
            if lr_model is not None:
                joblib.dump(lr_model, os.path.join(self.model_dir, "lr_model.pkl"))
            
            # ä¿å­˜GBDT+LRæ¨¡å‹ç›¸å…³ä¿¡æ¯
            if gbdt_model is not None:
                # ä¿å­˜å¿…è¦ä¿¡æ¯ç”¨äºAPIæœåŠ¡
                pd.Series([gbdt_model.best_iteration_]).to_csv(
                    os.path.join(self.model_dir, "actual_n_estimators.csv"), 
                    index=False, 
                    header=['n_estimators']
                )
                
                pd.Series(gbdt_model.feature_name_).to_csv(
                    os.path.join(self.model_dir, "train_feature_names.csv"), 
                    index=False, 
                    header=['feature']
                )
                
                pd.Series(category_feature).to_csv(
                    os.path.join(self.model_dir, "category_features.csv"), 
                    index=False, 
                    header=['feature']
                )
                
                pd.Series(continuous_feature).to_csv(
                    os.path.join(self.model_dir, "continuous_features.csv"), 
                    index=False, 
                    header=['feature']
                )
            
            # ä¿å­˜æ·±åº¦å­¦ä¹ æ¨¡å‹
            if dl_model is not None and HAS_TORCH:
                torch.save(dl_model.state_dict(), os.path.join(self.model_dir, "dl_model_best.pth"))
            
            # ä¿å­˜æ·±åº¦å­¦ä¹ æ¨¡å‹ä¿¡æ¯
            if dl_model_info is not None:
                pd.DataFrame([dl_model_info]).to_csv(os.path.join(self.model_dir, "dl_model_info.csv"), index=False)
                print("âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜è‡³ output/dl_model_info.csv")
            
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
            
    def onehot_encode_features(self, df, columns, prefix_sep='_'):
        """å¯¹æŒ‡å®šåˆ—è¿›è¡ŒOne-Hotç¼–ç """
        encoded_dfs = []
        for col in columns:
            if col in df.columns:
                # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç 
                onehot_df = pd.get_dummies(df[col], prefix=col, prefix_sep=prefix_sep)
                encoded_dfs.append(onehot_df)
        if encoded_dfs:
            return pd.concat(encoded_dfs, axis=1)
        return pd.DataFrame()
        
    def onehot_encode_leaf_features(self, leaf_indices, n_trees):
        """å¯¹å¶å­èŠ‚ç‚¹è¿›è¡ŒOne-Hotç¼–ç """
        leaf_dummies_list = []
        for i in range(n_trees):
            leaf_col_name = f"gbdt_leaf_{i}"
            leaf_series = pd.Series(leaf_indices[:, i], name=leaf_col_name)
            dummies = pd.get_dummies(leaf_series, prefix=leaf_col_name)
            leaf_dummies_list.append(dummies)
        leaf_dummies_combined = pd.concat(leaf_dummies_list, axis=1) if leaf_dummies_list else pd.DataFrame()
        return leaf_dummies_combined
        
    def match_features(self, df, required_features, fill_value=0):
        """ç¡®ä¿æ•°æ®æ¡†åŒ…å«æ‰€éœ€çš„ç‰¹å¾"""
        # è·å–å½“å‰ç‰¹å¾
        current_features = set(df.columns)
        
        # æ£€æŸ¥ç¼ºå°‘çš„ç‰¹å¾
        missing_features = set(required_features) - current_features
        if missing_features:
            print(f"âš ï¸  æ•°æ®ç¼ºå°‘ {len(missing_features)} ä¸ªç‰¹å¾")
            # ä¸ºç¼ºå°‘çš„ç‰¹å¾æ·»åŠ é»˜è®¤å€¼
            for feature in missing_features:
                df[feature] = fill_value
        
        # ç§»é™¤å¤šä½™çš„ç‰¹å¾
        extra_features = current_features - set(required_features)
        if extra_features:
            print(f"â„¹ï¸  ç§»é™¤ {len(extra_features)} ä¸ªå¤šä½™çš„ç‰¹å¾")
            df = df.drop(columns=list(extra_features))
        
        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸è¦æ±‚ä¸€è‡´
        df = df[required_features]
        return df
        
    def show_model_interpretation_prompt(self):
        """
        æ˜¾ç¤ºæç¤ºä¿¡æ¯ï¼ŒæŒ‡å¯¼ç”¨æˆ·å¦‚ä½•å°†æ¨¡å‹è®­ç»ƒæ—¥å¿—å¤åˆ¶åˆ°å¤§æ¨¡å‹è¿›è¡Œè§£è¯»
        """
        print("\nâœ… ======================================")
        print("âœ… å°†ä¸‹é¢çš„å†…å®¹å¤åˆ¶åˆ°å¤§æ¨¡å‹å†…è¿›è¡Œè§£è¯»ï¼ˆä¸åŒ…æ‹¬æ­¤ä¸‰è¡Œï¼‰")
        print("âœ… ======================================\n")
        print("å¯¹ä»¥ä¸‹(æ¨è/æˆä¿¡/é¢„è­¦)æ¨¡å‹è®­ç»ƒæ—¥å¿—è¿›è¡Œåˆ†æï¼Œè¾“å‡ºé“¶è¡Œä¸šåŠ¡äººå‘˜å¯ä»¥ç†è§£çš„è§£è¯»æŠ¥å‘Šï¼Œç›®åœ°æ˜¯è¿›è¡Œ(æ¨è/æˆä¿¡/é¢„è­¦)ï¼Œé€šè¿‡æ¨¡å‹åˆ†æèµ‹èƒ½ä¸šåŠ¡å†³ç­–ã€‚\n")
        
    def get_leaf_path_enhanced(self, booster, tree_index, leaf_index, feature_names, category_prefixes):
        """
        è§£ææŒ‡å®šå¶å­èŠ‚ç‚¹çš„å†³ç­–è·¯å¾„ï¼Œæ”¯æŒç¿»è¯‘ one-hot ç±»åˆ«ç‰¹å¾
        """
        try:
            model_dump = booster.dump_model()
            if tree_index >= len(model_dump['tree_info']):
                return None
            tree_info = model_dump['tree_info'][tree_index]['tree_structure']
        except Exception as e:
            print(f"è·å–æ ‘ç»“æ„å¤±è´¥: {e}")
            return None

        node_stack = [(tree_info, [])]  # (å½“å‰èŠ‚ç‚¹, è·¯å¾„åˆ—è¡¨)

        while node_stack:
            node, current_path = node_stack.pop()

            # å¦‚æœæ˜¯ç›®æ ‡å¶å­èŠ‚ç‚¹
            if 'leaf_index' in node and node['leaf_index'] == leaf_index:
                return current_path

            # å¦‚æœæ˜¯åˆ†è£‚èŠ‚ç‚¹
            if 'split_feature' in node:
                feat_idx = node['split_feature']
                if feat_idx >= len(feature_names):
                    feat_name = f"Feature_{feat_idx}"
                else:
                    feat_name = feature_names[feat_idx]

                threshold = node.get('threshold', 0.0)
                decision_type = node.get('decision_type', '<=')

                # æ£€æŸ¥æ˜¯å¦ä¸º one-hot ç±»åˆ«ç‰¹å¾
                is_category = False
                original_col = None
                category_value = None

                for prefix in category_prefixes:
                    if feat_name.startswith(prefix):
                        is_category = True
                        original_col = prefix.rstrip('_')
                        category_value = feat_name[len(prefix):]
                        break

                if is_category:
                    # ç±»åˆ«ç‰¹å¾é€šå¸¸ç”¨ > 0.5 åˆ¤æ–­æ˜¯å¦æ¿€æ´»
                    # å‡è®¾å³å­æ ‘æ˜¯"ç­‰äºè¯¥ç±»åˆ«"
                    right_rule = f"{original_col} == '{category_value}'"
                    left_rule = f"{original_col} != '{category_value}'"
                else:
                    # è¿ç»­ç‰¹å¾
                    if decision_type == '<=' or decision_type == 'no_greater':
                        right_rule = f"{feat_name} > {threshold:.4f}"
                        left_rule = f"{feat_name} <= {threshold:.4f}"
                    else:
                        right_rule = f"{feat_name} {decision_type} {threshold:.4f}"
                        left_rule = f"{feat_name} not {decision_type} {threshold:.4f}"

                # æ·»åŠ å·¦å³å­æ ‘åˆ°æ ˆ
                if 'right_child' in node:
                    node_stack.append((node['right_child'], current_path + [right_rule]))
                if 'left_child' in node:
                    node_stack.append((node['left_child'], current_path + [left_rule]))

        return None  # æœªæ‰¾åˆ°è·¯å¾„
        
    def calculate_ks_statistic(self, y_true, y_pred_prob):
        """
        è®¡ç®— KS ç»Ÿè®¡é‡
        """
        # å°†æ ·æœ¬æŒ‰é¢„æµ‹æ¦‚ç‡æ’åº
        data = pd.DataFrame({'y_true': y_true, 'y_pred_prob': y_pred_prob})
        data_sorted = data.sort_values('y_pred_prob', ascending=False)
        
        # è®¡ç®—ç´¯ç§¯åˆ†å¸ƒ
        cum_positive = (data_sorted['y_true'] == 1).cumsum() / (y_true == 1).sum()
        cum_negative = (data_sorted['y_true'] == 0).cumsum() / (y_true == 0).sum()
        
        # KSç»Ÿè®¡é‡æ˜¯ä¸¤ä¸ªç´¯ç§¯åˆ†å¸ƒä¹‹é—´çš„æœ€å¤§å·®å¼‚
        ks_stat = np.max(np.abs(cum_positive - cum_negative))
        return ks_stat
        
    def plot_roc_curve(self, y_true, y_pred_prob, output_path="output/roc_curve.png"):
        """
        ç»˜åˆ¶ ROC æ›²çº¿
        """
        fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
        auc = roc_auc_score(y_true, y_pred_prob)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC)')
        plt.legend(loc="lower right")
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… ROCæ›²çº¿å·²ä¿å­˜è‡³ {output_path}")
        return auc
        
    def analyze_feature_importance(self, booster, feature_names):
        """
        åˆ†æ GBDT ç‰¹å¾é‡è¦æ€§ï¼ˆå«å½±å“æ–¹å‘ï¼‰
        """
        # è·å– Gain ç±»å‹çš„é‡è¦æ€§ï¼ˆæ›´å‡†ç¡®åæ˜ ç‰¹å¾å½±å“ï¼‰
        gain_importance = booster.feature_importance(importance_type='gain')
        # è·å– Split ç±»å‹çš„é‡è¦æ€§ï¼ˆç‰¹å¾è¢«ç”¨äºåˆ†è£‚çš„æ¬¡æ•°ï¼‰
        split_importance = booster.feature_importance(importance_type='split')
        
        feat_imp = pd.DataFrame({
            'Feature': feature_names,
            'Gain_Importance': gain_importance,
            'Split_Importance': split_importance
        }).sort_values('Gain_Importance', ascending=False)
        
        # é€šè¿‡LightGBMå†…ç½®åŠŸèƒ½åˆ†æç‰¹å¾å½±å“æ–¹å‘
        try:
            # è¿™é‡Œéœ€è¦è®­ç»ƒæ•°æ®æ¥è®¡ç®—ç‰¹å¾è´¡çŒ®å€¼ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒæ—¶è°ƒç”¨
            # æ­¤å¤„ä»…è¿”å›åŸºæœ¬çš„ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
            feat_imp['Impact_Direction'] = 'Unknown'
            
        except Exception as e:
            # å¦‚æœåˆ†æå¤±è´¥ï¼Œä»ä¿ç•™åŸºæœ¬çš„ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
            feat_imp['Impact_Direction'] = 'Unknown'
            
        return feat_imp

    def plot_training_curves(self, train_losses, val_losses, train_aucs, val_aucs, output_path="output/training_curves.png"):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆæŸå¤±å’ŒAUCï¼‰
        """
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')
        plt.legend()
        
        plt.subplot(1, 2, 2)
        plt.plot(train_aucs, label='Train AUC')
        plt.plot(val_aucs, label='Validation AUC')
        plt.xlabel('Epoch')
        plt.ylabel('AUC')
        plt.title('Training and Validation AUC')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ {output_path}")

    def analyze_dl_feature_importance(self, model, x_val_tensor, x_train_columns, output_path="output/dl_feature_importance.csv"):
        """
        åˆ†ææ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        """
        # å®Œå…¨ç§»é™¤æ·±åº¦å­¦ä¹ ç›¸å…³åŠŸèƒ½ï¼Œé¿å…åœ¨æ²¡æœ‰PyTorchæ—¶å‡ºç°é”™è¯¯
        print("âŒ æœªå®‰è£…PyTorchï¼Œå·²ç¦ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸å…³åŠŸèƒ½")
        return None

import pandas as pd
import os
import glob
import re
from collections import defaultdict
import psutil
import gc
from abc import ABC, abstractmethod

# ä¸ºé¿å…å¾ªç¯å¯¼å…¥ï¼Œä½¿ç”¨å­—ç¬¦ä¸²æ£€æŸ¥è€Œä¸æ˜¯ç›´æ¥å¯¼å…¥
# from convert_train_data import TrainDataProcessor
# å¯¼å…¥ç¹ç®€ä½“è½¬æ¢åŒ…
try:
    from opencc import OpenCC
    # åˆ›å»ºè½¬æ¢å™¨å®ä¾‹
    cc_s2t = OpenCC('s2t')  # ç®€ä½“åˆ°ç¹ä½“
    cc_t2s = OpenCC('t2s')  # ç¹ä½“åˆ°ç®€ä½“
    HAS_OPENCC = True
except ImportError:
    HAS_OPENCC = False
    print("è­¦å‘Š: æœªå®‰è£…opencc-python-reimplementedåŒ…ï¼Œå°†è·³è¿‡ç¹ç®€ä½“è½¬æ¢å¤„ç†")

# æ•æ„Ÿå­—æ®µå…³é”®è¯åˆ—è¡¨ï¼ˆè€ƒè™‘ç¹ç®€ä½“å’Œè‹±æ–‡ï¼‰
# æ³¨æ„ï¼šå®¢æˆ·ç¼–å·ï¼ˆcusno, CUSNO, ci, CIï¼‰æ˜¯è®­ç»ƒæ—¶å¿…é¡»çš„ä¸»é”®ï¼Œä¸æ˜¯æ•æ„Ÿå­—æ®µ
SENSITIVE_FIELD_KEYWORDS = [
    # å®¢æˆ·ç›¸å…³ï¼ˆæ’é™¤å®¢æˆ·ç¼–å·ï¼‰
    "å®¢æˆ·åç§°", "å®¢æˆ¶åç¨±", "å®¢æˆ·å§“å", "å®¢æˆ¶å§“å", "å®¢æˆ·å·", "å®¢æˆ¶è™Ÿ",
    "customer name", "client name",
    # èº«ä»½ç›¸å…³
    "èº«ä»½è¯", "èº«åˆ†è­‰", "èº«åˆ†è¯è™Ÿç¢¼", "èº«ä»½è¯å·ç ", "èº«åˆ†è­‰å­—è™Ÿ", "èº«ä»½è¯å­—è™Ÿ",
    "id card", "identity card", "identification number", "id number",
    # è”ç³»ä¿¡æ¯
    "æ‰‹æœºå·", "æ‰‹æ©Ÿè™Ÿ", "æ‰‹æœºå·ç ", "æ‰‹æ©Ÿè™Ÿç¢¼", "ç”µè¯", "é›»è©±", "ç”µè¯å·ç ", "é›»è©±è™Ÿç¢¼",
    "mobile", "mobile number", "phone", "phone number", "telephone", "telephone number",
    # åœ°å€ç›¸å…³
    "åœ°å€", "åœ°å€è³‡è¨Š", "åœ°å€ä¿¡æ¯", "ä½å€",
    "address", "residential address", "home address",
    # è´¦æˆ·ç›¸å…³
    "è´¦å·", "å¸³è™Ÿ", "é“¶è¡Œè´¦å·", "éŠ€è¡Œå¸³è™Ÿ", "è´¦æˆ·", "å¸³æˆ¶",
    "account", "account number", "bank account", "bank account number"
]

# ä¸»é”®å­—æ®µåˆ—è¡¨ï¼ˆä¸æ˜¯æ•æ„Ÿå­—æ®µï¼‰
PRIMARY_KEY_FIELDS = ["cusno", "CUSNO", "ci", "CI", "å®¢æˆ·ç¼–å·", "å®¢æˆ¶ç·¨è™Ÿ", "customer id", "client id"]

class BaseDataProcessor(ABC):
    def __init__(self, input_dir, output_dir):
        self.input_dir = input_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
    def normalize_name(self, name):
        """
        æ ‡å‡†åŒ–åç§°ï¼Œå°†ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        """
        return re.sub(r'[^\w]', '_', name)

    def _normalize_name(self, name):
        """
        æ ‡å‡†åŒ–åç§°ï¼Œå°†ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        """
        return re.sub(r'[^\w]', '_', name)

    def check_sensitive_fields(self, df, file_name):
        """
        æ£€æŸ¥DataFrameä¸­çš„æ•æ„Ÿå­—æ®µï¼Œæ£€æµ‹åˆ°æ•æ„Ÿå­—æ®µæ—¶ç«‹å³é€€å‡ºç¨‹åº
        æ³¨æ„ï¼šå®¢æˆ·ç¼–å·ï¼ˆcusno, CUSNO, ci, CIï¼‰æ˜¯è®­ç»ƒæ—¶å¿…é¡»çš„ä¸»é”®ï¼Œä¸æ˜¯æ•æ„Ÿå­—æ®µ
        
        å‚æ•°:
        df: DataFrame
        file_name: æ–‡ä»¶å
        
        è¿”å›:
        list: æ•æ„Ÿå­—æ®µåˆ—è¡¨
        """
        sensitive_fields = []
        
        for col in df.columns:
            col_str = str(col).lower() if isinstance(col, str) else str(col)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸»é”®å­—æ®µ
            is_primary_key = False
            for pk_field in PRIMARY_KEY_FIELDS:
                pk_field_lower = pk_field.lower()
                if pk_field_lower == col_str or pk_field_lower in col_str:
                    is_primary_key = True
                    break
                # æ£€æŸ¥ç¹ç®€ä½“æƒ…å†µ
                if self._contains_chinese_keyword(col_str, pk_field_lower):
                    is_primary_key = True
                    break
            
            # å¦‚æœæ˜¯ä¸»é”®å­—æ®µï¼Œè·³è¿‡æ•æ„Ÿå­—æ®µæ£€æŸ¥
            if is_primary_key:
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿå…³é”®è¯
            for keyword in SENSITIVE_FIELD_KEYWORDS:
                # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
                keyword_lower = keyword.lower()
                
                # å®Œå…¨åŒ¹é…æˆ–åŒ…å«åŒ¹é…
                if keyword_lower == col_str or keyword_lower in col_str:
                    sensitive_fields.append(col)
                    break
                
                # å¤„ç†ç¹ç®€ä½“è½¬æ¢çš„ç‰¹æ®Šæƒ…å†µ
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯çš„ä¸€éƒ¨åˆ†
                if self._contains_chinese_keyword(col_str, keyword_lower):
                    sensitive_fields.append(col)
                    break
        
        if sensitive_fields:
            print(f"âš ï¸  åœ¨æ–‡ä»¶ {file_name} ä¸­æ£€æµ‹åˆ°æ•æ„Ÿå­—æ®µ: {sensitive_fields}")
            print("âŒ ç¨‹åºå·²é€€å‡ºï¼Œä»¥é˜²æ­¢æ•æ„Ÿæ•°æ®æ³„éœ²ã€‚")
            exit(1)
        
        return sensitive_fields

    def _contains_chinese_keyword(self, col_name, keyword):
        """
        æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«ä¸­æ–‡å…³é”®è¯ï¼ˆä½¿ç”¨ä¸“ä¸šçš„ç¹ç®€ä½“è½¬æ¢åŒ…ï¼‰
        
        å‚æ•°:
        col_name: åˆ—å
        keyword: å…³é”®è¯
        
        è¿”å›:
        bool: æ˜¯å¦åŒ…å«å…³é”®è¯
        """
        # ä½¿ç”¨ä¸“ä¸šçš„ç¹ç®€ä½“è½¬æ¢åŒ…è¿›è¡Œæ£€æŸ¥
        if HAS_OPENCC:
            # ç›´æ¥æ£€æŸ¥åŸå§‹å…³é”®è¯
            if keyword in col_name:
                return True
            
            # ç®€ä½“è½¬ç¹ä½“
            s2t_keyword = cc_s2t.convert(keyword)
            if s2t_keyword in col_name:
                return True
            
            # ç¹ä½“è½¬ç®€ä½“
            t2s_keyword = cc_t2s.convert(keyword)
            if t2s_keyword in col_name:
                return True
        else:
            # å¦‚æœæ²¡æœ‰å®‰è£…openccï¼Œåˆ™åªè¿›è¡ŒåŸºç¡€çš„æ–‡æœ¬åŒ¹é…
            if keyword in col_name:
                return True
        
        return False

    def read_config_file(self, file_path, required_columns):
        """
        è¯»å–é…ç½®æ–‡ä»¶çš„é€šç”¨å‡½æ•°
        
        å‚æ•°:
        file_path: é…ç½®æ–‡ä»¶è·¯å¾„
        required_columns: å¿…éœ€çš„åˆ—ååˆ—è¡¨
        
        è¿”å›:
        DataFrame: è¯»å–çš„é…ç½®æ•°æ®
        """
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path, dtype=str)
                # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
                missing_columns = [col for col in required_columns if col not in df.columns]
                if missing_columns:
                    print(f"é…ç½®æ–‡ä»¶ {file_path} ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
                    return pd.DataFrame()
                return df
            except Exception as e:
                print(f"è¯»å–é…ç½®æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return pd.DataFrame()

    def get_primary_key_mapping(self, config_dir="config"):
        """
        è¯»å–config/primary_key.csvæ–‡ä»¶ï¼Œè·å–ä¸»é”®æ˜ å°„
        """
        primary_key_file = os.path.join(config_dir, "primary_key.csv")
        mapping = {}
        
        df_pk = self.read_config_file(primary_key_file, ['file_name', 'primary_key'])
        if not df_pk.empty:
            for _, row in df_pk.iterrows():
                file = str(row.get('file_name', '')).strip()
                sheet = str(row.get('sheet_name', '')).strip() if 'sheet_name' in row else ''
                pk = str(row.get('primary_key', '')).strip()
                if file and pk:
                    mapping[(file, sheet)] = pk
        return mapping

    def get_category_feature_mapping(self, config_dir="config"):
        """
        è¯»å–config/category_type.csvæ–‡ä»¶ï¼Œè·å–éœ€è¦å¼ºåˆ¶ä½œä¸ºç±»åˆ«ç‰¹å¾çš„å­—æ®µæ˜ å°„
        """
        category_type_file = os.path.join(config_dir, "category_type.csv")
        mapping = {}
        
        df_category = self.read_config_file(category_type_file, ['file_name', 'column_name', 'feature_type'])
        if not df_category.empty:
            for _, row in df_category.iterrows():
                file = str(row.get('file_name', '')).strip()
                column = str(row.get('column_name', '')).strip()
                feature_type = str(row.get('feature_type', '')).strip()
                if file and column and feature_type:
                    # ä½¿ç”¨æ–‡ä»¶åå’Œåˆ—åä½œä¸ºé”®
                    mapping[(file, column)] = feature_type
        return mapping

    def auto_detect_key(self, df):
        primary_key_candidates = ['cusno', 'ci', 'å®¢æˆ·ç¼–å·', 'å®¢æˆ·å·']
        for col in df.columns:
            if any(candidate in col.lower() for candidate in primary_key_candidates):
                return col
        return None

    def determine_feature_type(self, col, wide_df, category_feature_mapping, file_name=None, orig_col_name=None):
        """
        ç¡®å®šç‰¹å¾ç±»å‹
        
        å‚æ•°:
        col: åˆ—å
        wide_df: æ•°æ®æ¡†
        category_feature_mapping: ç±»åˆ«ç‰¹å¾æ˜ å°„
        file_name: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        orig_col_name: åŸå§‹åˆ—åï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
        feature_type: ç‰¹å¾ç±»å‹ ('continuous', 'category', 'text', 'other')
        """
        # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨category_type.csvä¸­è¢«æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾
        is_forced_category = False
        
        # å¦‚æœæä¾›äº†file_nameå’Œorig_col_nameï¼Œç›´æ¥æ£€æŸ¥æ˜¯å¦åœ¨category_feature_mappingä¸­
        if file_name and orig_col_name and (file_name, orig_col_name) in category_feature_mapping:
            is_forced_category = True
        
        # éå†category_feature_mappingï¼Œæ£€æŸ¥åˆ—åæ˜¯å¦åŒ¹é…
        if not is_forced_category:
            for (f_name, column_name), feature_type in category_feature_mapping.items():
                # ç”Ÿæˆå¯èƒ½çš„å‰ç¼€
                safe_prefix = self.normalize_name(f_name.replace('.xlsx', ''))
                
                # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆä¸å¸¦å‰ç¼€çš„åŸå§‹åˆ—åï¼‰
                if col == column_name and feature_type == 'category':
                    is_forced_category = True
                    break
                
                # æ£€æŸ¥æ˜¯å¦å¸¦å‰ç¼€åŒ¹é…
                if col.startswith(f"{safe_prefix}_{column_name.lower()}") and feature_type == 'category':
                    is_forced_category = True
                    break
                
                # ç‰¹æ®Šå¤„ç†ï¼šå¤„ç†åˆ—åä¸­åŒ…å«ä¸‹åˆ’çº¿çš„æƒ…å†µ
                if feature_type == 'category':
                    # å°†åŸå§‹åˆ—åè½¬æ¢ä¸ºå°å†™å¹¶æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
                    normalized_column_name = self.normalize_name(column_name.lower())
                    if col.startswith(f"{safe_prefix}_{normalized_column_name}") or col.startswith(f"{safe_prefix}_{column_name.lower()}"):
                        is_forced_category = True
                        break
        
        if is_forced_category:
            # å¼ºåˆ¶ä½œä¸ºç±»åˆ«ç‰¹å¾
            return 'category'
        elif pd.api.types.is_numeric_dtype(wide_df[col]):
            return 'continuous'
        elif pd.api.types.is_string_dtype(wide_df[col]) or pd.api.types.is_object_dtype(wide_df[col]):
            if wide_df[col].nunique() <= 10:
                return 'category'
            else:
                return 'text'
        else:
            return 'other'

    def get_topk_by_coverage(self, value_counts, coverage_threshold=0.95, max_top_k=50):
        total = value_counts.sum()
        if total == 0:
            return [], 0.0, "no_data"
        cumulative = value_counts.cumsum() / total
        topk_idx = cumulative[cumulative <= coverage_threshold].index.tolist()
        if len(topk_idx) < len(value_counts):
            topk_idx.append(cumulative.index[len(topk_idx)])
        if len(topk_idx) > max_top_k:
            topk_idx = topk_idx[:max_top_k]
        
        # è®¡ç®—å®é™…è¦†ç›–ç‡
        if topk_idx:
            actual_coverage = cumulative[topk_idx[-1]]
        else:
            actual_coverage = 0.0
            
        # ç¡®å®šä¸¢å¼ƒåŸå› 
        discard_reason = "within_threshold"  # é»˜è®¤åœ¨é˜ˆå€¼å†…
        if len(topk_idx) >= max_top_k and max_top_k < len(value_counts):
            discard_reason = "exceeds_max_top_k"
        elif len(topk_idx) < len(value_counts) and cumulative.iloc[-1] > coverage_threshold:
            discard_reason = "below_coverage_threshold"
            
        return topk_idx, actual_coverage, discard_reason

    def calculate_derived_features(self, wide_df):
        if wide_df.empty:
            return wide_df
        numeric_cols = [col for col in wide_df.columns if pd.api.types.is_numeric_dtype(wide_df[col])]
        exclude_cols = ['ci', 'cusno', 'index', 'Id']
        numeric_cols = [col for col in numeric_cols if col.lower() not in [x.lower() for x in exclude_cols]]
        max_feature_cols = 500
        if len(numeric_cols) > max_feature_cols:
            numeric_cols = numeric_cols[:max_feature_cols]
        new_features = {}
        if len(numeric_cols) > 0:
            new_features['total_sum'] = wide_df[numeric_cols].sum(axis=1)
        if len(numeric_cols) > 1:
            new_features['total_mean'] = wide_df[numeric_cols].mean(axis=1)
            new_features['total_std'] = wide_df[numeric_cols].std(axis=1)
            new_features['total_max'] = wide_df[numeric_cols].max(axis=1)
            new_features['total_min'] = wide_df[numeric_cols].min(axis=1)
        if new_features:
            new_features_df = pd.DataFrame(new_features, index=wide_df.index)
            wide_df = pd.concat([wide_df, new_features_df], axis=1)
        return wide_df
        
    def process_all_excel_files(self):
        excel_files = glob.glob(os.path.join(self.input_dir, "*.xlsx"))
        print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
        all_data = defaultdict(list)
        all_primary_keys_set = set(['__primary_key__', 'id', 'cusno', 'ci', 'index'])

        primary_key_mapping = self.get_primary_key_mapping()
        category_feature_mapping = self.get_category_feature_mapping()

        for file_path in excel_files:
            file_name = os.path.basename(file_path)
            print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_name}")
            try:
                excel_data = pd.read_excel(file_path, sheet_name=None)
                for sheet_name, df in excel_data.items():
                    # åˆ—åè½¬å°å†™å¹¶å»ç©ºæ ¼
                    df.columns = [col.strip().lower() if isinstance(col, str) else col for col in df.columns]

                    # æ£€æŸ¥æ•æ„Ÿå­—æ®µ
                    sensitive_fields = self.check_sensitive_fields(df, file_name)

                    # === å…³é”®ï¼šä¸å†æ·»åŠ  source_file / sheet_name ===
                    safe_prefix = self.normalize_name(file_name.replace('.xlsx', ''))
                    new_columns = []
                    for col in df.columns:
                        if col == '__primary_key__':
                            new_columns.append(col)
                        else:
                            new_columns.append(f"{safe_prefix}_{col}")
                    df.columns = new_columns

                    # ç¡®å®šä¸»é”®
                    pk = None
                    if (file_name, sheet_name) in primary_key_mapping:
                        pk = primary_key_mapping[(file_name, sheet_name)].strip()
                    elif (file_name, '') in primary_key_mapping:
                        pk = primary_key_mapping[(file_name, '')].strip()
                    elif self.auto_detect_key(df):
                        pk = self.auto_detect_key(df)
                    else:
                        pk = 'index'
                        df[pk] = df.index.astype(str)

                    pk_lower = pk.lower()
                    if pk_lower in df.columns:
                        df['__primary_key__'] = df[pk_lower]
                    else:
                        df['__primary_key__'] = df[pk]

                    print(f"  ä½¿ç”¨ä¸»é”®: {pk}")
                    all_primary_keys_set.add(pk_lower)
                    all_data[file_name].append(df)

            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        
        return all_data, [k.lower() for k in all_primary_keys_set], category_feature_mapping

    def analyze_fields_and_dimensions(self, all_data, category_feature_mapping):
        """
        åˆ†æå­—æ®µå’Œç»´åº¦ï¼Œè€ƒè™‘ä»category_type.csvä¸­è¯»å–çš„ç±»åˆ«ç‰¹å¾
        """
        field_analysis = {}
        dimension_analysis = {}
        for file_name, dataframes in all_data.items():
            for df in dataframes:
                # è·å–è¯¥æ–‡ä»¶çš„å®‰å…¨å‰ç¼€
                safe_prefix = self.normalize_name(file_name.replace('.xlsx', ''))
                
                for col in df.columns:
                    if col not in field_analysis:
                        field_analysis[col] = {
                            'data_types': set(),
                            'files': set(),
                            'sample_values': set()
                        }
                    field_analysis[col]['data_types'].add(str(df[col].dtype))
                    field_analysis[col]['files'].add(file_name)
                    sample_vals = df[col].dropna().unique()[:5]
                    field_analysis[col]['sample_values'].update(sample_vals)
                
                for col in df.columns:
                    # === ä¸å†æ’é™¤ source_file/sheet_nameï¼ˆå› ä¸ºå®ƒä»¬ä¸å­˜åœ¨ï¼‰===
                    # æå–åŸå§‹åˆ—åï¼ˆå»é™¤å‰ç¼€ï¼‰
                    orig_col_name = col
                    if col.startswith(safe_prefix + '_'):
                        orig_col_name = col[len(safe_prefix) + 1:]
                    
                    # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨category_type.csvä¸­è¢«æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾
                    is_forced_category = False
                    # æ£€æŸ¥æ˜¯å¦åœ¨category_feature_mappingä¸­
                    if (file_name, orig_col_name) in category_feature_mapping:
                        is_forced_category = True
                    
                    # å¦‚æœæ˜¯å¼ºåˆ¶ç±»åˆ«ç‰¹å¾ï¼Œæˆ–è€…åŸæœ¬å°±æ˜¯å­—ç¬¦ä¸²/å¯¹è±¡ç±»å‹ä¸”ä¸æ˜¯ä¸»é”®ï¼Œåˆ™ä½œä¸ºç»´åº¦å¤„ç†
                    if (is_forced_category or 
                        ((pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col])) 
                         and col != '__primary_key__')):
                        if col not in dimension_analysis:
                            dimension_analysis[col] = {
                                'files': set(),
                                'values': set()
                            }
                        dimension_analysis[col]['files'].add(file_name)
                        unique_vals = df[col].dropna().unique()
                        dimension_analysis[col]['values'].update(unique_vals[:100])
        return field_analysis, dimension_analysis

    def create_wide_table_per_file(self, all_data, dimension_analysis, all_primary_keys, category_feature_mapping, coverage_threshold=0.95, max_top_k=50):
        file_wide_tables = {}

        for file_name, dataframes in all_data.items():
            print(f"\n=== å¤„ç†æ–‡ä»¶å®½è¡¨: {file_name} ===")
            file_wide_dfs = []
            category_features = []  # ç”¨äºå­˜å‚¨ç±»åˆ«å‹ç‰¹å¾

            safe_prefix = self.normalize_name(file_name.replace('.xlsx', ''))

            def strip_file_prefix(full_col):
                expected_start = safe_prefix + '_'
                if full_col.startswith(expected_start):
                    return full_col[len(expected_start):]
                return full_col

            for df in dataframes:
                primary_key = '__primary_key__'
                if primary_key not in df.columns:
                    df[primary_key] = df.index.astype(str)

                # ç¡®å®šæ•°å€¼å‹åˆ—å’Œç»´åº¦åˆ—
                numeric_cols = []
                dimension_cols = []
                
                for col in df.columns:
                    if col == primary_key or col.lower() in all_primary_keys:
                        continue
                    
                    # æå–åŸå§‹åˆ—åï¼ˆå»é™¤å‰ç¼€ï¼‰
                    orig_col_name = col
                    if col.startswith(safe_prefix + '_'):
                        orig_col_name = col[len(safe_prefix) + 1:]
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨category_type.csvä¸­è¢«æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾
                    is_forced_category = (file_name, orig_col_name) in category_feature_mapping
                    
                    # å¦‚æœæ˜¯å¼ºåˆ¶ç±»åˆ«ç‰¹å¾ï¼Œåˆ™åŠ å…¥ç»´åº¦åˆ—
                    if is_forced_category:
                        dimension_cols.append(col)
                    # å¦‚æœæ˜¯æ•°å€¼å‹ä¸”æœªè¢«å¼ºåˆ¶æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾ï¼Œåˆ™åŠ å…¥æ•°å€¼åˆ—
                    elif pd.api.types.is_numeric_dtype(df[col]):
                        numeric_cols.append(col)
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²/å¯¹è±¡ç±»å‹ä¸”æœªè¢«å¼ºåˆ¶æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾ï¼Œåˆ™åŠ å…¥ç»´åº¦åˆ—
                    elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):
                        dimension_cols.append(col)

                # ä¿å­˜åŸå§‹çš„ç±»åˆ«å‹ç‰¹å¾åˆ—
                for dim_col in dimension_cols:
                    if dim_col in dimension_analysis:
                        value_counts = df[dim_col].value_counts()
                        topk_values, actual_coverage, discard_reason = self.get_topk_by_coverage(value_counts, coverage_threshold=coverage_threshold, max_top_k=max_top_k)
                        df[dim_col] = df[dim_col].where(df[dim_col].isin(topk_values), other='other')
                        
                        # ä¿å­˜ç±»åˆ«å‹ç‰¹å¾åˆ—ï¼Œç”¨äºåç»­åˆå¹¶
                        category_features.append(dim_col)
                        
                        # è®°å½•åŒ¹é…ç‡å’Œä¸¢å¼ƒåŸå› 
                        print(f"  ç»´åº¦ {dim_col} åŒ¹é…ç‡: {actual_coverage:.2%}, ä¸¢å¼ƒåŸå› : {discard_reason}")

                        for numeric_col in numeric_cols:
                            try:
                                max_rows = 50000
                                df_subset = df.iloc[:max_rows] if len(df) > max_rows else df

                                pivot = df_subset.pivot_table(
                                    index=primary_key,
                                    columns=dim_col,
                                    values=numeric_col,
                                    aggfunc='sum',
                                    fill_value=0
                                )

                                orig_numeric = strip_file_prefix(numeric_col)
                                orig_dim = strip_file_prefix(dim_col)
                                pivot.columns = [
                                    f"{safe_prefix}_{orig_numeric}_{orig_dim}_{col}"
                                    for col in pivot.columns
                                ]
                                pivot = pivot.reset_index()

                                # === å…³é”®ï¼šä¸å†æ·»åŠ  source_file / dimension / value_field ===
                                file_wide_dfs.append(pivot)

                                process = psutil.Process(os.getpid())
                                memory_info = process.memory_info()
                                if memory_info.rss / psutil.virtual_memory().total > 0.7:
                                    gc.collect()
                                    print("  æ‰§è¡Œåƒåœ¾å›æ”¶")

                                del pivot, df_subset
                            except Exception as e:
                                print(f"  è­¦å‘Šï¼šé€è§†å¤±è´¥ ({dim_col}, {numeric_col}): {e}")
                    else:
                        print(f"  è·³è¿‡æœªåˆ†æç»´åº¦: {dim_col}")

            if not file_wide_dfs and not category_features:
                print(f"  âš ï¸ æ–‡ä»¶ {file_name} æœªç”Ÿæˆä»»ä½•é€è§†è¡¨")
                continue

            final_df = None
            
            # å¦‚æœæœ‰é€è§†è¡¨æ•°æ®ï¼Œåˆ™å¤„ç†é€è§†è¡¨
            if file_wide_dfs:
                print(f"  åˆå¹¶ {len(file_wide_dfs)} ä¸ªé€è§†è¡¨...")
                merged_df = pd.concat(file_wide_dfs, axis=0, sort=False)
                del file_wide_dfs
                gc.collect()

                numeric_columns = [col for col in merged_df.columns if pd.api.types.is_numeric_dtype(merged_df[col])]
                other_columns = [col for col in merged_df.columns if col not in numeric_columns]

                max_agg_cols = 1000
                if len(numeric_columns) > max_agg_cols:
                    numeric_columns = numeric_columns[:max_agg_cols]

                agg_dict = {}
                for col in numeric_columns:
                    if col != '__primary_key__':
                        agg_dict[col] = 'sum'
                for col in other_columns:
                    if col != '__primary_key__' and len(agg_dict) < max_agg_cols:
                        agg_dict[col] = 'first'

                final_df = merged_df.groupby('__primary_key__').agg(agg_dict).reset_index()
                del merged_df
                gc.collect()

                numeric_cols_final = [col for col in final_df.columns if pd.api.types.is_numeric_dtype(final_df[col])]
                other_cols_final = [col for col in final_df.columns if col not in numeric_cols_final]

                final_df[numeric_cols_final] = final_df[numeric_cols_final].fillna(0)
                for col in other_cols_final:
                    if col != '__primary_key__':
                        final_df[col] = final_df[col].fillna('Unknown')
            else:
                # å¦‚æœæ²¡æœ‰é€è§†è¡¨æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªåªåŒ…å«ä¸»é”®çš„DataFrame
                all_dfs = dataframes
                if all_dfs:
                    first_df = all_dfs[0]
                    final_df = pd.DataFrame({'__primary_key__': first_df['__primary_key__'].unique()})
            
            # æ·»åŠ åŸå§‹çš„ç±»åˆ«å‹ç‰¹å¾åˆ—ï¼ˆè½¬æ¢ä¸ºæ•°å€¼å‹ï¼‰
            if category_features and dataframes:
                # è·å–æ‰€æœ‰æ•°æ®å¸§ä¸­çš„ç±»åˆ«ç‰¹å¾å¹¶åˆå¹¶
                all_category_data = []
                for df in dataframes:
                    cols_to_include = ['__primary_key__'] + [col for col in category_features if col in df.columns]
                    if cols_to_include:
                        # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œå»é‡å’Œèšåˆ
                        category_df = df[cols_to_include].copy()
                        all_category_data.append(category_df)
                
                if all_category_data:
                    # åˆå¹¶æ‰€æœ‰ç±»åˆ«ç‰¹å¾æ•°æ®
                    combined_category_df = pd.concat(all_category_data, axis=0, sort=False)
                    
                    # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œèšåˆï¼ˆå–ç¬¬ä¸€ä¸ªå€¼ï¼‰
                    category_agg_dict = {}
                    for col in combined_category_df.columns:
                        if col != '__primary_key__':
                            category_agg_dict[col] = 'first'
                    
                    if category_agg_dict:
                        unique_category_df = combined_category_df.groupby('__primary_key__').agg(category_agg_dict).reset_index()
                        
                        # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç 
                        encoded_dfs = []
                        for col in category_agg_dict.keys():
                            if col in unique_category_df.columns:
                                # å¡«å……ç¼ºå¤±å€¼
                                unique_category_df[col] = unique_category_df[col].fillna('Unknown')
                                # One-Hotç¼–ç 
                                onehot_df = pd.get_dummies(unique_category_df[col], prefix=col)
                                # æ·»åŠ ä¸»é”®åˆ—
                                onehot_df['__primary_key__'] = unique_category_df['__primary_key__'].values
                                encoded_dfs.append(onehot_df)
                        
                        # åˆå¹¶æ‰€æœ‰One-Hotç¼–ç çš„ç‰¹å¾
                        if encoded_dfs:
                            combined_encoded_df = encoded_dfs[0]
                            for encoded_df in encoded_dfs[1:]:
                                combined_encoded_df = pd.merge(combined_encoded_df, encoded_df, on='__primary_key__', how='outer')
                            
                            # å°†ç¼–ç åçš„ç‰¹å¾åˆå¹¶åˆ°æœ€ç»ˆDataFrameä¸­
                            if final_df is not None:
                                final_df = pd.merge(final_df, combined_encoded_df, on='__primary_key__', how='left')
                            else:
                                final_df = combined_encoded_df

            if final_df is None:
                print(f"  âš ï¸ æ–‡ä»¶ {file_name} æ— æ³•ç”Ÿæˆå®½è¡¨")
                continue

            # å¡«å……ç±»åˆ«ç‰¹å¾çš„ç¼ºå¤±å€¼
            category_cols = [col for col in final_df.columns 
                            if (pd.api.types.is_string_dtype(final_df[col]) or pd.api.types.is_object_dtype(final_df[col]))
                            and col != '__primary_key__']
            for col in category_cols:
                final_df[col] = final_df[col].fillna('Unknown')

            if '__primary_key__' in final_df.columns:
                final_df.rename(columns={'__primary_key__': 'Id'}, inplace=True)

            # === ä¸å†éœ€è¦åˆ é™¤å…ƒå­—æ®µï¼Œå› ä¸ºä»æœªæ·»åŠ è¿‡ ===
            file_wide_tables[file_name] = final_df
            print(f"  âœ… å®Œæˆæ–‡ä»¶ {file_name}ï¼Œå®½è¡¨å½¢çŠ¶: {final_df.shape}")

        return file_wide_tables

    def generate_feature_dictionary(self, wide_df, category_feature_mapping):
        """
        ç”Ÿæˆç‰¹å¾å­—å…¸ï¼Œè€ƒè™‘ä»category_type.csvä¸­è¯»å–çš„ç±»åˆ«ç‰¹å¾
        """
        feature_dict = []
        for col in wide_df.columns:
            # ä½¿ç”¨å·¥å…·å‡½æ•°ç¡®å®šç‰¹å¾ç±»å‹
            feature_type = self.determine_feature_type(col, wide_df, category_feature_mapping)
                
            feature_dict.append({
                'feature_name': col,
                'feature_type': feature_type
            })
        return pd.DataFrame(feature_dict)
        
    def merge_all_wide_tables(self, file_wide_tables, table_purpose="å»ºæ¨¡"):
        """
        åˆå¹¶æ‰€æœ‰æ–‡ä»¶å®½è¡¨
        
        å‚æ•°:
        file_wide_tables: æ–‡ä»¶å®½è¡¨å­—å…¸
        table_purpose: è¡¨æ ¼ç”¨é€”æè¿°ï¼ˆç”¨äºæ‰“å°ä¿¡æ¯ï¼‰
        
        è¿”å›:
        DataFrame: åˆå¹¶åçš„å…¨å±€å®½è¡¨
        """
        print(f"\n=== åˆå¹¶æ‰€æœ‰æ–‡ä»¶å®½è¡¨ï¼ˆç”¨äº{table_purpose}ï¼‰===")
        all_wide_dfs = list(file_wide_tables.values())
        if len(all_wide_dfs) == 1:
            global_wide = all_wide_dfs[0].copy()
            print(f"  åªæœ‰ä¸€ä¸ªæ–‡ä»¶å®½è¡¨ï¼ŒåŒ¹é…ç‡: 100%")
        else:
            global_wide = all_wide_dfs[0].copy()
            total_ids = len(global_wide)
            for df in all_wide_dfs[1:]:
                matched_ids = len(pd.merge(global_wide[['Id']], df[['Id']], on='Id', how='inner'))
                match_rate = matched_ids / total_ids if total_ids > 0 else 0
                print(f"  ä¸ {df.shape[0]} è¡Œçš„å®½è¡¨åˆå¹¶ï¼ŒåŸºäºä¸»é”®(Id)åŒ¹é…ç‡: {match_rate:.2%}")
                global_wide = pd.merge(global_wide, df, on='Id', how='outer')
                total_ids = len(global_wide)
                
        return global_wide
        
    def save_global_results(self, global_wide, category_feature_mapping, global_output_file, feature_dict_file):
        """
        ä¿å­˜å…¨å±€å®½è¡¨å’Œç‰¹å¾å­—å…¸
        
        å‚æ•°:
        global_wide: å…¨å±€å®½è¡¨DataFrame
        category_feature_mapping: ç±»åˆ«ç‰¹å¾æ˜ å°„
        global_output_file: å…¨å±€å®½è¡¨è¾“å‡ºæ–‡ä»¶è·¯å¾„
        feature_dict_file: ç‰¹å¾å­—å…¸è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # è®¡ç®—è¡ç”Ÿç‰¹å¾
        global_wide = self.calculate_derived_features(global_wide)
        
        # ä¿å­˜å…¨å±€å®½è¡¨
        global_output = os.path.join(self.output_dir, global_output_file)
        global_wide.to_csv(global_output, index=False, encoding='utf-8')
        
        # ç”Ÿæˆå¹¶ä¿å­˜ç‰¹å¾å­—å…¸
        global_dict = self.generate_feature_dictionary(global_wide, category_feature_mapping)
        global_dict.to_csv(os.path.join(self.output_dir, feature_dict_file), index=False, encoding='utf-8')
        
        return global_wide, global_dict, global_output
        
    @abstractmethod
    def process_specific_results(self, global_wide, global_dict):
        """
        å¤„ç†ç‰¹å®šäºå­ç±»çš„ç»“æœï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰
        
        å‚æ•°:
        global_wide: å…¨å±€å®½è¡¨DataFrame
        global_dict: å…¨å±€ç‰¹å¾å­—å…¸DataFrame
        """
        pass
        
    def main(self, coverage_threshold=0.95, max_top_k=50):
        """
        ä¸»å¤„ç†æµç¨‹
        """
        # è·å–ç±»åˆ«ç‰¹å¾æ˜ å°„
        all_data, all_primary_keys, category_feature_mapping = self.process_all_excel_files()
        
        # è¿›è¡Œæ•°æ®åˆ†æ
        print("\n=== å¼€å§‹æ•°æ®åˆ†æ ===")
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å­ç±»æä¾›analyzerå±æ€§
        if hasattr(self, 'analyzer'):
            # æ£€æŸ¥æ˜¯å¦æ˜¯è®­ç»ƒæ•°æ®å¤„ç†å™¨ï¼ˆconvert_train_data.pyï¼‰
            # å¦‚æœæ˜¯ï¼Œåˆ™ä½¿ç”¨åŸæœ‰æ–¹å¼ï¼ˆç®€å•çš„æ•°æ®æ¦‚è§ˆã€å­—æ®µåˆ†æå’Œç¼ºå¤±å€¼åˆ†æï¼‰
            if self.__class__.__name__ == 'TrainDataProcessor':
                # ä½¿ç”¨åŸæœ‰æ–¹å¼ï¼šåªè¿›è¡ŒåŸºæœ¬çš„æ•°æ®åˆ†æï¼ˆå‚è€ƒdata_analyzer_prev.pyï¼‰
                analysis_results = self.analyzer.analyze_dataset(all_data, use_business_view=False)
            else:
                # ä½¿ç”¨ä¿®æ”¹åçš„æ–¹å¼ï¼šè¿›è¡Œä¸šåŠ¡è§†è§’çš„æ•°æ®åˆ†æå¹¶ä¿å­˜åˆ°Excelæ–‡ä»¶
                analysis_results = self.analyzer.analyze_dataset(all_data, use_business_view=True)
            print("âœ… æ•°æ®åˆ†æå®Œæˆ")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ•°æ®åˆ†æå™¨ï¼Œè·³è¿‡æ•°æ®åˆ†æ")
        
        field_analysis, dimension_analysis = self.analyze_fields_and_dimensions(all_data, category_feature_mapping)
        print(f"\nåˆ†æäº† {len(field_analysis)} ä¸ªå­—æ®µ, {len(dimension_analysis)} ä¸ªç»´åº¦")

        file_wide_tables = self.create_wide_table_per_file(
            all_data,
            dimension_analysis,
            all_primary_keys,
            category_feature_mapping,
            coverage_threshold=coverage_threshold,
            max_top_k=max_top_k
        )

        if not file_wide_tables:
            print("âŒ æœªç”Ÿæˆä»»ä½•å®½è¡¨")
            return

        # ä¸å†ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„ç‹¬ç«‹å®½è¡¨ï¼Œåªä¿ç•™æœ€ç»ˆå¤§å®½è¡¨
        # ç”Ÿæˆæ¯ä¸ªæ–‡ä»¶çš„ç‰¹å¾å­—å…¸ä½†ä¸ä¿å­˜
        for file_name, wide_df in file_wide_tables.items():
            feature_dict_df = self.generate_feature_dictionary(wide_df, category_feature_mapping)

        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶å®½è¡¨
        global_wide = self.merge_all_wide_tables(file_wide_tables)
        
        # ä¿å­˜å…¨å±€ç»“æœå¹¶è·å–è¿”å›å€¼
        global_wide, global_dict, global_output = self.save_global_results(
            global_wide, 
            category_feature_mapping,
            self.get_global_output_filename(),
            self.get_feature_dict_filename()
        )
        
        # å¤„ç†ç‰¹å®šäºå­ç±»çš„ç»“æœ
        self.process_specific_results(global_wide, global_dict)
        
        print(f"\nğŸ“Š å…¨å±€å®½è¡¨æœ€ç»ˆå½¢çŠ¶: {global_wide.shape[0]} è¡Œ, {global_wide.shape[1]} åˆ—")
        
    @abstractmethod
    def get_global_output_filename(self):
        """
        è·å–å…¨å±€è¾“å‡ºæ–‡ä»¶åï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰
        """
        pass
        
    @abstractmethod
    def get_feature_dict_filename(self):
        """
        è·å–ç‰¹å¾å­—å…¸æ–‡ä»¶åï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰
        """
        pass
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# æ·±åº¦å­¦ä¹ ç›¸å…³å¯¼å…¥
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    print("è­¦å‘Š: æœªå®‰è£…PyTorchï¼Œå°†è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸å…³åŠŸèƒ½")

# å°è¯•å¯¼å…¥BaseModelProcessorï¼Œå¯èƒ½ä¾èµ–PyTorch
try:
    from base.base_model_processor import BaseModelProcessor
except ImportError as e:
    if not HAS_TORCH:
        print(f"è­¦å‘Š: æœªå®‰è£…PyTorchï¼ŒBaseModelProcessorå¯¼å…¥å¤±è´¥: {e}")
        BaseModelProcessor = None
    else:
        raise

# ========== æ•°æ®é¢„å¤„ç† ==========
def preProcess():
    path = 'data_train/'
    try:
        df_train = pd.read_csv(path + 'train.csv', encoding='utf-8')
    except UnicodeDecodeError:
        print("âš ï¸ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GBK ç¼–ç ...")
        df_train = pd.read_csv(path + 'train.csv', encoding='gbk')
    
    df_train.drop(['Id'], axis=1, inplace=True)
    data = df_train.fillna(-1)
    
    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹ï¼Œå°†éæ•°å€¼ç±»å‹è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    for col in data.columns:
        if data[col].dtype == 'object':
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ç½®ä¸º-1
            data[col] = pd.to_numeric(data[col], errors='coerce').fillna(-1)
    
    data.to_csv('data_train/data.csv', index=False, encoding='utf-8')
    return data

# ========== æ·±åº¦å­¦ä¹ è®­ç»ƒå‡½æ•° ==========
def deep_learning_train(data, category_feature, continuous_feature):
    """
    ä½¿ç”¨æ·±åº¦å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œå¢å¼ºå¯è§£é‡Šæ€§è¾“å‡º
    """
    # æ£€æŸ¥BaseModelProcessoræ˜¯å¦å¯ç”¨
    if BaseModelProcessor is None:
        print("âŒ BaseModelProcessorä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒ")
        return
    
    processor = BaseModelProcessor()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)

    # ========== Step 1: ç±»åˆ«ç‰¹å¾ One-Hot ç¼–ç  ==========
    for col in category_feature:
        if data[col].dtype == 'object':
            data[col] = data[col].astype('category')
        onehot_feats = pd.get_dummies(data[col], prefix=col)
        data.drop([col], axis=1, inplace=True)
        data = pd.concat([data, onehot_feats], axis=1)

    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    target = data.pop('Label')
    train = data.copy()

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹
    for col in train.columns:
        if train[col].dtype == 'object':
            train[col] = pd.to_numeric(train[col], errors='coerce').fillna(-1)
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼Œé¿å…ç±»å‹è½¬æ¢é”™è¯¯
        train[col] = pd.to_numeric(train[col], errors='coerce').fillna(-1).astype('float32')
    
    # å†æ¬¡æ£€æŸ¥å¹¶ç¡®ä¿æ²¡æœ‰objectç±»å‹çš„æ•°æ®
    object_cols = train.dtypes[train.dtypes == 'object'].index.tolist()
    if object_cols:
        for col in object_cols:
            train[col] = pd.to_numeric(train[col], errors='coerce').fillna(-1).astype('float32')

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    # ç¡®ä¿æ ‡ç­¾ä¹Ÿæ˜¯æ•°å€¼ç±»å‹
    target = pd.to_numeric(target, errors='coerce').fillna(0).astype('float32')
    
    x_train, x_val, y_train, y_val = train_test_split(
        train, target, test_size=0.2, random_state=2020, stratify=target
    )

    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    if not HAS_TORCH:
        print("âŒ æœªå®‰è£…PyTorchï¼Œæ— æ³•è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹")
        return None
        
    x_train_tensor = torch.FloatTensor(x_train.values)
    x_val_tensor = torch.FloatTensor(x_val.values)
    y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)
    y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)
    val_dataset = torch.utils.data.TensorDataset(x_val_tensor, y_val_tensor)
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)

    # ========== Step 2: åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ ==========
    input_dim = x_train.shape[1]
    model = processor.DeepLearningModel(input_dim, hidden_dims=[256, 128, 64], dropout_rate=0.3)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = torch.nn.BCELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

    # ========== Step 3: è®­ç»ƒæ¨¡å‹ ==========
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    num_epochs = 100
    best_val_loss = float('inf')
    early_stopping_patience = 15
    patience_counter = 0
    
    train_losses = []
    val_losses = []
    train_aucs = []
    val_aucs = []
    
    print("ğŸš€ å¼€å§‹æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ...")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item() * batch_x.size(0)
            train_preds.extend(outputs.detach().cpu().numpy())
            train_targets.extend(batch_y.cpu().numpy())
        
        train_loss /= len(train_loader.dataset)
        train_auc = roc_auc_score(np.array(train_targets), np.array(train_preds))
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item() * batch_x.size(0)
                val_preds.extend(outputs.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        val_loss /= len(val_loader.dataset)
        val_auc = roc_auc_score(np.array(val_targets), np.array(val_preds))
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)
        
        # è®°å½•æŸå¤±å’ŒAUC
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_aucs.append(train_auc)
        val_aucs.append(val_auc)
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], '
                  f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '
                  f'Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}')
        
        # æ—©åœæœºåˆ¶
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), 'output/dl_model_best.pth')
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

    # ========== Step 4: åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼° ==========
    model.load_state_dict(torch.load('output/dl_model_best.pth'))
    model.eval()
    
    # è®¡ç®—æœ€ç»ˆé¢„æµ‹ç»“æœ
    with torch.no_grad():
        tr_pred_prob = model(x_train_tensor.to(device)).cpu().numpy().flatten()
        val_pred_prob = model(x_val_tensor.to(device)).cpu().numpy().flatten()
    
    tr_logloss = log_loss(y_train, tr_pred_prob)
    val_logloss = log_loss(y_val, val_pred_prob)
    
    # è®¡ç®— KS ç»Ÿè®¡é‡
    tr_ks = processor.calculate_ks_statistic(y_train, tr_pred_prob)
    val_ks = processor.calculate_ks_statistic(y_val, val_pred_prob)
    
    tr_auc = roc_auc_score(y_train, tr_pred_prob)
    val_auc = roc_auc_score(y_val, val_pred_prob)
    
    print('\nâœ… Train LogLoss:', tr_logloss)
    print('âœ… Val LogLoss:', val_logloss)
    print('âœ… Train KS:', tr_ks)
    print('âœ… Val KS:', val_ks)
    print('âœ… Train AUC:', tr_auc)
    print('âœ… Val AUC:', val_auc)

    # ========== Step 5: å¯è§†åŒ–ç»“æœ ==========
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    processor.plot_training_curves(train_losses, val_losses, train_aucs, val_aucs, "output/dl_training_curves.png")
    
    # æ·»åŠ ROCæ›²çº¿å¯è§†åŒ–
    processor.plot_roc_curve(y_val, val_pred_prob, "output/dl_roc_curve.png")

    # ========== Step 6: ç‰¹å¾é‡è¦æ€§åˆ†æ ==========
    feat_imp = processor.analyze_dl_feature_importance(model, x_val_tensor, x_train.columns, "output/dl_feature_importance.csv")

    # ========== Step 7: ä¿å­˜æ¨¡å‹ä¿¡æ¯ ==========
    # ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯
    model_info = {
        'input_dim': input_dim,
        'hidden_dims': [256, 128, 64],
        'dropout_rate': 0.3,
        'best_val_loss': best_val_loss,
        'final_epoch': len(train_losses)
    }
    
    # ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯
    processor.save_models(dl_model=model, dl_model_info=model_info)
    
    print("âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
    print("   - dl_model_best.pth")
    print("   - dl_feature_importance.csv")
    print("   - dl_training_curves.png")
    print("   - dl_roc_curve.png")
    print("   - dl_model_info.csv")

    return model

# ========== ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    data = preProcess()

    # ========== ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾å®šä¹‰ ==========
    print("ğŸ“‚ æ­£åœ¨åŠ è½½ç‰¹å¾é…ç½®...")
    # æ£€æŸ¥BaseModelProcessoræ˜¯å¦å¯ç”¨
    if BaseModelProcessor is None:
        print("âŒ BaseModelProcessorä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½ç‰¹å¾é…ç½®")
        exit(1)
        
    processor = BaseModelProcessor()
    if not processor.load_feature_config():
        print("âŒ åŠ è½½ç‰¹å¾é…ç½®å¤±è´¥")
        exit(1)
        
    continuous_feature = processor.continuous_features
    category_feature = processor.category_features

    print("âœ… è¿ç»­ç‰¹å¾:", continuous_feature)
    print("âœ… ç±»åˆ«ç‰¹å¾:", category_feature)

    # æ˜¾ç¤ºå¤§æ¨¡å‹è§£è¯»æç¤º
    processor.show_model_interpretation_prompt()

    print("ğŸ§  å¼€å§‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
    model = deep_learning_train(data, category_feature, continuous_feature)

    if model is not None:
        print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
        print("   - dl_model_best.pth")
        print("   - dl_feature_importance.csv")
        print("   - dl_training_curves.png")
        print("   - dl_roc_curve.png")
        print("   - dl_model_info.csv")
    else:
        print("\nâŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼")

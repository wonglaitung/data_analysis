import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import platform
from base.base_model_processor import BaseModelProcessor

# ä»…åœ¨Windowsç³»ç»Ÿä¸Šè®¾ç½®ä¸­æ–‡å­—ä½“
if platform.system() == 'Windows':
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # Windows å¾®è½¯é›…é»‘
    plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# ========== æ·±åº¦å­¦ä¹ æ¨¡å‹å®šä¹‰ ==========
class DeepLearningModel(nn.Module):
    def __init__(self, input_dim, hidden_dims=[128, 64], dropout_rate=0.3):
        super(DeepLearningModel, self).__init__()
        
        # è¾“å…¥å±‚
        self.input_layer = nn.Linear(input_dim, hidden_dims[0])
        self.input_bn = nn.BatchNorm1d(hidden_dims[0])
        
        # éšè—å±‚
        self.hidden_layers = nn.ModuleList()
        self.hidden_bns = nn.ModuleList()
        self.hidden_drops = nn.ModuleList()
        
        for i in range(len(hidden_dims) - 1):
            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))
            self.hidden_bns.append(nn.BatchNorm1d(hidden_dims[i+1]))
            self.hidden_drops.append(nn.Dropout(dropout_rate))
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_dims[-1], 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # è¾“å…¥å±‚
        x = F.relu(self.input_bn(self.input_layer(x)))
        x = F.dropout(x, p=0.3, training=self.training)
        
        # éšè—å±‚
        for layer, bn, drop in zip(self.hidden_layers, self.hidden_bns, self.hidden_drops):
            x = F.relu(bn(layer(x)))
            x = drop(x)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        x = self.sigmoid(x)
        
        return x

# ========== æ•°æ®é¢„å¤„ç† ==========
def preProcess():
    path = 'data_train/'
    try:
        df_train = pd.read_csv(path + 'train.csv', encoding='utf-8')
    except UnicodeDecodeError:
        print("âš ï¸ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GBK ç¼–ç ...")
        df_train = pd.read_csv(path + 'train.csv', encoding='gbk')
    
    df_train.drop(['Id'], axis=1, inplace=True)
    data = df_train.fillna(-1)
    
    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹ï¼Œå°†éæ•°å€¼ç±»å‹è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    for col in data.columns:
        if data[col].dtype == 'object':
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ç½®ä¸º-1
            data[col] = pd.to_numeric(data[col], errors='coerce').fillna(-1)
    
    data.to_csv('data_train/data.csv', index=False, encoding='utf-8')
    return data

# ========== æ·±åº¦å­¦ä¹ è®­ç»ƒå‡½æ•° ==========
def deep_learning_train(data, category_feature, continuous_feature):
    """
    ä½¿ç”¨æ·±åº¦å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œå¢å¼ºå¯è§£é‡Šæ€§è¾“å‡º
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)

    # ========== Step 1: ç±»åˆ«ç‰¹å¾ One-Hot ç¼–ç  ==========
    for col in category_feature:
        if data[col].dtype == 'object':
            data[col] = data[col].astype('category')
        onehot_feats = pd.get_dummies(data[col], prefix=col)
        data.drop([col], axis=1, inplace=True)
        data = pd.concat([data, onehot_feats], axis=1)

    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    target = data.pop('Label')
    train = data.copy()

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹
    for col in train.columns:
        if train[col].dtype == 'object':
            train[col] = pd.to_numeric(train[col], errors='coerce').fillna(-1)
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼Œé¿å…ç±»å‹è½¬æ¢é”™è¯¯
        train[col] = pd.to_numeric(train[col], errors='coerce').fillna(-1).astype('float32')
    
    # å†æ¬¡æ£€æŸ¥å¹¶ç¡®ä¿æ²¡æœ‰objectç±»å‹çš„æ•°æ®
    object_cols = train.dtypes[train.dtypes == 'object'].index.tolist()
    if object_cols:
        for col in object_cols:
            train[col] = pd.to_numeric(train[col], errors='coerce').fillna(-1).astype('float32')

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    # ç¡®ä¿æ ‡ç­¾ä¹Ÿæ˜¯æ•°å€¼ç±»å‹
    target = pd.to_numeric(target, errors='coerce').fillna(0).astype('float32')
    
    x_train, x_val, y_train, y_val = train_test_split(
        train, target, test_size=0.2, random_state=2020, stratify=target
    )

    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    x_train_tensor = torch.FloatTensor(x_train.values)
    x_val_tensor = torch.FloatTensor(x_val.values)
    y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)
    y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    # ========== Step 2: åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ ==========
    input_dim = x_train.shape[1]
    model = DeepLearningModel(input_dim, hidden_dims=[256, 128, 64], dropout_rate=0.3)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCELoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

    # ========== Step 3: è®­ç»ƒæ¨¡å‹ ==========
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    num_epochs = 100
    best_val_loss = float('inf')
    early_stopping_patience = 15
    patience_counter = 0
    
    train_losses = []
    val_losses = []
    train_aucs = []
    val_aucs = []
    
    print("ğŸš€ å¼€å§‹æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ...")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item() * batch_x.size(0)
            train_preds.extend(outputs.detach().cpu().numpy())
            train_targets.extend(batch_y.cpu().numpy())
        
        train_loss /= len(train_loader.dataset)
        train_auc = roc_auc_score(np.array(train_targets), np.array(train_preds))
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item() * batch_x.size(0)
                val_preds.extend(outputs.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        val_loss /= len(val_loader.dataset)
        val_auc = roc_auc_score(np.array(val_targets), np.array(val_preds))
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)
        
        # è®°å½•æŸå¤±å’ŒAUC
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_aucs.append(train_auc)
        val_aucs.append(val_auc)
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], '
                  f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '
                  f'Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}')
        
        # æ—©åœæœºåˆ¶
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), 'output/dl_model_best.pth')
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

    # ========== Step 4: åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼° ==========
    model.load_state_dict(torch.load('output/dl_model_best.pth'))
    model.eval()
    
    # è®¡ç®—æœ€ç»ˆé¢„æµ‹ç»“æœ
    with torch.no_grad():
        tr_pred_prob = model(x_train_tensor.to(device)).cpu().numpy().flatten()
        val_pred_prob = model(x_val_tensor.to(device)).cpu().numpy().flatten()
    
    tr_logloss = log_loss(y_train, tr_pred_prob)
    val_logloss = log_loss(y_val, val_pred_prob)
    
    # è®¡ç®— KS ç»Ÿè®¡é‡
    def calculate_ks_statistic(y_true, y_pred_prob):
        from scipy.stats import ks_2samp
        # å°†æ ·æœ¬æŒ‰é¢„æµ‹æ¦‚ç‡æ’åº
        data = pd.DataFrame({'y_true': y_true, 'y_pred_prob': y_pred_prob})
        data_sorted = data.sort_values('y_pred_prob', ascending=False)
        
        # è®¡ç®—ç´¯ç§¯åˆ†å¸ƒ
        cum_positive = (data_sorted['y_true'] == 1).cumsum() / (y_true == 1).sum()
        cum_negative = (data_sorted['y_true'] == 0).cumsum() / (y_true == 0).sum()
        
        # KSç»Ÿè®¡é‡æ˜¯ä¸¤ä¸ªç´¯ç§¯åˆ†å¸ƒä¹‹é—´çš„æœ€å¤§å·®å¼‚
        ks_stat = np.max(np.abs(cum_positive - cum_negative))
        return ks_stat
    
    tr_ks = calculate_ks_statistic(y_train, tr_pred_prob)
    val_ks = calculate_ks_statistic(y_val, val_pred_prob)
    
    tr_auc = roc_auc_score(y_train, tr_pred_prob)
    val_auc = roc_auc_score(y_val, val_pred_prob)
    
    print('\nâœ… Train LogLoss:', tr_logloss)
    print('âœ… Val LogLoss:', val_logloss)
    print('âœ… Train KS:', tr_ks)
    print('âœ… Val KS:', val_ks)
    print('âœ… Train AUC:', tr_auc)
    print('âœ… Val AUC:', val_auc)

    # ========== Step 5: å¯è§†åŒ–ç»“æœ ==========
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(train_aucs, label='Train AUC')
    plt.plot(val_aucs, label='Validation AUC')
    plt.xlabel('Epoch')
    plt.ylabel('AUC')
    plt.title('Training and Validation AUC')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig("output/dl_training_curves.png", dpi=150, bbox_inches='tight')
    plt.close()
    print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ output/dl_training_curves.png")
    
    # æ·»åŠ ROCæ›²çº¿å¯è§†åŒ–
    fpr, tpr, _ = roc_curve(y_val, val_pred_prob)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {val_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.savefig("output/dl_roc_curve.png", dpi=150, bbox_inches='tight')
    plt.close()
    print("âœ… ROCæ›²çº¿å·²ä¿å­˜è‡³ output/dl_roc_curve.png")

    # ========== Step 6: ç‰¹å¾é‡è¦æ€§åˆ†æ ==========
    print("\n" + "="*60)
    print("ğŸ§  æ­£åœ¨åˆ†æç‰¹å¾é‡è¦æ€§...")
    print("="*60)
    
    # ä½¿ç”¨æ¢¯åº¦æ–¹æ³•è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆä»…åœ¨CPUä¸Šè®¡ç®—ä»¥é¿å…GPUå†…å­˜é—®é¢˜ï¼‰
    model.eval()
    x_sample = x_val_tensor[:500].to("cpu")  # ä½¿ç”¨æ›´å°‘çš„æ ·æœ¬è®¡ç®—é‡è¦æ€§
    x_sample.requires_grad = True
    
    output = model(x_sample)
    output.sum().backward()
    
    # è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆæ¢¯åº¦çš„ç»å¯¹å€¼ï¼‰
    feature_importance = torch.abs(x_sample.grad).mean(dim=0).cpu().numpy()
    
    # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
    feat_imp = pd.DataFrame({
        'Feature': x_train.columns,
        'Importance': feature_importance
    }).sort_values('Importance', ascending=False)
    
    # ========== å¢åŠ ï¼šé€šè¿‡æ¢¯åº¦ç¬¦å·åˆ†æç‰¹å¾å½±å“æ–¹å‘ ==========
    try:
        print("\n" + "="*60)
        print("ğŸ§  æ­£åœ¨é€šè¿‡æ¢¯åº¦ç¬¦å·åˆ†æç‰¹å¾å½±å“æ–¹å‘...")
        print("="*60)
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡æ¢¯åº¦å€¼ï¼Œç”¨äºåˆ¤æ–­å½±å“æ–¹å‘
        mean_grad_values = x_sample.grad.mean(dim=0).cpu().numpy()
        
        # å°†å¹³å‡æ¢¯åº¦å€¼æ·»åŠ åˆ°ç‰¹å¾é‡è¦æ€§DataFrameä¸­
        feat_imp['Mean_Grad_Value'] = mean_grad_values
        # æ ¹æ®å¹³å‡æ¢¯åº¦å€¼åˆ¤æ–­å½±å“æ–¹å‘ï¼šæ­£æ•°ä¸ºæ­£å‘å½±å“ï¼Œè´Ÿæ•°ä¸ºè´Ÿå‘å½±å“
        feat_imp['Impact_Direction'] = feat_imp['Mean_Grad_Value'].apply(lambda x: 'Positive' if x > 0 else 'Negative')
        
        # ä¿å­˜åŒ…å«æ‰€æœ‰ä¿¡æ¯çš„ç‰¹å¾é‡è¦æ€§æ–‡ä»¶
        feat_imp.to_csv('output/dl_feature_importance.csv', index=False)
        print("âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³ output/dl_feature_importance.csv")
        
        # æ˜¾ç¤ºå‰20ä¸ªé‡è¦ç‰¹å¾çš„å®Œæ•´ä¿¡æ¯
        print("\nğŸ“Š æ·±åº¦å­¦ä¹ æ¨¡å‹ Top 20 é‡è¦ç‰¹å¾ (å«å½±å“æ–¹å‘):")
        print("="*60)
        print(feat_imp[['Feature', 'Importance', 'Impact_Direction']].head(20))
        
    except Exception as e:
        print(f"âš ï¸ ç‰¹å¾å½±å“æ–¹å‘åˆ†æå¤±è´¥: {e}")
        # å¦‚æœåˆ†æå¤±è´¥ï¼Œä»ä¿ç•™åŸºæœ¬çš„ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
        feat_imp['Impact_Direction'] = 'Unknown'
        # ä¿å­˜åŒ…å«æ‰€æœ‰ä¿¡æ¯çš„ç‰¹å¾é‡è¦æ€§æ–‡ä»¶
        feat_imp.to_csv('output/dl_feature_importance.csv', index=False)
        print("âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³ output/dl_feature_importance.csv")
        
        # æ˜¾ç¤ºå‰20ä¸ªé‡è¦ç‰¹å¾
        print("\nğŸ“Š æ·±åº¦å­¦ä¹ æ¨¡å‹ Top 20 é‡è¦ç‰¹å¾:")
        print("="*60)
        print(feat_imp.head(20))

    # ========== Step 7: ä¿å­˜æ¨¡å‹ä¿¡æ¯ ==========
    # ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯
    model_info = {
        'input_dim': input_dim,
        'hidden_dims': [256, 128, 64],
        'dropout_rate': 0.3,
        'best_val_loss': best_val_loss,
        'final_epoch': len(train_losses)
    }
    
    pd.DataFrame([model_info]).to_csv('output/dl_model_info.csv', index=False)
    print("âœ… æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜è‡³ output/dl_model_info.csv")

    print("âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
    print("   - dl_model_best.pth")
    print("   - dl_feature_importance.csv")
    print("   - dl_training_curves.png")
    print("   - dl_roc_curve.png")
    print("   - dl_model_info.csv")

    return model

# ========== ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    data = preProcess()

    # ========== ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾å®šä¹‰ ==========
    print("ğŸ“‚ æ­£åœ¨åŠ è½½ç‰¹å¾é…ç½®...")
    feature_config = pd.read_csv('config/features.csv')
    continuous_feature = feature_config[feature_config['feature_type'] == 'continuous']['feature_name'].tolist()
    category_feature = feature_config[feature_config['feature_type'] == 'category']['feature_name'].tolist()

    print("âœ… è¿ç»­ç‰¹å¾:", continuous_feature)
    print("âœ… ç±»åˆ«ç‰¹å¾:", category_feature)

    print("\nâœ… ======================================")
    print("âœ… å°†ä¸‹é¢çš„å†…å®¹å¤åˆ¶åˆ°å¤§æ¨¡å‹å†…è¿›è¡Œè§£è¯»ï¼ˆä¸åŒ…æ‹¬æ­¤ä¸‰è¡Œï¼‰")
    print("âœ… ======================================\n")

    print("å¯¹ä»¥ä¸‹(æ¨è/æˆä¿¡/é¢„è­¦)æ¨¡å‹è®­ç»ƒæ—¥å¿—è¿›è¡Œåˆ†æï¼Œè¾“å‡ºé“¶è¡Œä¸šåŠ¡äººå‘˜å¯ä»¥ç†è§£çš„è§£è¯»æŠ¥å‘Šï¼Œç›®åœ°æ˜¯è¿›è¡Œ(æ¨è/æˆä¿¡/é¢„è­¦)ï¼Œé€šè¿‡æ¨¡å‹åˆ†æèµ‹èƒ½ä¸šåŠ¡å†³ç­–ã€‚\n")

    print("ğŸ§  å¼€å§‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
    model = deep_learning_train(data, category_feature, continuous_feature)

    print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
    print("   - dl_model_best.pth")
    print("   - dl_feature_importance.csv")
    print("   - dl_training_curves.png")
    print("   - dl_roc_curve.png")
    print("   - dl_model_info.csv")

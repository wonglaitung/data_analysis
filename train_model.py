import os
#os.environ["NUMBA_DISABLE_TBB"] = "1"
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss
from sklearn.metrics import roc_auc_score, roc_curve
from lightgbm import log_evaluation
import matplotlib.pyplot as plt
import platform
from base.base_model_processor import BaseModelProcessor

# ä»…åœ¨Windowsç³»ç»Ÿä¸Šè®¾ç½®ä¸­æ–‡å­—ä½“
if platform.system() == 'Windows':
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # Windows å¾®è½¯é›…é»‘
    plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·


# ========== æ•°æ®é¢„å¤„ç† ==========
def preProcess():
    path = 'data_train/'
    try:
        df_train = pd.read_csv(path + 'train.csv', encoding='utf-8')
    except UnicodeDecodeError:
        print("âš ï¸ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GBK ç¼–ç ...")
        df_train = pd.read_csv(path + 'train.csv', encoding='gbk')
    
    df_train.drop(['Id'], axis=1, inplace=True)
    data = df_train.fillna(-1)
    
    data.to_csv('data_train/data.csv', index=False, encoding='utf-8')
    return data


# ========== GBDT + LR æ ¸å¿ƒè®­ç»ƒå‡½æ•° ==========
def gbdt_lr_train(data, category_feature, continuous_feature):
    """
    ä½¿ç”¨ GBDT + LR è®­ç»ƒæ¨¡å‹ï¼Œå¢å¼ºå¯è§£é‡Šæ€§è¾“å‡º
    """
    processor = BaseModelProcessor()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)

    # ========== Step 1: ç±»åˆ«ç‰¹å¾ One-Hot ç¼–ç  ==========
    for col in category_feature:
        if data[col].dtype == 'object':
            data[col] = data[col].astype('category')
        onehot_feats = pd.get_dummies(data[col], prefix=col)
        data.drop([col], axis=1, inplace=True)
        data = pd.concat([data, onehot_feats], axis=1)

    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    target = data.pop('Label')
    train = data.copy()

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    x_train, x_val, y_train, y_val = train_test_split(
        train, target, test_size=0.2, random_state=2020, stratify=target
    )

    # ========== Step 2: è®­ç»ƒ GBDT ==========
    n_estimators = 32
    num_leaves = 64

    model = lgb.LGBMClassifier(
        objective='binary',
        boosting_type='gbdt',
        subsample=0.8,
        min_child_weight=0.1,
        min_child_samples=10,
        colsample_bytree=0.7,
        num_leaves=num_leaves,
        learning_rate=0.05,
        n_estimators=n_estimators,
        random_state=2020,
        n_jobs=-1
    )

    model.fit(
        x_train, y_train,
        eval_set=[(x_train, y_train), (x_val, y_val)],
        eval_metric='binary_logloss',
        callbacks=[
            log_evaluation(0),
            lgb.early_stopping(stopping_rounds=5, verbose=False)
        ]
    )

    # ========== ğŸ†• è·å–å®é™…è®­ç»ƒçš„æ ‘æ•°é‡ ==========
    actual_n_estimators = model.best_iteration_
    print(f"âœ… å®é™…è®­ç»ƒæ ‘æ•°é‡: {actual_n_estimators} (åŸè®¡åˆ’: {n_estimators})")

    # ========== Step 2.5: è¾“å‡º GBDT ç‰¹å¾é‡è¦æ€§ï¼ˆå«å½±å“æ–¹å‘ï¼‰ ==========
    # ä½¿ç”¨åŸºç±»ä¸­çš„æ–¹æ³•åˆ†æç‰¹å¾é‡è¦æ€§
    feat_imp = processor.analyze_feature_importance(model.booster_, x_train.columns.tolist())
    
    # ========== å¢åŠ ï¼šé€šè¿‡LightGBMå†…ç½®åŠŸèƒ½åˆ†æç‰¹å¾å½±å“æ–¹å‘ ==========
    try:
        # è·å–è®­ç»ƒé›†æ ·æœ¬çš„ç‰¹å¾è´¡çŒ®å€¼
        contrib_values = model.booster_.predict(x_train.values, pred_contrib=True)
        
        # contrib_valuesçš„å½¢çŠ¶ä¸º (n_samples, n_features + 1)
        # æœ€åä¸€åˆ—æ˜¯æœŸæœ›å€¼ï¼ˆbase valueï¼‰ï¼Œå‰é¢çš„åˆ—æ˜¯å„ç‰¹å¾çš„è´¡çŒ®å€¼
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡è´¡çŒ®å€¼ï¼Œç”¨äºåˆ¤æ–­å½±å“æ–¹å‘
        mean_contrib_values = np.mean(contrib_values[:, :-1], axis=0)  # æ’é™¤æœ€åä¸€åˆ—æœŸæœ›å€¼
        
        # å°†å¹³å‡è´¡çŒ®å€¼æ·»åŠ åˆ°ç‰¹å¾é‡è¦æ€§DataFrameä¸­
        feat_imp['Mean_Contrib_Value'] = mean_contrib_values
        # æ ¹æ®å¹³å‡è´¡çŒ®å€¼åˆ¤æ–­å½±å“æ–¹å‘ï¼šæ­£æ•°ä¸ºæ­£å‘å½±å“ï¼Œè´Ÿæ•°ä¸ºè´Ÿå‘å½±å“
        feat_imp['Impact_Direction'] = feat_imp['Mean_Contrib_Value'].apply(lambda x: 'Positive' if x > 0 else 'Negative')
        
        # ä¿å­˜åŒ…å«æ‰€æœ‰ä¿¡æ¯çš„ç‰¹å¾é‡è¦æ€§æ–‡ä»¶
        feat_imp.to_csv('output/gbdt_feature_importance.csv', index=False)
        print("âœ… å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§æ–‡ä»¶è‡³ output/gbdt_feature_importance.csv")
        
        # æ˜¾ç¤ºå‰20ä¸ªé‡è¦ç‰¹å¾çš„å®Œæ•´ä¿¡æ¯ï¼ˆä¸é‡å¤æ‰“å°æ ‡é¢˜ï¼‰
        print("\n" + "="*60)
        print("ğŸ“Š GBDT Top 20 é‡è¦ç‰¹å¾ (å«å½±å“æ–¹å‘):")
        print("="*60)
        print(feat_imp[['Feature', 'Gain_Importance', 'Split_Importance', 'Impact_Direction']].head(20))
        
    except Exception as e:
        print(f"âš ï¸ ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
        # å¦‚æœåˆ†æå¤±è´¥ï¼Œä»ä¿ç•™åŸºæœ¬çš„ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
        feat_imp['Impact_Direction'] = 'Unknown'

    # ========== Step 3: è·å–å¶å­èŠ‚ç‚¹ç´¢å¼• ==========
    gbdt_feats_train = model.booster_.predict(x_train.values, pred_leaf=True)
    gbdt_feats_val = model.booster_.predict(x_val.values, pred_leaf=True)

    # ä¸å†è¾“å‡ºå¶å­èŠ‚ç‚¹ç´¢å¼•çš„è¯¦ç»†ä¿¡æ¯

    # ========== Step 4: å¯¹å¶å­èŠ‚ç‚¹åš One-Hot ç¼–ç  ==========
    # ğŸ†• ä½¿ç”¨ actual_n_estimators æ›¿ä»£ç¡¬ç¼–ç  n_estimators
    gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(actual_n_estimators)]

    df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns=gbdt_feats_name)
    df_val_gbdt_feats = pd.DataFrame(gbdt_feats_val, columns=gbdt_feats_name)

    data_gbdt = pd.concat([df_train_gbdt_feats, df_val_gbdt_feats], ignore_index=True)

    for col in gbdt_feats_name:
        onehot_feats = pd.get_dummies(data_gbdt[col], prefix=col)
        data_gbdt.drop([col], axis=1, inplace=True)
        data_gbdt = pd.concat([data_gbdt, onehot_feats], axis=1)

    train_len = df_train_gbdt_feats.shape[0]
    train_lr = data_gbdt.iloc[:train_len, :].reset_index(drop=True)
    val_lr = data_gbdt.iloc[train_len:, :].reset_index(drop=True)

    # ========== Step 5: è®­ç»ƒ LR æ¨¡å‹ ==========
    x_train_lr, x_val_lr, y_train_lr, y_val_lr = train_test_split(
        train_lr, y_train, test_size=0.3, random_state=2018, stratify=y_train
    )

    lr = LogisticRegression(
        penalty='l2',
        C=0.1,
        solver='liblinear',
        random_state=2018,
        max_iter=1000
    )
    lr.fit(x_train_lr, y_train_lr)

    # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„é¢„æµ‹æ¦‚ç‡
    tr_pred_prob = lr.predict_proba(x_train_lr)[:, 1]
    val_pred_prob = lr.predict_proba(x_val_lr)[:, 1]

    tr_logloss = log_loss(y_train_lr, tr_pred_prob)
    val_logloss = log_loss(y_val_lr, val_pred_prob)
    
    # è®¡ç®— KS ç»Ÿè®¡é‡
    tr_ks = processor.calculate_ks_statistic(y_train_lr, tr_pred_prob)
    val_ks = processor.calculate_ks_statistic(y_val_lr, val_pred_prob)
    
    tr_auc = roc_auc_score(y_train_lr, tr_pred_prob)
    val_auc = roc_auc_score(y_val_lr, val_pred_prob)
    print('\nâœ… Train LogLoss:', tr_logloss)
    print('âœ… Val LogLoss:', val_logloss)
    print('âœ… Train KS:', tr_ks)
    print('âœ… Val KS:', val_ks)
    print('âœ… Train AUC:', tr_auc)
    print('âœ… Val AUC:', val_auc)

    # æ·»åŠ ROCæ›²çº¿å¯è§†åŒ–
    processor.plot_roc_curve(y_val_lr, lr.predict_proba(x_val_lr)[:, 1], "output/roc_curve.png")

    # ========== Step 5.5: è¾“å‡º LR ç³»æ•°ï¼ˆå“ªäº›å¶å­è§„åˆ™æœ€é‡è¦ï¼‰ ==========
    lr_coef = pd.DataFrame({
        'Leaf_Feature': x_train_lr.columns,
        'Coefficient': lr.coef_[0]
    }).sort_values('Coefficient', key=abs, ascending=False)

    print("\n" + "="*60)
    print("ğŸ“Š LR Top 20 é‡è¦å¶å­ç‰¹å¾ï¼ˆæŒ‰ç³»æ•°ç»å¯¹å€¼æ’åºï¼‰:")
    #print("="*60)
    #print(lr_coef.head(20))
    lr_coef.to_csv('output/lr_leaf_coefficients.csv', index=False)
    print("âœ… å·²ä¿å­˜è‡³ output/lr_leaf_coefficients.csv")

    # ========== Step 5.6: å¯¹é«˜æƒé‡å¶å­è¿›è¡Œè§„åˆ™è§£æ ==========
    print("\n" + "="*70)
    print("ğŸ§  è§£æ LR ä¸­é«˜æƒé‡å¶å­èŠ‚ç‚¹å¯¹åº”çš„åŸå§‹è§„åˆ™")
    print("="*70)
    
    top_leaves = lr_coef.head(5)  # è§£æå‰5ä¸ªæœ€é‡è¦å¶å­
    category_prefixes = [col + "_" for col in category_feature]
    
    for idx, row in top_leaves.iterrows():
        leaf_feat = row['Leaf_Feature']
        coef = row['Coefficient']
        
        # è§£æå¶å­åç§°ï¼Œå¦‚ "gbdt_leaf_5_22"
        if leaf_feat.startswith('gbdt_leaf_'):
            parts = leaf_feat.split('_')
            if len(parts) >= 4:
                tree_idx = int(parts[2])
                leaf_idx = int(parts[3])
                
                print(f"\nğŸ” è§£æ {leaf_feat} (LRç³»æ•°: {coef:.4f})")
                try:
                    rule = processor.get_leaf_path_enhanced(
                        model.booster_,
                        tree_index=tree_idx,
                        leaf_index=leaf_idx,
                        feature_names=x_train.columns.tolist(),
                        category_prefixes=category_prefixes
                    )
                    if rule:
                        for i, r in enumerate(rule, 1):
                            print(f"   {i}. {r}")
                    else:
                        print("   âš ï¸ è·¯å¾„æœªæ‰¾åˆ°")
                except Exception as e:
                    print(f"   âš ï¸ è§£æå¤±è´¥: {e}")

    # ========== Step 6: ç‰¹å¾è´¡çŒ®å¯è§†åŒ– ==========
    print("\n" + "="*60)
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆç‰¹å¾è´¡çŒ®å¯è§†åŒ–å›¾è¡¨...")
    print("="*60)
    
    # ä½¿ç”¨LightGBMå†…ç½®çš„ç‰¹å¾è´¡çŒ®è®¡ç®—
    print("â„¹ï¸  å·²ä½¿ç”¨LightGBMå†…ç½®åŠŸèƒ½è®¡ç®—ç‰¹å¾è´¡çŒ®")

    # ä¿å­˜æ¨¡å‹å’Œç›¸å…³é…ç½®
    processor.save_models(model, lr, category_feature, continuous_feature)
    
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
    print("   - gbdt_feature_importance.csv")
    print("   - lr_leaf_coefficients.csv")
    print("   - actual_n_estimators.csv")
    print("   - train_feature_names.csv")
    print("   - category_features.csv")
    print("   - continuous_features.csv")

    return model, lr


# ========== ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    data = preProcess()

    # ========== ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾å®šä¹‰ ==========
    print("ğŸ“‚ æ­£åœ¨åŠ è½½ç‰¹å¾é…ç½®...")
    processor = BaseModelProcessor()
    if not processor.load_feature_config():
        print("âŒ åŠ è½½ç‰¹å¾é…ç½®å¤±è´¥")
        exit(1)
        
    continuous_feature = processor.continuous_features
    category_feature = processor.category_features

    print("âœ… è¿ç»­ç‰¹å¾:", continuous_feature)
    print("âœ… ç±»åˆ«ç‰¹å¾:", category_feature)

    # æ˜¾ç¤ºå¤§æ¨¡å‹è§£è¯»æç¤º
    processor.show_model_interpretation_prompt()

    print("ğŸ§  å¼€å§‹è®­ç»ƒ GBDT + LR æ¨¡å‹...")
    model, lr = gbdt_lr_train(data, category_feature, continuous_feature)

    print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
    print("   - gbdt_feature_importance.csv")
    print("   - lr_leaf_coefficients.csv")
    print("   - actual_n_estimators.csv") 

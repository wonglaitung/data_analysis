import pandas as pd
import os
from base_data_processor import BaseDataProcessor

class PredictDataProcessor(BaseDataProcessor):
    def __init__(self):
        super().__init__("data_predict", "output")
        
    def main(self, coverage_threshold=0.95, max_top_k=50):
        # è·å–ç±»åˆ«ç‰¹å¾æ˜ å°„
        all_data, all_primary_keys, category_feature_mapping = self.process_all_excel_files()
        field_analysis, dimension_analysis = self.analyze_fields_and_dimensions(all_data, category_feature_mapping)
        print(f"\nåˆ†æäº† {len(field_analysis)} ä¸ªå­—æ®µ, {len(dimension_analysis)} ä¸ªç»´åº¦")

        file_wide_tables = self.create_wide_table_per_file(
            all_data,
            dimension_analysis,
            all_primary_keys,
            category_feature_mapping,
            coverage_threshold=coverage_threshold,
            max_top_k=max_top_k
        )

        if not file_wide_tables:
            print("âŒ æœªç”Ÿæˆä»»ä½•å®½è¡¨")
            return

        # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„ç‹¬ç«‹å®½è¡¨
        for file_name, wide_df in file_wide_tables.items():
            safe_name = self._normalize_name(file_name.replace('.xlsx', ''))
            output_csv = os.path.join(self.output_dir, f"wide_table_predict_{safe_name}.csv")
            wide_df.to_csv(output_csv, index=False, encoding='utf-8')
            print(f"âœ… ä¿å­˜æ–‡ä»¶å®½è¡¨: {output_csv}")

            feature_dict_df = self.generate_feature_dictionary(wide_df, category_feature_mapping)
            dict_csv = os.path.join(self.output_dir, f"feature_dict_predict_{safe_name}.csv")
            feature_dict_df.to_csv(dict_csv, index=False, encoding='utf-8')

        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶å®½è¡¨ï¼ˆç”¨äºé¢„æµ‹ï¼‰
        print("\n=== åˆå¹¶æ‰€æœ‰æ–‡ä»¶å®½è¡¨ï¼ˆç”¨äºé¢„æµ‹ï¼‰===")
        all_wide_dfs = list(file_wide_tables.values())
        if len(all_wide_dfs) == 1:
            global_wide = all_wide_dfs[0].copy()
        else:
            global_wide = all_wide_dfs[0].copy()
            for df in all_wide_dfs[1:]:
                global_wide = pd.merge(global_wide, df, on='Id', how='outer')

        global_wide = self.calculate_derived_features(global_wide)

        global_output = os.path.join(self.output_dir, "ml_wide_table_predict_global.csv")
        global_wide.to_csv(global_output, index=False, encoding='utf-8')

        global_dict = self.generate_feature_dictionary(global_wide, category_feature_mapping)
        global_dict.to_csv(os.path.join(self.output_dir, "feature_dictionary_predict_global.csv"), index=False, encoding='utf-8')

        # æ£€æŸ¥é¢„æµ‹æ•°æ®ç‰¹å¾å­—æ®µä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾å­—æ®µæ˜¯å¦åŒ¹é…
        self.check_feature_compatibility(global_wide)

        print(f"\nâœ… é¢„æµ‹å…¨å±€å®½è¡¨å·²ä¿å­˜: {global_output}")
        print(f"âœ… é¢„æµ‹å…¨å±€å­—æ®µå­—å…¸: {os.path.join(self.output_dir, 'feature_dictionary_predict_global.csv')}")
        print(f"\nğŸ“Š é¢„æµ‹å…¨å±€å®½è¡¨æœ€ç»ˆå½¢çŠ¶: {global_wide.shape[0]} è¡Œ, {global_wide.shape[1]} åˆ—")
        
    def _normalize_name(self, name):
        """
        æ ‡å‡†åŒ–åç§°ï¼Œå°†ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        """
        return self.normalize_name(name)

    def check_feature_compatibility(self, predict_wide_df):
        """
        æ£€æŸ¥é¢„æµ‹æ•°æ®ç‰¹å¾å­—æ®µä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾å­—æ®µæ˜¯å¦åŒ¹é…
        """
        # è¯»å–è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾å­—æ®µ
        train_features_file = os.path.join("config", "features.csv")
        if not os.path.exists(train_features_file):
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾å­—æ®µæ–‡ä»¶ï¼Œè·³è¿‡å…¼å®¹æ€§æ£€æŸ¥")
            return

        try:
            train_features_df = pd.read_csv(train_features_file)
            train_features = set(train_features_df['feature_name'].tolist())
            
            # è·å–é¢„æµ‹æ•°æ®çš„ç‰¹å¾å­—æ®µ
            predict_features = set(predict_wide_df.columns.tolist())
            
            # æ£€æŸ¥ç¼ºå°‘çš„ç‰¹å¾å­—æ®µ
            missing_features = train_features - predict_features
            # æ’é™¤Idå­—æ®µï¼Œå› ä¸ºå®ƒå¯èƒ½åœ¨é¢„æµ‹æ•°æ®ä¸­æœ‰ä¸åŒçš„è¡¨ç¤º
            if 'Id' in missing_features:
                missing_features.remove('Id')
            
            # æ£€æŸ¥å¤šä½™çš„ç‰¹å¾å­—æ®µ
            extra_features = predict_features - train_features
            # æ’é™¤Idå­—æ®µï¼Œå› ä¸ºå®ƒå¯èƒ½åœ¨é¢„æµ‹æ•°æ®ä¸­æœ‰ä¸åŒçš„è¡¨ç¤º
            if 'Id' in extra_features:
                extra_features.remove('Id')
            
            if missing_features:
                print(f"âš ï¸  è­¦å‘Š: é¢„æµ‹æ•°æ®ç¼ºå°‘ {len(missing_features)} ä¸ªè®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾å­—æ®µ")
                if len(missing_features) <= 10:
                    print(f"   ç¼ºå°‘çš„ç‰¹å¾å­—æ®µ: {', '.join(sorted(missing_features))}")
                else:
                    print(f"   ç¼ºå°‘çš„ç‰¹å¾å­—æ®µ (å‰10ä¸ª): {', '.join(sorted(list(missing_features))[:10])}")
            
            if extra_features:
                print(f"â„¹ï¸  æç¤º: é¢„æµ‹æ•°æ®åŒ…å« {len(extra_features)} ä¸ªé¢å¤–çš„ç‰¹å¾å­—æ®µ")
                if len(extra_features) <= 10:
                    print(f"   é¢å¤–çš„ç‰¹å¾å­—æ®µ: {', '.join(sorted(extra_features))}")
                else:
                    print(f"   é¢å¤–çš„ç‰¹å¾å­—æ®µ (å‰10ä¸ª): {', '.join(sorted(list(extra_features))[:10])}")
            
            if not missing_features and not extra_features:
                print("âœ… é¢„æµ‹æ•°æ®ç‰¹å¾å­—æ®µä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾å­—æ®µå®Œå…¨åŒ¹é…")
            elif not missing_features:
                print("âœ… é¢„æµ‹æ•°æ®åŒ…å«äº†è®­ç»ƒæ—¶ä½¿ç”¨çš„æ‰€æœ‰ç‰¹å¾å­—æ®µ")
            else:
                print("âŒ é”™è¯¯: é¢„æµ‹æ•°æ®ç¼ºå°‘è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾å­—æ®µï¼Œæ¨¡å‹å¯èƒ½æ— æ³•ä½¿ç”¨")
                
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: æ£€æŸ¥ç‰¹å¾å…¼å®¹æ€§æ—¶å‡ºé”™: {e}")

def main(coverage_threshold=0.95, max_top_k=50):
    processor = PredictDataProcessor()
    processor.main(coverage_threshold, max_top_k)

if __name__ == "__main__":
    main(coverage_threshold=0.95, max_top_k=50)
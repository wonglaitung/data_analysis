import pandas as pd
import os
import glob
from collections import defaultdict
import psutil
import gc

# ==============================
# 1. è·¯å¾„è®¾ç½®
# ==============================
input_dir = "data"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

def get_primary_key(all_data):
    """
    ä¼˜å…ˆä» config/primary_key.csv è¯»å–ä¸»é”®å­—æ®µï¼Œè‹¥ä¸å­˜åœ¨åˆ™è‡ªåŠ¨è¯†åˆ«
    """
    config_dir = "config"
    primary_key_file = os.path.join(config_dir, "primary_key.csv")
    if os.path.exists(primary_key_file):
        try:
            df_pk = pd.read_csv(primary_key_file)
            # æ”¯æŒä»¥ä¸‹ä¸¤ç§æ ¼å¼ï¼š
            # 1. ç¬¬ä¸€åˆ—å« primary_keyï¼Œå†…å®¹æ˜¯ä¸»é”®å
            # 2. åªæœ‰ä¸€ä¸ªå­—æ®µï¼ˆæ— è¡¨å¤´ï¼‰ï¼Œç¬¬ä¸€è¡Œå†…å®¹æ˜¯ä¸»é”®å
            if 'primary_key' in df_pk.columns:
                primary_key = str(df_pk['primary_key'].iloc[0]).strip()
            else:
                primary_key = str(df_pk.iloc[0, 0]).strip()
            print(f"ä»é…ç½®æ–‡ä»¶è¯»å–ä¸»é”®å­—æ®µ: {primary_key}")
            return primary_key
        except Exception as e:
            print(f"è¯»å–ä¸»é”®é…ç½®æ–‡ä»¶ {primary_key_file} æ—¶å‡ºé”™: {e}")
            # ç»§ç»­è‡ªåŠ¨è¯†åˆ«

    # è‡ªåŠ¨è¯†åˆ«ä¸»é”®å­—æ®µ
    primary_key_candidates = ['cusno', 'ci', 'å®¢æˆ·ç¼–å·', 'å®¢æˆ·å·']
    for file_name, dataframes in all_data.items():
        for df in dataframes:
            for col in df.columns:
                if any(candidate in col.lower() for candidate in primary_key_candidates):
                    print(f"è‡ªåŠ¨è¯†åˆ«åˆ°ä¸»é”®å­—æ®µ: {col}")
                    return col
    print("æœªæ‰¾åˆ°ä¸»é”®å­—æ®µï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºä¸»é”®")
    return 'index'

def process_all_excel_files():
    """
    é€šç”¨æœºå™¨å­¦ä¹ å®½è¡¨è½¬åŒ–å™¨
    1. è¯»å–data/ç›®å½•ä¸‹çš„æ‰€æœ‰Excelæ–‡ä»¶
    2. è‡ªåŠ¨å­¦ä¹ å„æ–‡ä»¶çš„å­—æ®µï¼Œåˆ†æå„ç§ç»´åº¦
    3. æ ¹æ®å„ç§ç»´åº¦å¯¹æ•°æ®è¿›è¡Œé€è§†ï¼Œç”Ÿæˆç”¨äºæœºå™¨å­¦ä¹ çš„å®½è¡¨
    4. è‡ªåŠ¨è®¡ç®—ä¸€äº›è¡ç”Ÿç‰¹å¾
    5. å°†æœ€ç»ˆçš„å®½è¡¨å’Œå­—æ®µæè¿°åˆ†åˆ«ä¿å­˜ä¸ºCSVæ–‡ä»¶åˆ°output/ç›®å½•ä¸­
    """
    # è·å–æ‰€æœ‰Excelæ–‡ä»¶
    excel_files = glob.glob(os.path.join(input_dir, "*.xlsx"))
    print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
    
    # å­˜å‚¨æ‰€æœ‰æ•°æ®çš„å­—å…¸
    all_data = defaultdict(list)
    
    # è¯»å–æ‰€æœ‰Excelæ–‡ä»¶
    for file_path in excel_files:
        print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        try:
            # è¯»å–Excelæ–‡ä»¶çš„æ‰€æœ‰sheet
            excel_data = pd.read_excel(file_path, sheet_name=None)
            
            # éå†æ¯ä¸ªsheet
            for sheet_name, df in excel_data.items():
                # æ ‡å‡†åŒ–åˆ—åï¼ˆè½¬æ¢ä¸ºå°å†™å¹¶å»é™¤ç©ºæ ¼ï¼‰
                df.columns = [col.strip().lower() if isinstance(col, str) else col for col in df.columns]
                
                # æ·»åŠ æ–‡ä»¶æ¥æºåˆ—
                df['source_file'] = os.path.basename(file_path)
                df['sheet_name'] = sheet_name
                
                # å­˜å‚¨æ•°æ®
                all_data[os.path.basename(file_path)].append(df)
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return all_data

def analyze_fields_and_dimensions(all_data):
    """
    è‡ªåŠ¨å­¦ä¹ å­—æ®µå’Œåˆ†æç»´åº¦
    """
    # åˆ†ææ‰€æœ‰å­—æ®µ
    field_analysis = {}
    dimension_analysis = {}
    
    for file_name, dataframes in all_data.items():
        for df in dataframes:
            # åˆ†æå­—æ®µ
            for col in df.columns:
                if col not in field_analysis:
                    field_analysis[col] = {
                        'data_types': set(),
                        'files': set(),
                        'sample_values': set()
                    }
                
                # è®°å½•æ•°æ®ç±»å‹
                field_analysis[col]['data_types'].add(str(df[col].dtype))
                
                # è®°å½•å‡ºç°çš„æ–‡ä»¶
                field_analysis[col]['files'].add(file_name)
                
                # è®°å½•æ ·æœ¬å€¼
                sample_vals = df[col].dropna().unique()[:5]  # å–å‰5ä¸ªå”¯ä¸€å€¼ä½œä¸ºæ ·æœ¬
                field_analysis[col]['sample_values'].update(sample_vals)
            
            # åˆ†æå¯èƒ½çš„ç»´åº¦ï¼ˆæ•°å€¼å‹å­—æ®µé™¤å¤–ï¼‰
            for col in df.columns:
                if (pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col])) and col not in ['source_file', 'sheet_name']:
                    if col not in dimension_analysis:
                        dimension_analysis[col] = {
                            'files': set(),
                            'values': set()
                        }
                    
                    # è®°å½•å‡ºç°çš„æ–‡ä»¶
                    dimension_analysis[col]['files'].add(file_name)
                    
                    # è®°å½•å”¯ä¸€å€¼
                    unique_vals = df[col].dropna().unique()
                    dimension_analysis[col]['values'].update(unique_vals[:100])  # é™åˆ¶å­˜å‚¨æ•°é‡
    
    return field_analysis, dimension_analysis

def create_wide_table(all_data, dimension_analysis):
    """
    æ ¹æ®ç»´åº¦åˆ›å»ºå®½è¡¨
    """
    # ä¸»é”®å­—æ®µ
    primary_key = get_primary_key(all_data)
    print(f"ä½¿ç”¨ä¸»é”®å­—æ®µ: {primary_key}")
    
    # åˆ›å»ºå®½è¡¨
    wide_dfs = []
    
    # éå†æ¯ä¸ªæ–‡ä»¶å’Œsheet
    for file_name, dataframes in all_data.items():
        for df in dataframes:
            # å¦‚æœæ²¡æœ‰ä¸»é”®å­—æ®µï¼Œåˆ™æ·»åŠ ç´¢å¼•
            if primary_key not in df.columns:
                df[primary_key] = df.index.astype(str)
            
            # è·å–æ•°å€¼å‹å­—æ®µç”¨äºèšåˆ
            numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col != primary_key]
            
            # è·å–ç»´åº¦å­—æ®µ
            dimension_cols = [col for col in df.columns 
                            if (pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col])) 
                            and col not in ['source_file', 'sheet_name', primary_key]]
            
            # å¯¹æ¯ä¸ªç»´åº¦å­—æ®µè¿›è¡Œé€è§†
            for dim_col in dimension_cols:
                if dim_col in dimension_analysis and len(dimension_analysis[dim_col]['values']) <= 50:  # é™ä½ç»´åº¦å€¼æ•°é‡é™åˆ¶
                    print(f"æ­£åœ¨å¤„ç†ç»´åº¦: {dim_col} (å”¯ä¸€å€¼æ•°é‡: {len(dimension_analysis[dim_col]['values'])})")
                    
                    for numeric_col in numeric_cols:
                        try:
                            max_rows = 50000
                            if len(df) > max_rows:
                                df_subset = df.iloc[:max_rows]
                            else:
                                df_subset = df
                            
                            # åˆ›å»ºé€è§†è¡¨
                            pivot = df_subset.pivot_table(
                                index=primary_key,
                                columns=dim_col,
                                values=numeric_col,
                                aggfunc='sum',
                                fill_value=0
                            )
                            
                            # é‡å‘½ååˆ—
                            pivot.columns = [f"{numeric_col}_{dim_col}_{col}" for col in pivot.columns]
                            
                            # é‡ç½®ç´¢å¼•
                            pivot = pivot.reset_index()
                            
                            # æ·»åŠ æ ‡è¯†åˆ—
                            pivot['source_file'] = file_name
                            pivot['dimension'] = dim_col
                            pivot['value_field'] = numeric_col
                            
                            wide_dfs.append(pivot)
                            
                            # ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
                            process = psutil.Process(os.getpid())
                            memory_info = process.memory_info()
                            if memory_info.rss / psutil.virtual_memory().total > 0.7:
                                gc.collect()
                                print("æ‰§è¡Œåƒåœ¾å›æ”¶")
                                
                            del pivot
                            del df_subset
                        except Exception as e:
                            print(f"åˆ›å»ºé€è§†è¡¨æ—¶å‡ºé”™ ({dim_col}, {numeric_col}): {e}")
                            gc.collect()
                else:
                    if dim_col in dimension_analysis:
                        print(f"ç»´åº¦ {dim_col} çš„å”¯ä¸€å€¼æ•°é‡è¿‡å¤š ({len(dimension_analysis[dim_col]['values'])})ï¼Œè·³è¿‡å¤„ç†")
                    else:
                        print(f"ç»´åº¦ {dim_col} æœªåœ¨ç»´åº¦åˆ†æä¸­æ‰¾åˆ°")
    
    # åˆå¹¶æ‰€æœ‰å®½è¡¨
    if wide_dfs:
        print(f"å…±æœ‰ {len(wide_dfs)} ä¸ªæ•°æ®æ¡†éœ€è¦åˆå¹¶")
        
        batch_size = 10
        merged_dfs = []
        
        for i in range(0, len(wide_dfs), batch_size):
            batch = wide_dfs[i:i+batch_size]
            batch_merged = pd.concat(batch, axis=0, sort=False)
            merged_dfs.append(batch_merged)
            print(f"åˆå¹¶æ‰¹æ¬¡ {i//batch_size + 1}/{(len(wide_dfs)-1)//batch_size + 1}")
            gc.collect()
        
        merged_df = pd.concat(merged_dfs, axis=0, sort=False)
        del merged_dfs
        gc.collect()
        
        numeric_columns = [col for col in merged_df.columns if pd.api.types.is_numeric_dtype(merged_df[col])]
        other_columns = [col for col in merged_df.columns if col not in numeric_columns]
        
        max_agg_cols = 1000
        if len(numeric_columns) > max_agg_cols:
            print(f"æ•°å€¼åˆ—æ•°é‡è¿‡å¤š ({len(numeric_columns)})ï¼Œä»…å¤„ç†å‰ {max_agg_cols} åˆ—")
            numeric_columns = numeric_columns[:max_agg_cols]
        
        agg_dict = {}
        for col in numeric_columns:
            if col != primary_key:
                agg_dict[col] = 'sum'
        
        for col in other_columns:
            if col != primary_key and len(agg_dict) < max_agg_cols:
                agg_dict[col] = 'first'
        
        print("æ­£åœ¨è¿›è¡Œåˆ†ç»„èšåˆ...")
        final_df = merged_df.groupby(primary_key).agg(agg_dict).reset_index()
        del merged_df
        gc.collect()
        
        final_numeric_columns = [col for col in final_df.columns if pd.api.types.is_numeric_dtype(final_df[col])]
        final_df[final_numeric_columns] = final_df[final_numeric_columns].fillna(0)
        
        final_other_columns = [col for col in final_df.columns if col not in final_numeric_columns]
        for col in final_other_columns:
            if col != primary_key:
                final_df[col] = final_df[col].fillna('Unknown')
        
        if primary_key in final_df.columns:
            final_df.rename(columns={primary_key: 'Id'}, inplace=True)
        
        if 'source_file' in final_df.columns:
            final_df.drop(columns=['source_file'], inplace=True)
        
        return final_df
    else:
        print("æœªèƒ½åˆ›å»ºä»»ä½•é€è§†è¡¨")
        return pd.DataFrame()

def calculate_derived_features(wide_df):
    """
    è®¡ç®—è¡ç”Ÿç‰¹å¾
    """
    if wide_df.empty:
        return wide_df
    
    # è·å–æ‰€æœ‰æ•°å€¼å‹å­—æ®µï¼ˆæ’é™¤æ ‡è¯†åˆ—ï¼‰
    numeric_cols = [col for col in wide_df.columns if pd.api.types.is_numeric_dtype(wide_df[col])]
    exclude_cols = ['ci', 'cusno', 'index']  # å¯èƒ½çš„ä¸»é”®åˆ—
    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    max_feature_cols = 500
    if len(numeric_cols) > max_feature_cols:
        print(f"æ•°å€¼åˆ—æ•°é‡è¿‡å¤š ({len(numeric_cols)})ï¼Œä»…ä½¿ç”¨å‰ {max_feature_cols} åˆ—è®¡ç®—è¡ç”Ÿç‰¹å¾")
        numeric_cols = numeric_cols[:max_feature_cols]
    
    new_features = {}
    
    if len(numeric_cols) > 0:
        print("æ­£åœ¨è®¡ç®—æ€»å’Œç‰¹å¾...")
        new_features['total_sum'] = wide_df[numeric_cols].sum(axis=1)
    
    if len(numeric_cols) > 1:
        print("æ­£åœ¨è®¡ç®—ç»Ÿè®¡ç‰¹å¾...")
        new_features['total_mean'] = wide_df[numeric_cols].mean(axis=1)
        new_features['total_std'] = wide_df[numeric_cols].std(axis=1)
        new_features['total_max'] = wide_df[numeric_cols].max(axis=1)
        new_features['total_min'] = wide_df[numeric_cols].min(axis=1)
    
    if new_features:
        new_features_df = pd.DataFrame(new_features, index=wide_df.index)
        wide_df = pd.concat([wide_df, new_features_df], axis=1)
    
    print(f"è®¡ç®—äº†è¡ç”Ÿç‰¹å¾ï¼Œå½“å‰å®½è¡¨å½¢çŠ¶: {wide_df.shape}")
    return wide_df

def generate_feature_dictionary(wide_df):
    """
    ç”Ÿæˆå­—æ®µæè¿°å­—å…¸
    """
    feature_dict = []
    
    for col in wide_df.columns:
        # ç”¨ pandas API åˆ¤æ–­å­—æ®µç±»å‹
        if pd.api.types.is_numeric_dtype(wide_df[col]):
            feature_type = 'continuous'
        elif pd.api.types.is_string_dtype(wide_df[col]) or pd.api.types.is_object_dtype(wide_df[col]):
            if wide_df[col].nunique() <= 10:
                feature_type = 'category'
            else:
                feature_type = 'text'
        else:
            feature_type = 'other'
        
        feature_dict.append({
            'feature_name': col,
            'feature_type': feature_type
        })
    
    return pd.DataFrame(feature_dict)

def main():
    # å¤„ç†æ‰€æœ‰Excelæ–‡ä»¶
    all_data = process_all_excel_files()
    
    # åˆ†æå­—æ®µå’Œç»´åº¦
    field_analysis, dimension_analysis = analyze_fields_and_dimensions(all_data)
    print(f"åˆ†æäº† {len(field_analysis)} ä¸ªå­—æ®µ, {len(dimension_analysis)} ä¸ªç»´åº¦")
    
    # åˆ›å»ºå®½è¡¨
    wide_df = create_wide_table(all_data, dimension_analysis)
    print(f"å®½è¡¨å½¢çŠ¶: {wide_df.shape}")
    
    # è®¡ç®—è¡ç”Ÿç‰¹å¾
    wide_df = calculate_derived_features(wide_df)
    
    # ç”Ÿæˆå­—æ®µå­—å…¸
    feature_dict_df = generate_feature_dictionary(wide_df)
    
    # ä¿å­˜ç»“æœ
    output_csv = os.path.join(output_dir, "ml_wide_table.csv")
    output_dict = os.path.join(output_dir, "feature_dictionary.csv")
    
    wide_df.to_csv(output_csv, index=False, encoding='utf-8')
    feature_dict_df.to_csv(output_dict, index=False, encoding='utf-8')
    
    # å¤åˆ¶ feature_dictionary.csv åˆ° config/features.csvï¼Œå¹¶åˆ é™¤ç±»å‹ä¸º text çš„è®°å½•è¡Œ
    config_dir = "config"
    os.makedirs(config_dir, exist_ok=True)
    config_file = os.path.join(config_dir, "features.csv")
    
    feature_dict_df_filtered = feature_dict_df[feature_dict_df['feature_type'] != 'text']
    feature_dict_df_filtered.to_csv(config_file, index=False, encoding='utf-8')
    
    print(f"âœ… å®½è¡¨å·²ä¿å­˜åˆ°: {output_csv} (UTF-8)")
    print(f"âœ… å­—æ®µæè¿°å·²ä¿å­˜åˆ°: {output_dict} (UTF-8)")
    print(f"âœ… è¿‡æ»¤åçš„å­—æ®µæè¿°å·²ä¿å­˜åˆ°: {config_file} (UTF-8)")
    print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {wide_df.shape[0]} è¡Œ, {wide_df.shape[1]} åˆ—")

if __name__ == "__main__":
    main()

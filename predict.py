import os
#os.environ["NUMBA_DISABLE_TBB"] = "1"
import pandas as pd
import numpy as np
from base_model_processor import BaseModelProcessor
import warnings
import joblib
import logging
import lightgbm as lgb
from pathlib import Path
import sys

warnings.filterwarnings('ignore')

# ========== åŠ è½½æ¨¡å‹å’Œå…ƒæ•°æ® ==========
MODEL_DIR = 'output'

def load_models():
    required_files = [
        'gbdt_model.pkl',
        'lr_model.pkl',
        'train_feature_names.csv',
        'category_features.csv',
        'continuous_features.csv',
        'actual_n_estimators.csv'
    ]
    model_dir = Path(MODEL_DIR)
    for f in required_files:
        if not (model_dir / f).exists():
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å¿…éœ€æ–‡ä»¶: {model_dir / f}")

    gbdt_model = joblib.load(model_dir / 'gbdt_model.pkl')
    lr_model = joblib.load(model_dir / 'lr_model.pkl')
    train_feature_names = pd.read_csv(model_dir / 'train_feature_names.csv')['feature'].tolist()
    category_features = pd.read_csv(model_dir / 'category_features.csv')['feature'].tolist()
    continuous_features = pd.read_csv(model_dir / 'continuous_features.csv')['feature'].tolist()
    actual_n_estimators = pd.read_csv(model_dir / 'actual_n_estimators.csv')['n_estimators'].iloc[0]
    category_prefixes = [col + "_" for col in category_features]

    logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå®é™…æ ‘æ•°é‡: {actual_n_estimators}")
    return {
        'gbdt_model': gbdt_model,
        'lr_model': lr_model,
        'train_feature_names': train_feature_names,
        'category_features': category_features,
        'continuous_features': continuous_features,
        'actual_n_estimators': actual_n_estimators,
        'category_prefixes': category_prefixes
    }


# ========== å·¥å…·å‡½æ•°ï¼šè§£æå¶å­è·¯å¾„ ==========
def get_leaf_path_enhanced(booster, tree_index, leaf_index, feature_names, category_prefixes):
    try:
        model_dump = booster.dump_model()
        if tree_index >= len(model_dump['tree_info']):
            return None
        tree_info = model_dump['tree_info'][tree_index]['tree_structure']
    except Exception as e:
        logging.warning(f"è§£ææ ‘ç»“æ„å¤±è´¥: {e}")
        return None

    node_stack = [(tree_info, [])]
    while node_stack:
        node, current_path = node_stack.pop()
        if 'leaf_index' in node and node['leaf_index'] == leaf_index:
            return current_path
        if 'split_feature' in node:
            feat_idx = node['split_feature']
            feat_name = feature_names[feat_idx] if feat_idx < len(feature_names) else f"Feature_{feat_idx}"
            threshold = node.get('threshold', 0.0)
            decision_type = node.get('decision_type', '<=')

            is_category = False
            original_col = None
            category_value = None
            for prefix in category_prefixes:
                if feat_name.startswith(prefix):
                    is_category = True
                    original_col = prefix.rstrip('_')
                    category_value = feat_name[len(prefix):]
                    break

            if is_category:
                right_rule = f"{original_col} == '{category_value}'"
                left_rule = f"{original_col} != '{category_value}'"
            else:
                if decision_type in ('<=', 'no_greater'):
                    right_rule = f"{feat_name} > {threshold:.4f}"
                    left_rule = f"{feat_name} <= {threshold:.4f}"
                else:
                    right_rule = f"{feat_name} {decision_type} {threshold:.4f}"
                    left_rule = f"{feat_name} not {decision_type} {threshold:.4f}"

            if 'right_child' in node:
                node_stack.append((node['right_child'], current_path + [right_rule]))
            if 'left_child' in node:
                node_stack.append((node['left_child'], current_path + [left_rule]))
    return None


# ========== é¢„å¤„ç†å•æ ·æœ¬ ==========
def preprocess_single_sample(sample_dict, continuous_features, category_features, train_feature_names):
    sample_df = pd.DataFrame([sample_dict])
    # è¿ç»­ç‰¹å¾
    for col in continuous_features:
        if col not in sample_df.columns:
            sample_df[col] = -1
        else:
            sample_df[col] = pd.to_numeric(sample_df[col], errors='coerce').fillna(-1)
    # åˆ†ç±»ç‰¹å¾
    all_dummies_list = []
    for col in category_features:
        if col in sample_df.columns:
            val = sample_df[col].iloc[0]
            sample_df[col] = "-1" if pd.isna(val) or val == "" else str(val)
        else:
            sample_df[col] = "-1"
        sample_df[col] = sample_df[col].astype('category')
        dummies = pd.get_dummies(sample_df[col], prefix=col)
        # è¡¥é½è®­ç»ƒæ—¶çš„ dummy åˆ—
        missing_cols = [train_col for train_col in train_feature_names if train_col.startswith(col + "_") and train_col not in dummies.columns]
        if missing_cols:
            missing_df = pd.DataFrame(0, index=dummies.index, columns=missing_cols)
            dummies = pd.concat([dummies, missing_df], axis=1)
        all_dummies_list.append(dummies)
    # åˆå¹¶
    if all_dummies_list:
        dummies_combined = pd.concat(all_dummies_list, axis=1)
        # ç§»é™¤dummies_combinedä¸­çš„é‡å¤åˆ—
        dummies_combined = dummies_combined.loc[:, ~dummies_combined.columns.duplicated()]
        sample_df = pd.concat([sample_df.drop(columns=category_features), dummies_combined], axis=1)
    else:
        sample_df = sample_df.drop(columns=category_features)
    # è¡¥é½æ‰€æœ‰è®­ç»ƒç‰¹å¾
    missing_final_cols = set(train_feature_names) - set(sample_df.columns)
    if missing_final_cols:
        missing_final_df = pd.DataFrame(0, index=sample_df.index, columns=list(missing_final_cols))
        sample_df = pd.concat([sample_df, missing_final_df], axis=1)
    # ç§»é™¤sample_dfä¸­çš„é‡å¤åˆ—
    sample_df = sample_df.loc[:, ~sample_df.columns.duplicated()]
    return sample_df.reindex(columns=train_feature_names, fill_value=0)


# ========== æ ¸å¿ƒé¢„æµ‹å‡½æ•° ==========
def predict_core(sample_df_list, models, return_explanation=True, generate_plot=False, calculate_shap=False):
    """
    ä¸ app.py ä¸­çš„ predict_core å®Œå…¨ä¸€è‡´
    """
    if not sample_df_list:
        return []

    from sklearn.linear_model import LogisticRegression
    import numpy as np

    batch_df = pd.concat(sample_df_list, ignore_index=True)
    gbdt_model = models['gbdt_model']
    lr_model = models['lr_model']
    train_feature_names = models['train_feature_names']
    actual_n_estimators = models['actual_n_estimators']
    category_prefixes = models['category_prefixes']

    # Step 1: GBDT å¶å­ç´¢å¼•
    leaf_indices_batch = gbdt_model.booster_.predict(batch_df.values, pred_leaf=True)
    n_trees = actual_n_estimators

    # Step 2: å¶å­ One-Hot
    leaf_dummies_list = []
    for i in range(n_trees):
        leaf_col_name = f"gbdt_leaf_{i}"
        leaf_series = pd.Series(leaf_indices_batch[:, i], name=leaf_col_name)
        dummies = pd.get_dummies(leaf_series, prefix=leaf_col_name)
        leaf_dummies_list.append(dummies)
    leaf_dummies_combined = pd.concat(leaf_dummies_list, axis=1) if leaf_dummies_list else pd.DataFrame()

    lr_feature_names = getattr(lr_model, 'feature_names_in_', [f"feature_{i}" for i in range(len(lr_model.coef_[0]))])
    missing_leaf_cols = set(lr_feature_names) - set(leaf_dummies_combined.columns)
    if missing_leaf_cols:
        missing_leaf_df = pd.DataFrame(0, index=leaf_dummies_combined.index, columns=list(missing_leaf_cols))
        leaf_dummies_combined = pd.concat([leaf_dummies_combined, missing_leaf_df], axis=1)
    leaf_dummies_combined = leaf_dummies_combined.reindex(columns=lr_feature_names, fill_value=0)

    # Step 3: LR æ¦‚ç‡
    probabilities = lr_model.predict_proba(leaf_dummies_combined)[:, 1]

    if not return_explanation or not calculate_shap:
        # å¦‚æœä¸è¿”å›è§£é‡Šæˆ–ä¸è®¡ç®—SHAPå€¼ï¼Œç›´æ¥è¿”å›æ¦‚ç‡
        return [{"probability": round(float(p), 4), "explanation": None} for p in probabilities]

    # Step 4: ç‰¹å¾è´¡çŒ®è§£é‡Šï¼ˆä½¿ç”¨LightGBMå†…ç½®åŠŸèƒ½æ›¿ä»£SHAPï¼‰
    contrib_values_batch = None
    if calculate_shap:
        try:
            # ä½¿ç”¨LightGBMå†…ç½®çš„pred_contribåŠŸèƒ½è®¡ç®—ç‰¹å¾è´¡çŒ®
            contrib_values_batch = gbdt_model.booster_.predict(batch_df.values, pred_contrib=True)
            # contrib_values_batchçš„å½¢çŠ¶ä¸º (n_samples, n_features + 1)
            # æœ€åä¸€åˆ—æ˜¯æœŸæœ›å€¼ï¼ˆbase valueï¼‰ï¼Œå‰é¢çš„åˆ—æ˜¯å„ç‰¹å¾çš„è´¡çŒ®å€¼
        except Exception as e:
            logging.error(f"ç‰¹å¾è´¡çŒ®è®¡ç®—å¤±è´¥: {e}")
            # è¿”å›ç©ºè§£é‡Š
            return [{
                "probability": round(float(probabilities[i]), 4),
                "explanation": {
                    "important_features": [],
                    "shap_plot_base64": "",
                    "top_rules": [],
                    "feature_based_rules": []
                }
            } for i in range(len(sample_df_list))]
    else:
        # å¦‚æœä¸è®¡ç®—ç‰¹å¾è´¡çŒ®ï¼Œè¿”å›ç©ºè§£é‡Š
        return [{
            "probability": round(float(probabilities[i]), 4),
            "explanation": {
                "important_features": [],
                "shap_plot_base64": "",
                "top_rules": [],
                "feature_based_rules": []
            }
        } for i in range(len(sample_df_list))]

    results = []
    for idx in range(len(sample_df_list)):
        # è·å–å½“å‰æ ·æœ¬çš„ç‰¹å¾è´¡çŒ®å€¼ï¼ˆæ’é™¤æœ€åçš„æœŸæœ›å€¼åˆ—ï¼‰
        contrib_vals = contrib_values_batch[idx, :-1]
        feature_imp = [(train_feature_names[i], float(contrib_vals[i])) for i in range(len(contrib_vals))]
        feature_imp.sort(key=lambda x: abs(x[1]), reverse=True)
        important_features = [{"feature": feat, "shap_value": round(val, 4)} for feat, val in feature_imp[:3]]
        top_contrib_features = [feat for feat, val in feature_imp[:5]]
        leaf_indices = leaf_indices_batch[idx]

        # åŸå§‹è·¯å¾„è§„åˆ™
        path_rules = []
        for tree_idx in range(min(3, len(leaf_indices))):
            leaf_idx = leaf_indices[tree_idx]
            rule = get_leaf_path_enhanced(gbdt_model.booster_, tree_idx, leaf_idx, train_feature_names, category_prefixes)
            if rule:
                path_rules.extend(rule[:3])
        seen = set()
        unique_path_rules = []
        for r in path_rules:
            if r not in seen:
                seen.add(r)
                unique_path_rules.append(r)
        top_rules = unique_path_rules[:5]

        # ç‰¹å¾å…³è”è§„åˆ™
        feature_rules = []
        for tree_idx in range(min(10, len(leaf_indices))):
            leaf_idx = leaf_indices[tree_idx]
            rule = get_leaf_path_enhanced(gbdt_model.booster_, tree_idx, leaf_idx, train_feature_names, category_prefixes)
            if rule:
                for r in rule:
                    for feat in top_contrib_features:
                        if feat in r or (feat.split('_')[0] + " " in r) or (feat.split('_')[0] + " ==" in r):
                            contrib_val = next((val for f, val in feature_imp if f == feat), 0)
                            rule_with_contrib = f"{r} (è´¡çŒ®å€¼: {contrib_val:+.4f})"
                            if rule_with_contrib not in feature_rules:
                                feature_rules.append(rule_with_contrib)
                            break
                    if len(feature_rules) >= 5:
                        break
            if len(feature_rules) >= 5:
                break

        # ç”Ÿæˆå›¾ï¼ˆä»…ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
        # ç”±äºä¸å†ä½¿ç”¨SHAPï¼Œæ­¤éƒ¨åˆ†å°†ç”Ÿæˆä¸€ä¸ªç®€å•çš„ç‰¹å¾è´¡çŒ®å›¾
        shap_plot_b64 = ""
        if generate_plot and idx == 0:
            try:
                import matplotlib.pyplot as plt
                import base64
                import io
                
                # è·å–å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾åŠå…¶è´¡çŒ®å€¼
                top_features_plot = feature_imp[:10]
                features_plot = [feat for feat, _ in top_features_plot]
                contribs_plot = [val for _, val in top_features_plot]
                
                # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
                plt.figure(figsize=(10, 6))
                y_pos = range(len(features_plot))
                colors = ['green' if x > 0 else 'red' for x in contribs_plot]
                plt.barh(y_pos, contribs_plot, color=colors)
                plt.yticks(y_pos, features_plot)
                plt.xlabel('Feature Contribution')
                plt.title('Top 10 Feature Contributions for Prediction')
                plt.tight_layout()
                
                buf = io.BytesIO()
                plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
                plt.close()
                buf.seek(0)
                shap_plot_b64 = "image/png;base64," + base64.b64encode(buf.read()).decode('utf-8')
                buf.close()
            except Exception as e:
                logging.warning(f"ç‰¹å¾è´¡çŒ®å›¾ç”Ÿæˆå¤±è´¥: {e}")

        explanation = {
            "important_features": important_features,
            "shap_plot_base64": shap_plot_b64,
            "top_rules": top_rules,
            "feature_based_rules": feature_rules[:5]
        }

        results.append({
            "probability": round(float(probabilities[idx]), 4),
            "explanation": explanation
        })

    return results


class PredictModel(BaseModelProcessor):
        
    def load_models(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        return super().load_models()
    
    def load_feature_config(self):
        """åŠ è½½ç‰¹å¾é…ç½®"""
        return super().load_feature_config()
    
    def prepare_predict_data(self, predict_data_path):
        """å‡†å¤‡é¢„æµ‹æ•°æ®"""
        try:
            # åŠ è½½é¢„æµ‹æ•°æ®
            predict_df = pd.read_csv(predict_data_path)
            print(f"âœ… é¢„æµ‹æ•°æ®å·²åŠ è½½: {predict_data_path}, å½¢çŠ¶: {predict_df.shape}")
            
            # ç¡®ä¿Idåˆ—å­˜åœ¨
            if 'Id' not in predict_df.columns:
                print("âŒ é¢„æµ‹æ•°æ®ä¸­ç¼ºå°‘Idåˆ—")
                return None
            
            # è·å–é¢„æµ‹æ•°æ®ä¸­çš„ç‰¹å¾ï¼ˆä¸åŒ…æ‹¬Idï¼‰
            predict_features = set(predict_df.columns.tolist()) - {'Id'}
            
            # è·å–è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾
            train_features = set(self.train_feature_names)
            
            # æ£€æŸ¥ç¼ºå°‘çš„ç‰¹å¾
            missing_features = train_features - predict_features
            
            if missing_features:
                print(f"â„¹ï¸  ä¸ºä¸è®­ç»ƒæ•°æ®ç‰¹å¾å¯¹é½ï¼Œå°†å¡«å…… {len(missing_features)} ä¸ªç¼ºå¤±ç‰¹å¾")
                print("   åŸå› ï¼šè®­ç»ƒæ—¶æŸäº›ç±»åˆ«ç‰¹å¾ç»One-Hotç¼–ç åäº§ç”Ÿäº†æ›´å¤šç‰¹å¾ï¼Œé¢„æµ‹æ•°æ®ä¸­ç¼ºå°‘è¿™äº›ç¼–ç åçš„ç‰¹å¾")
                # ä¸ºç¼ºå°‘çš„ç‰¹å¾æ·»åŠ é»˜è®¤å€¼0
                for feature in missing_features:
                    predict_df[feature] = 0
            
            # ç§»é™¤å¤šä½™çš„ç‰¹å¾
            extra_features = predict_features - train_features
            
            if extra_features:
                print(f"â„¹ï¸  ç§»é™¤ {len(extra_features)} ä¸ªè®­ç»ƒæ—¶æœªä½¿ç”¨çš„ç‰¹å¾")
                print("   åŸå› ï¼šé¢„æµ‹æ•°æ®ä¸­æŸäº›ç±»åˆ«ç‰¹å¾çš„å–å€¼èŒƒå›´ä¸è®­ç»ƒæ•°æ®ä¸åŒï¼Œäº§ç”Ÿäº†é¢å¤–çš„One-Hotç¼–ç ç‰¹å¾")
                predict_df = predict_df.drop(columns=list(extra_features))
            
            # ç¡®ä¿ç‰¹å¾é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
            feature_columns = [col for col in self.train_feature_names if col in predict_df.columns]
            final_columns = ['Id'] + feature_columns
            predict_df = predict_df[final_columns]
            
            print(f"âœ… é¢„æµ‹æ•°æ®å‡†å¤‡å®Œæˆ, æœ€ç»ˆå½¢çŠ¶: {predict_df.shape}")
            return predict_df
        except Exception as e:
            print(f"âŒ å‡†å¤‡é¢„æµ‹æ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    def predict_with_explanation(self, predict_df, calculate_shap=False):
        """è¿›è¡Œé¢„æµ‹å¹¶ç”Ÿæˆè§£é‡Šæ€§ä¿¡æ¯"""
        try:
            # åŠ è½½æ¨¡å‹å’Œå…ƒæ•°æ®
            models = load_models()
            
            # é¢„å¤„ç†æ•°æ®
            processed_rows = []
            for _, row in predict_df.iterrows():
                processed = preprocess_single_sample(
                    row.to_dict(),
                    models['continuous_features'],
                    models['category_features'],
                    models['train_feature_names']
                )
                # ä¿®å¤å¯èƒ½çš„é‡å¤åˆ—åé—®é¢˜
                processed = processed.loc[:, ~processed.columns.duplicated()]
                processed_rows.append(processed)
            
            # æ£€æŸ¥processed_rowsä¸­çš„DataFrameæ˜¯å¦æœ‰é‡å¤åˆ—å
            for i, df in enumerate(processed_rows):
                if df.columns.duplicated().any():
                    print(f"ç¬¬{i}ä¸ªæ ·æœ¬å­˜åœ¨é‡å¤åˆ—å")
                    duplicated_cols = df.columns[df.columns.duplicated()]
                    print(f"é‡å¤çš„åˆ—å: {duplicated_cols}")
                    # ç§»é™¤é‡å¤åˆ—ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
                    processed_rows[i] = df.loc[:, ~df.columns.duplicated()]
            
            # ä½¿ç”¨predict_coreè¿›è¡Œé¢„æµ‹ï¼Œè¿”å›è§£é‡Šæ€§ä¿¡æ¯
            results = predict_core(processed_rows, models, return_explanation=True, generate_plot=False, calculate_shap=calculate_shap)
            
            # æ„é€  CSV ç»“æœï¼ˆå®Œå…¨å¤åˆ» app.pyï¼‰
            csv_results = []
            for r in results:
                exp = r["explanation"]
                # æ£€æŸ¥expæ˜¯å¦ä¸ºNone
                if exp is None:
                    top_features = ""
                    top_rules = ""
                else:
                    top_features = "; ".join([f"{feat['feature']}({feat['shap_value']:+.3f})" for feat in exp["important_features"]]) if exp.get("important_features") else ""
                    top_rules = "; ".join(exp["top_rules"][:3]) if exp.get("top_rules") else ""
                csv_results.append({"probability": r["probability"], "top_features": top_features, "top_rules": top_rules})

            # ç”Ÿæˆç»“æœ DataFrame
            result_df = pd.DataFrame()
            result_df['Id'] = predict_df['Id'].values
            result_df['PredictedProb'] = [r['probability'] for r in csv_results]
            result_df['top_features'] = [r['top_features'] for r in csv_results]
            result_df['top_rules'] = [r['top_rules'] for r in csv_results]
            
            return result_df
        except Exception as e:
            print(f"âŒ é¢„æµ‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_prediction(self, predict_data_path, output_path="output/prediction_results.csv", calculate_shap=False):
        """è¿è¡Œå®Œæ•´çš„é¢„æµ‹æµç¨‹"""
        print("=== å¼€å§‹é¢„æµ‹æµç¨‹ ===")
        
        # åŠ è½½æ¨¡å‹
        if not self.load_models():
            return False
        
        # åŠ è½½ç‰¹å¾é…ç½®
        if not self.load_feature_config():
            return False
        
        # å‡†å¤‡é¢„æµ‹æ•°æ®
        predict_df = self.prepare_predict_data(predict_data_path)
        if predict_df is None:
            return False
        
        # è¿›è¡Œé¢„æµ‹å¹¶ç”Ÿæˆè§£é‡Šæ€§ä¿¡æ¯
        result_df = self.predict_with_explanation(predict_df, calculate_shap)
        if result_df is None:
            return False
        
        # é‡å‘½ååˆ—ä»¥åŒ¹é…local_batch_predict.pyçš„è¾“å‡ºæ ¼å¼
        # æ³¨æ„ï¼šæˆ‘ä»¬ä¿ç•™åŸå§‹åˆ—åï¼Œä¸è¿›è¡Œé‡å‘½åï¼Œä»¥ç¡®ä¿ä¸è¾“å‡ºæ–‡ä»¶æ ¼å¼ä¸€è‡´
        # result_df = result_df.rename(columns={
        #     'PredictedProb': 'prediction_probability',
        #     'top_features': 'top_3_features',
        #     'top_rules': 'top_3_rules'
        # })
        
        # ä¿å­˜ç»“æœ
        try:
            result_df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {output_path}")
            print(f"ğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡: æœ€å°å€¼={result_df['PredictedProb'].min():.4f}, æœ€å¤§å€¼={result_df['PredictedProb'].max():.4f}, å¹³å‡å€¼={result_df['PredictedProb'].mean():.4f}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜é¢„æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
            return False

def main():
    # æ£€æŸ¥æ˜¯å¦æœ‰--shapå‚æ•°
    calculate_shap = "--shap" in sys.argv
    
    # åˆ›å»ºé¢„æµ‹æ¨¡å‹å®ä¾‹
    predictor = PredictModel()
    
    # è¿è¡Œé¢„æµ‹
    predict_data_path = "output/ml_wide_table_predict_global.csv"
    output_path = "output/prediction_results.csv"
    
    success = predictor.run_prediction(predict_data_path, output_path, calculate_shap)
    
    if success:
        print("\nâœ… é¢„æµ‹å®Œæˆ!")
        if calculate_shap:
            print("âœ… SHAPå€¼å·²è®¡ç®—å¹¶åŒ…å«åœ¨ç»“æœä¸­")
        else:
            print("â„¹ï¸  ä»…è¿›è¡Œé¢„æµ‹ï¼Œæœªè®¡ç®—SHAPå€¼ï¼ˆä½¿ç”¨--shapå‚æ•°å¯å¯ç”¨SHAPè®¡ç®—ï¼‰")
    else:
        print("\nâŒ é¢„æµ‹å¤±è´¥!")

if __name__ == "__main__":
    main()

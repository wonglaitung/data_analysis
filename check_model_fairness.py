import pandas as pd
import numpy as np
import os
import joblib
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# ========== åŸºç¡€å…¬å¹³æ€§å¤„ç†å™¨ ==========
class BaseFairnessProcessor:
    """å…¬å¹³æ€§æ£€æµ‹å¤„ç†å™¨"""
    
    @staticmethod
    def calculate_demographic_parity(y_pred, sensitive_attr):
        """
        è®¡ç®— Demographic Parity (äººå£ç»Ÿè®¡å­¦å…¬å¹³æ€§)
        """
        groups = np.unique(sensitive_attr)
        positive_rates = []
        
        for group in groups:
            mask = sensitive_attr == group
            positive_rate = np.mean(y_pred[mask])
            positive_rates.append(positive_rate)
        
        # è®¡ç®—æœ€å¤§å·®å¼‚ä½œä¸ºä¸å…¬å¹³åº¦é‡
        dp_diff = np.max(positive_rates) - np.min(positive_rates)
        return 1 - dp_diff  # è½¬æ¢ä¸ºå…¬å¹³æ€§å¾—åˆ† (0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨å…¬å¹³)

    @staticmethod
    def calculate_equal_opportunity(y_true, y_pred, sensitive_attr):
        """
        è®¡ç®— Equal Opportunity (æœºä¼šå‡ç­‰)
        """
        groups = np.unique(sensitive_attr)
        tpr_rates = []
        
        for group in groups:
            mask = (sensitive_attr == group) & (y_true == 1)
            if np.sum(mask) > 0:
                tpr = np.mean(y_pred[mask])
            else:
                tpr = 0
            tpr_rates.append(tpr)
        
        # è®¡ç®—æœ€å¤§å·®å¼‚ä½œä¸ºä¸å…¬å¹³åº¦é‡
        eo_diff = np.max(tpr_rates) - np.min(tpr_rates)
        return 1 - eo_diff  # è½¬æ¢ä¸ºå…¬å¹³æ€§å¾—åˆ† (0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨å…¬å¹³)

    @staticmethod
    def calculate_equalized_odds(y_true, y_pred, sensitive_attr):
        """
        è®¡ç®— Equalized Odds (å‡è¡¡å‡ ç‡)
        """
        groups = np.unique(sensitive_attr)
        tpr_rates = []
        fpr_rates = []
        
        for group in groups:
            # è®¡ç®—çœŸé˜³æ€§ç‡ (TPR)
            tp_mask = (sensitive_attr == group) & (y_true == 1)
            if np.sum(tp_mask) > 0:
                tpr = np.mean(y_pred[tp_mask])
            else:
                tpr = 0
            tpr_rates.append(tpr)
            
            # è®¡ç®—å‡é˜³æ€§ç‡ (FPR)
            fp_mask = (sensitive_attr == group) & (y_true == 0)
            if np.sum(fp_mask) > 0:
                fpr = np.mean(y_pred[fp_mask])
            else:
                fpr = 0
            fpr_rates.append(fpr)
        
        # è®¡ç®—TPRå’ŒFPRçš„æœ€å¤§å·®å¼‚
        tpr_diff = np.max(tpr_rates) - np.min(tpr_rates)
        fpr_diff = np.max(fpr_rates) - np.min(fpr_rates)
        
        # ç»¼åˆä¸å…¬å¹³åº¦é‡
        eo_diff = (tpr_diff + fpr_diff) / 2
        return 1 - eo_diff  # è½¬æ¢ä¸ºå…¬å¹³æ€§å¾—åˆ† (0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨å…¬å¹³)

    @staticmethod
    def calculate_predictive_parity(y_true, y_pred, sensitive_attr):
        """
        è®¡ç®— Predictive Parity (é¢„æµ‹å…¬å¹³æ€§)
        """
        groups = np.unique(sensitive_attr)
        ppv_rates = []
        
        for group in groups:
            # è®¡ç®—é¢„æµ‹å€¼ä¸ºæ­£çš„æ ·æœ¬ä¸­çœŸå®å€¼ä¸ºæ­£çš„æ¯”ä¾‹ (Positive Predictive Value, PPV)
            pred_pos_mask = y_pred == 1
            group_mask = sensitive_attr == group
            combined_mask = pred_pos_mask & group_mask
            
            if np.sum(combined_mask) > 0:
                ppv = np.mean(y_true[combined_mask])
            else:
                ppv = 0
            ppv_rates.append(ppv)
        
        # è®¡ç®—æœ€å¤§å·®å¼‚ä½œä¸ºä¸å…¬å¹³åº¦é‡
        pp_diff = np.max(ppv_rates) - np.min(ppv_rates)
        return 1 - pp_diff  # è½¬æ¢ä¸ºå…¬å¹³æ€§å¾—åˆ† (0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨å…¬å¹³)

    @staticmethod
    def calculate_fairness_metrics(y_true, y_pred, sensitive_attr):
        """
        è®¡ç®—æ‰€æœ‰å…¬å¹³æ€§æŒ‡æ ‡
        """
        if sensitive_attr is None:
            return None
            
        # å°†é¢„æµ‹æ¦‚ç‡è½¬æ¢ä¸ºäºŒå€¼é¢„æµ‹ï¼ˆé˜ˆå€¼è®¾ä¸º0.5ï¼‰
        y_pred_binary = (y_pred >= 0.5).astype(int)
        
        # è®¡ç®—å„ç§å…¬å¹³æ€§æŒ‡æ ‡
        demographic_parity = BaseFairnessProcessor.calculate_demographic_parity(y_pred_binary, sensitive_attr)
        equal_opportunity = BaseFairnessProcessor.calculate_equal_opportunity(y_true, y_pred_binary, sensitive_attr)
        equalized_odds = BaseFairnessProcessor.calculate_equalized_odds(y_true, y_pred_binary, sensitive_attr)
        predictive_parity = BaseFairnessProcessor.calculate_predictive_parity(y_true, y_pred_binary, sensitive_attr)
        
        # è¿”å›ç»“æœ
        fairness_metrics = pd.DataFrame({
            'Metric': ['Demographic Parity', 'Equal Opportunity', 'Equalized Odds', 'Predictive Parity'],
            'Score': [demographic_parity, equal_opportunity, equalized_odds, predictive_parity]
        })
        
        return fairness_metrics

# ========== æ•°æ®åŠ è½½å™¨ ==========
class DataLoader:
    def __init__(self):
        self.data = None
        self.target = None
        self.train_data = None
        self.x_train = None
        self.x_val = None
        self.y_train = None
        self.y_val = None
        
    def load_training_data(self, data_path='data_train/data.csv'):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        try:
            # åŠ è½½è®­ç»ƒæ•°æ®
            self.data = pd.read_csv(data_path)
            print(f"âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ: {self.data.shape}")
            
            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            self.target = self.data.pop('Label')
            self.train_data = self.data.copy()
            
            # ç¡®ä¿è®­ç»ƒæ•°æ®éƒ½æ˜¯æ•°å€¼ç±»å‹
            for col in self.train_data.columns:
                if self.train_data[col].dtype == 'bool':
                    self.train_data[col] = self.train_data[col].astype(int)
                elif self.train_data[col].dtype == 'object':
                    # å°è¯•è½¬æ¢å¯¹è±¡ç±»å‹çš„åˆ—
                    self.train_data[col] = pd.to_numeric(self.train_data[col], errors='coerce').fillna(-1)
            
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def split_data(self, test_size=0.2, random_state=2020):
        """åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†"""
        try:
            # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼Œä¿æŒç´¢å¼•
            x_train, x_val, y_train, y_val = train_test_split(
                self.train_data, self.target, test_size=test_size, random_state=random_state, stratify=self.target
            )
            
            # é‡ç½®ç´¢å¼•ï¼Œä½†ä¿ç•™åŸç´¢å¼•ä½œä¸ºåˆ—
            self.x_train = x_train.reset_index(drop=False)
            self.x_val = x_val.reset_index(drop=False)
            self.y_train = y_train.reset_index(drop=False)
            self.y_val = y_val.reset_index(drop=False)
            
            print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {self.x_train.shape}, éªŒè¯é›† {self.x_val.shape}")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®é›†åˆ’åˆ†å¤±è´¥: {e}")
            return False

# ========== ç‰¹å¾å¤„ç†å™¨ ==========
class FeatureProcessor:
    def __init__(self, model_dir="output", config_dir="config"):
        self.model_dir = model_dir
        self.config_dir = config_dir
        self.category_features = []
        self.continuous_features = []
        self.train_feature_names = []
        
    def load_feature_config(self):
        """åŠ è½½ç‰¹å¾é…ç½®"""
        try:
            # åŠ è½½ç‰¹å¾é…ç½®æ–‡ä»¶
            features_path = os.path.join(self.config_dir, "features.csv")
            features_df = pd.read_csv(features_path)
            self.category_features = features_df[features_df['feature_type'] == 'category']['feature_name'].tolist()
            self.continuous_features = features_df[features_df['feature_type'] == 'continuous']['feature_name'].tolist()
            print(f"âœ… ç‰¹å¾é…ç½®åŠ è½½æˆåŠŸ")
            print(f"   - ç±»åˆ«ç‰¹å¾: {len(self.category_features)} ä¸ª")
            print(f"   - è¿ç»­ç‰¹å¾: {len(self.continuous_features)} ä¸ª")
            
            # åŠ è½½è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
            train_features_path = os.path.join(self.model_dir, "train_feature_names.csv")
            train_features_df = pd.read_csv(train_features_path)
            self.train_feature_names = train_features_df['feature'].tolist()
            print(f"âœ… è®­ç»ƒæ—¶ç‰¹å¾åç§°å·²åŠ è½½: {len(self.train_feature_names)} ä¸ªç‰¹å¾")
            return True
        except Exception as e:
            print(f"âŒ ç‰¹å¾é…ç½®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def encode_categorical_features(self, data):
        """å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç """
        try:
            print("æ­£åœ¨è¿›è¡Œç±»åˆ«ç‰¹å¾One-Hotç¼–ç ...")
            encoded_features = []
            remaining_features = data.columns.tolist()
            
            # å¤„ç†ç±»åˆ«ç‰¹å¾
            for col in self.category_features:
                if col in data.columns:
                    # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç 
                    onehot_df = pd.get_dummies(data[col], prefix=col)
                    encoded_features.append(onehot_df)
                    remaining_features.remove(col)
            
            # åˆå¹¶ç¼–ç åçš„ç‰¹å¾å’Œå‰©ä½™ç‰¹å¾
            if encoded_features:
                encoded_data = pd.concat(encoded_features, axis=1)
                remaining_data = data[remaining_features]
                processed_data = pd.concat([remaining_data, encoded_data], axis=1)
                print(f"âœ… One-Hotç¼–ç å®Œæˆ: {len(encoded_features)} ä¸ªç±»åˆ«ç‰¹å¾è¢«ç¼–ç ")
                return processed_data
            else:
                print("â„¹ï¸  æœªå‘ç°éœ€è¦ç¼–ç çš„ç±»åˆ«ç‰¹å¾")
                return data
        except Exception as e:
            print(f"âŒ ç±»åˆ«ç‰¹å¾ç¼–ç å¤±è´¥: {e}")
            return None
    
    def align_features(self, data):
        """ç¡®ä¿ç‰¹å¾é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´"""
        try:
            # è·å–è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
            train_feature_set = set(self.train_feature_names)
            current_feature_set = set(data.columns)
            
            # æ£€æŸ¥ç¼ºå¤±çš„ç‰¹å¾
            missing_features = train_feature_set - current_feature_set
            if missing_features:
                print(f"âš ï¸  å‘ç° {len(missing_features)} ä¸ªç¼ºå¤±ç‰¹å¾ï¼Œå°†ç”¨0å¡«å……")
                for feature in missing_features:
                    data[feature] = 0
            
            # ç§»é™¤å¤šä½™çš„ç‰¹å¾
            extra_features = current_feature_set - train_feature_set
            if extra_features:
                print(f"âš ï¸  ç§»é™¤ {len(extra_features)} ä¸ªå¤šä½™ç‰¹å¾")
                data = data.drop(columns=list(extra_features))
            
            # æŒ‰è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºé‡æ–°æ’åˆ—
            data = data[self.train_feature_names]
            print(f"âœ… ç‰¹å¾å¯¹é½å®Œæˆ: {data.shape[1]} ä¸ªç‰¹å¾")
            return data
        except Exception as e:
            print(f"âŒ ç‰¹å¾å¯¹é½å¤±è´¥: {e}")
            return None

# ========== æ¨¡å‹å¤„ç†å™¨ ==========
class ModelHandler:
    def __init__(self):
        self.gbdt_model = None
        self.lr_model = None
        self.actual_n_estimators = 0
        self.lr_feature_names = []
        
    def load_models(self, gbdt_path='output/gbdt_model.pkl', lr_path='output/lr_model.pkl'):
        """åŠ è½½GBDTå’ŒLRæ¨¡å‹"""
        try:
            self.gbdt_model = joblib.load(gbdt_path)
            self.lr_model = joblib.load(lr_path)
            
            # è·å–LRæ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾åç§°
            self.lr_feature_names = self.lr_model.feature_names_in_.tolist()
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"âœ… LRæ¨¡å‹ç‰¹å¾åç§°å·²åŠ è½½: {len(self.lr_feature_names)} ä¸ªç‰¹å¾")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_leaf_features(self, x_train, x_val):
        """è·å–å¶å­èŠ‚ç‚¹ç‰¹å¾"""
        try:
            # è·å–GBDTæ¨¡å‹å®é™…è®­ç»ƒçš„æ ‘æ•°é‡
            self.actual_n_estimators = self.gbdt_model.best_iteration_
            print(f"âœ… GBDTå®é™…æ ‘æ•°é‡: {self.actual_n_estimators}")
            
            # è·å–å¶å­èŠ‚ç‚¹ç´¢å¼•
            print("æ­£åœ¨è·å–å¶å­èŠ‚ç‚¹ç´¢å¼•...")
            gbdt_feats_train = self.gbdt_model.booster_.predict(x_train.values, pred_leaf=True)
            gbdt_feats_val = self.gbdt_model.booster_.predict(x_val.values, pred_leaf=True)
            
            # å¯¹å¶å­èŠ‚ç‚¹åš One-Hot ç¼–ç 
            gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(self.actual_n_estimators)]
            df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns=gbdt_feats_name)
            df_val_gbdt_feats = pd.DataFrame(gbdt_feats_val, columns=gbdt_feats_name)
            
            data_gbdt = pd.concat([df_train_gbdt_feats, df_val_gbdt_feats], ignore_index=True)
            
            for col in gbdt_feats_name:
                # ç¡®ä¿åˆ—æ•°æ®æ˜¯æ•´æ•°ç±»å‹
                data_gbdt[col] = data_gbdt[col].astype(int)
                onehot_feats = pd.get_dummies(data_gbdt[col], prefix=col)
                data_gbdt.drop([col], axis=1, inplace=True)
                data_gbdt = pd.concat([data_gbdt, onehot_feats], axis=1)
            
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
            for col in data_gbdt.columns:
                if data_gbdt[col].dtype == 'bool':
                    data_gbdt[col] = data_gbdt[col].astype(int)
                # æ£€æŸ¥æ˜¯å¦æœ‰å­—ç¬¦ä¸²ç±»å‹çš„åˆ—
                elif data_gbdt[col].dtype == 'object':
                    print(f"âš ï¸  å‘ç°éæ•°å€¼åˆ—: {col}, ç±»å‹: {data_gbdt[col].dtype}")
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                    data_gbdt[col] = pd.to_numeric(data_gbdt[col], errors='coerce').fillna(0)
            
            train_len = df_train_gbdt_feats.shape[0]
            train_lr = data_gbdt.iloc[:train_len, :].reset_index(drop=True)
            val_lr = data_gbdt.iloc[train_len:, :].reset_index(drop=True)
            
            # åˆ’åˆ†LRè®­ç»ƒ/éªŒè¯é›†ï¼Œè¿”å›æ­£ç¡®çš„ç´¢å¼•
            x_train_lr, x_val_lr, y_train_lr_indices, y_val_lr_indices = train_test_split(
                train_lr, range(len(train_lr)), test_size=0.3, random_state=2018
            )
            
            # è½¬æ¢ç´¢å¼•ä¸ºnumpyæ•°ç»„
            y_train_lr = np.array(y_train_lr_indices)
            y_val_lr = np.array(y_val_lr_indices)
            
            print("âœ… å¶å­èŠ‚ç‚¹ç‰¹å¾å¤„ç†å®Œæˆ")
            
            return x_train_lr, x_val_lr, y_train_lr, y_val_lr
        except Exception as e:
            print(f"âŒ å¶å­èŠ‚ç‚¹ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
            return None, None, None, None
    
    def align_lr_features(self, data):
        """ç¡®ä¿æ•°æ®ç‰¹å¾ä¸LRæ¨¡å‹åŒ¹é…"""
        try:
            # è·å–å½“å‰ç‰¹å¾
            current_features = set(data.columns)
            
            # æ£€æŸ¥ç¼ºå°‘çš„ç‰¹å¾
            missing_features = set(self.lr_feature_names) - current_features
            if missing_features:
                print(f"âš ï¸  æ•°æ®ç¼ºå°‘ {len(missing_features)} ä¸ªç‰¹å¾")
                # ä¸ºç¼ºå°‘çš„ç‰¹å¾æ·»åŠ é»˜è®¤å€¼0
                for feature in missing_features:
                    data[feature] = 0
            
            # ç§»é™¤å¤šä½™çš„ç‰¹å¾
            extra_features = current_features - set(self.lr_feature_names)
            if extra_features:
                print(f"â„¹ï¸  ç§»é™¤ {len(extra_features)} ä¸ªå¤šä½™çš„ç‰¹å¾")
                data = data.drop(columns=list(extra_features))
            
            # ç¡®ä¿ç‰¹å¾é¡ºåºä¸LRæ¨¡å‹ä¸€è‡´
            data = data[self.lr_feature_names]
            return data
        except Exception as e:
            print(f"âŒ LRç‰¹å¾å¯¹é½å¤±è´¥: {e}")
            return None

# ========== å…¬å¹³æ€§è®¡ç®—å™¨ ==========
class FairnessCalculator:
    def __init__(self):
        self.sensitive_attr = None
        
    def load_sensitive_config(self, config_path='config/sensitive_attr.csv'):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½æ•æ„Ÿå±æ€§é…ç½®"""
        try:
            # è¯»å–æ•æ„Ÿå±æ€§é…ç½®æ–‡ä»¶
            config_df = pd.read_csv(config_path)
            if len(config_df) > 0:
                config = config_df.iloc[0]  # å–ç¬¬ä¸€è¡Œé…ç½®
                return config['file_name'], config['sheet_name'], config['column_name']
            else:
                print("âŒ æ•æ„Ÿå±æ€§é…ç½®æ–‡ä»¶ä¸ºç©º")
                return None, None, None
        except Exception as e:
            print(f"âŒ æ•æ„Ÿå±æ€§é…ç½®åŠ è½½å¤±è´¥: {e}")
            return None, None, None
    
    def load_sensitive_attribute(self, file_name, column_name, sheet_name=None):
        """ä»æŒ‡å®šæ–‡ä»¶åŠ è½½æ•æ„Ÿå±æ€§"""
        try:
            # è¯»å–æ•æ„Ÿå±æ€§æ–‡ä»¶
            if sheet_name and pd.notna(sheet_name):
                sensitive_data = pd.read_excel(f'data_train/{file_name}', sheet_name=sheet_name)
            else:
                sensitive_data = pd.read_excel(f'data_train/{file_name}')
            print(f"âœ… æ•æ„Ÿå±æ€§æ–‡ä»¶åŠ è½½æˆåŠŸ: {sensitive_data.shape}")
            
            # è·å–æ•æ„Ÿå±æ€§åˆ—
            if column_name in sensitive_data.columns:
                self.sensitive_attr = sensitive_data[column_name]
                unique_values = self.sensitive_attr.unique()
                print(f"âœ… æ•æ„Ÿå±æ€§åˆ— '{column_name}' å·²åŠ è½½ï¼ŒåŒ…å« {len(unique_values)} ä¸ªå”¯ä¸€å€¼: {unique_values}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§æ¥è¿›è¡Œå…¬å¹³æ€§åˆ†æ
                if len(unique_values) < 2:
                    print("âš ï¸  æ•æ„Ÿå±æ€§åˆ—çš„å”¯ä¸€å€¼è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„å…¬å¹³æ€§åˆ†æ")
                    return False
                return True
            else:
                print(f"âŒ æ•æ„Ÿå±æ€§åˆ— '{column_name}' åœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°")
                return False
        except Exception as e:
            print(f"âŒ æ•æ„Ÿå±æ€§åŠ è½½å¤±è´¥: {e}")
            return False
    
    def calculate_fairness(self, y_true, y_pred_prob, sensitive_attr):
        """è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡"""
        try:
            # å°†é¢„æµ‹æ¦‚ç‡è½¬æ¢ä¸ºäºŒå€¼é¢„æµ‹ï¼ˆé˜ˆå€¼è®¾ä¸º0.5ï¼‰
            y_pred_binary = (y_pred_prob >= 0.5).astype(int)
            
            # è®¡ç®—å„ç§å…¬å¹³æ€§æŒ‡æ ‡
            fairness_metrics = BaseFairnessProcessor.calculate_fairness_metrics(
                y_true, y_pred_prob, sensitive_attr
            )
            
            if fairness_metrics is not None:
                # è¾“å‡ºå…¬å¹³æ€§æŒ‡æ ‡ç»“æœ
                print("\nğŸ“Š æ¨¡å‹å…¬å¹³æ€§æŒ‡æ ‡:")
                for _, row in fairness_metrics.iterrows():
                    print(f"   {row['Metric']}: {row['Score']:.4f}")
                
                # ä¿å­˜å…¬å¹³æ€§æŒ‡æ ‡åˆ°CSVæ–‡ä»¶
                fairness_metrics.to_csv('output/fairness_metrics.csv', index=False)
                print("\nâœ… å…¬å¹³æ€§æŒ‡æ ‡å·²ä¿å­˜è‡³ output/fairness_metrics.csv")
                
                return True
            else:
                print("âš ï¸  å…¬å¹³æ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥")
                return False
        except Exception as e:
            print(f"âŒ å…¬å¹³æ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return False

# ========== ä¸»å‡½æ•° ==========
def calculate_fairness_metrics():
    """
    è®¡ç®—æ¨¡å‹çš„å…¬å¹³æ€§æŒ‡æ ‡
    """
    print("âš–ï¸  å¼€å§‹è®¡ç®—æ¨¡å‹å…¬å¹³æ€§æŒ‡æ ‡...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)
    
    # ========== 1. åˆå§‹åŒ–å„æ¨¡å— ==========
    data_loader = DataLoader()
    feature_processor = FeatureProcessor()
    model_handler = ModelHandler()
    fairness_calculator = FairnessCalculator()
    
    # ========== 2. åŠ è½½æ¨¡å‹å’Œé…ç½® ==========
    try:
        # åŠ è½½ç‰¹å¾é…ç½®
        if not feature_processor.load_feature_config():
            print("âŒ ç‰¹å¾é…ç½®åŠ è½½å¤±è´¥")
            return
        
        # åŠ è½½GBDTæ¨¡å‹å’ŒLRæ¨¡å‹
        if not model_handler.load_models():
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            return
            
        print(f"âœ… æ¨¡å‹å’Œé…ç½®åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # ========== 3. åŠ è½½å’Œå¤„ç†æ•°æ® ==========
    try:
        # åŠ è½½è®­ç»ƒæ•°æ®
        if not data_loader.load_training_data():
            print("âŒ è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥")
            return
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        if not data_loader.split_data():
            print("âŒ æ•°æ®é›†åˆ’åˆ†å¤±è´¥")
            return
        
        # ä¿å­˜åŸå§‹ç´¢å¼•
        original_x_train_indices = data_loader.x_train['index'].values
        original_x_val_indices = data_loader.x_val['index'].values
        original_y_train_indices = data_loader.y_train['index'].values
        original_y_val_indices = data_loader.y_val['index'].values
        
        # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç 
        encoded_data = feature_processor.encode_categorical_features(data_loader.train_data)
        if encoded_data is None:
            print("âŒ ç±»åˆ«ç‰¹å¾ç¼–ç å¤±è´¥")
            return
        
        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
        aligned_data = feature_processor.align_features(encoded_data)
        if aligned_data is None:
            print("âŒ ç‰¹å¾å¯¹é½å¤±è´¥")
            return
        
        # æ›´æ–°æ•°æ®åŠ è½½å™¨ä¸­çš„æ•°æ®ï¼Œä½¿ç”¨åŸå§‹ç´¢å¼•
        data_loader.x_train = aligned_data.iloc[original_x_train_indices].reset_index(drop=True)
        data_loader.x_val = aligned_data.iloc[original_x_val_indices].reset_index(drop=True)
        data_loader.y_train = data_loader.target.iloc[original_y_train_indices].reset_index(drop=True)
        data_loader.y_val = data_loader.target.iloc[original_y_val_indices].reset_index(drop=True)
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return
    
    # ========== 4. è·å–å¶å­èŠ‚ç‚¹ç‰¹å¾ ==========
    try:
        x_train_lr, x_val_lr, y_train_lr, y_val_lr = model_handler.get_leaf_features(
            data_loader.x_train, data_loader.x_val
        )
        if x_train_lr is None:
            print("âŒ å¶å­èŠ‚ç‚¹ç‰¹å¾å¤„ç†å¤±è´¥")
            return
        
        # ç¡®ä¿å¶å­èŠ‚ç‚¹ç‰¹å¾ä¸LRæ¨¡å‹åŒ¹é…
        x_val_lr_aligned = model_handler.align_lr_features(x_val_lr)
        if x_val_lr_aligned is None:
            print("âŒ LRç‰¹å¾å¯¹é½å¤±è´¥")
            return
        
    except Exception as e:
        print(f"âŒ å¶å­èŠ‚ç‚¹ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
        return
    
    # ========== 5. åŠ è½½æ•æ„Ÿå±æ€§ ==========
    # ä»é…ç½®æ–‡ä»¶è¯»å–æ•æ„Ÿå±æ€§é…ç½®
    sensitive_file, sheet_name, sensitive_column = fairness_calculator.load_sensitive_config()
    
    if not sensitive_file or not sensitive_column:
        print("âŒ æ— æ³•ä»é…ç½®æ–‡ä»¶åŠ è½½æ•æ„Ÿå±æ€§é…ç½®")
        return
    
    print(f"â„¹ï¸  ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ•æ„Ÿå±æ€§: æ–‡ä»¶='{sensitive_file}', è¡¨='{sheet_name}', åˆ—='{sensitive_column}'")
    
    if not fairness_calculator.load_sensitive_attribute(sensitive_file, sensitive_column, sheet_name):
        print("âŒ æ•æ„Ÿå±æ€§åŠ è½½å¤±è´¥")
        return
    
    # ========== 6. è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡ ==========
    print("\n" + "="*60)
    print("âš–ï¸  æ­£åœ¨è®¡ç®—æ¨¡å‹å…¬å¹³æ€§æŒ‡æ ‡...")
    print("="*60)
    
    try:
        # ä½¿ç”¨LRæ¨¡å‹è¿›è¡Œé¢„æµ‹
        y_val_pred_prob = model_handler.lr_model.predict_proba(x_val_lr_aligned)[:, 1]
        print("âœ… é¢„æµ‹å®Œæˆ")
        
        # è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡
        # ä¿®å¤æ•æ„Ÿå±æ€§ç´¢å¼•åŒ¹é…é—®é¢˜
        # è·å–åŸå§‹éªŒè¯é›†çš„æ ‡ç­¾å€¼
        original_y_val = data_loader.target.iloc[original_y_val_indices].values
        # ç¡®ä¿y_val_lrçš„ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        valid_indices_mask = y_val_lr < len(original_y_val)
        valid_indices = y_val_lr[valid_indices_mask]
        y_val_for_fairness = original_y_val[valid_indices]
        y_val_pred_prob_filtered = y_val_pred_prob[valid_indices_mask]
        
        # è·å–å¯¹åº”çš„æ•æ„Ÿå±æ€§å€¼
        sensitive_attr_values = fairness_calculator.sensitive_attr.values[valid_indices]
        
        fairness_calculator.calculate_fairness(
            y_val_for_fairness, 
            y_val_pred_prob_filtered, 
            sensitive_attr_values
        )
        
    except Exception as e:
        print(f"âŒ å…¬å¹³æ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    calculate_fairness_metrics()
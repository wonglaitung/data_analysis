import pandas as pd
import numpy as np
import os
import joblib
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from base.base_fairness_processor import BaseFairnessProcessor
from base.base_model_processor import BaseModelProcessor

def calculate_fairness_metrics():
    """
    è®¡ç®—æ¨¡å‹çš„å…¬å¹³æ€§æŒ‡æ ‡
    """
    print("âš–ï¸  å¼€å§‹è®¡ç®—æ¨¡å‹å…¬å¹³æ€§æŒ‡æ ‡...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)
    
    # ========== 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé…ç½® ==========
    try:
        # åˆå§‹åŒ–åŸºç¡€æ¨¡å‹å¤„ç†å™¨
        processor = BaseModelProcessor()
        
        # åŠ è½½ç‰¹å¾é…ç½®
        if not processor.load_feature_config():
            print("âŒ ç‰¹å¾é…ç½®åŠ è½½å¤±è´¥")
            return
        
        # åŠ è½½GBDTæ¨¡å‹å’ŒLRæ¨¡å‹
        if not processor.load_models():
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            return
            
        gbdt_model = processor.gbdt_model
        lr_model = processor.lr_model
        category_features = processor.category_features
        continuous_features = processor.continuous_features
        train_feature_names = processor.train_feature_names
        
        print(f"âœ… æ¨¡å‹å’Œé…ç½®åŠ è½½æˆåŠŸ")
        print(f"   - ç±»åˆ«ç‰¹å¾: {len(category_features)} ä¸ª")
        print(f"   - è¿ç»­ç‰¹å¾: {len(continuous_features)} ä¸ª")
        print(f"   - è®­ç»ƒç‰¹å¾: {len(train_feature_names)} ä¸ª")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # ========== 2. åŠ è½½éªŒè¯æ•°æ® ==========
    try:
        # åŠ è½½è®­ç»ƒæ•°æ®
        data = pd.read_csv('data_train/data.csv')
        print(f"âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ: {data.shape}")
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        target = data.pop('Label')
        train_data = data.copy()
        
        # ç¡®ä¿è®­ç»ƒæ•°æ®éƒ½æ˜¯æ•°å€¼ç±»å‹
        for col in train_data.columns:
            if train_data[col].dtype == 'bool':
                train_data[col] = train_data[col].astype(int)
            elif train_data[col].dtype == 'object':
                # å°è¯•è½¬æ¢å¯¹è±¡ç±»å‹çš„åˆ—
                train_data[col] = pd.to_numeric(train_data[col], errors='coerce').fillna(-1)
        
        # ä¿å­˜ä¸€ä»½åŸå§‹æ•°æ®ç”¨äºæ•æ„Ÿå±æ€§åˆ†æ
        original_train_data = train_data.copy()
        
        # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        print("æ­£åœ¨è¿›è¡Œç±»åˆ«ç‰¹å¾One-Hotç¼–ç ...")
        encoded_features = []
        remaining_features = train_data.columns.tolist()
        
        # å¤„ç†ç±»åˆ«ç‰¹å¾
        for col in category_features:
            if col in train_data.columns:
                # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç 
                onehot_df = pd.get_dummies(train_data[col], prefix=col)
                encoded_features.append(onehot_df)
                remaining_features.remove(col)
        
        # åˆå¹¶ç¼–ç åçš„ç‰¹å¾å’Œå‰©ä½™ç‰¹å¾
        if encoded_features:
            encoded_data = pd.concat(encoded_features, axis=1)
            remaining_data = train_data[remaining_features]
            train_data = pd.concat([remaining_data, encoded_data], axis=1)
            print(f"âœ… One-Hotç¼–ç å®Œæˆ: {len(encoded_features)} ä¸ªç±»åˆ«ç‰¹å¾è¢«ç¼–ç ")
        else:
            print("â„¹ï¸  æœªå‘ç°éœ€è¦ç¼–ç çš„ç±»åˆ«ç‰¹å¾")
        
        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
        # è·å–è®­ç»ƒæ—¶çš„ç‰¹å¾åç§°
        train_feature_set = set(train_feature_names)
        current_feature_set = set(train_data.columns)
        
        # æ£€æŸ¥ç¼ºå¤±çš„ç‰¹å¾
        missing_features = train_feature_set - current_feature_set
        if missing_features:
            print(f"âš ï¸  å‘ç° {len(missing_features)} ä¸ªç¼ºå¤±ç‰¹å¾ï¼Œå°†ç”¨0å¡«å……")
            for feature in missing_features:
                train_data[feature] = 0
        
        # ç§»é™¤å¤šä½™çš„ç‰¹å¾
        extra_features = current_feature_set - train_feature_set
        if extra_features:
            print(f"âš ï¸  ç§»é™¤ {len(extra_features)} ä¸ªå¤šä½™ç‰¹å¾")
            train_data = train_data.drop(columns=list(extra_features))
        
        # æŒ‰è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºé‡æ–°æ’åˆ—
        train_data = train_data[train_feature_names]
        print(f"âœ… ç‰¹å¾å¯¹é½å®Œæˆ: {train_data.shape[1]} ä¸ªç‰¹å¾")
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„éšæœºç§å­ï¼‰
        x_train, x_val, y_train, y_val = train_test_split(
            train_data, target, test_size=0.2, random_state=2020, stratify=target
        )
        print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {x_train.shape}, éªŒè¯é›† {x_val.shape}")
        
        # åŒæ—¶å¯¹åŸå§‹æ•°æ®è¿›è¡Œç›¸åŒçš„åˆ’åˆ†ï¼Œç”¨äºæ•æ„Ÿå±æ€§åˆ†æ
        _, original_x_val, _, _ = train_test_split(
            original_train_data, target, test_size=0.2, random_state=2020, stratify=target
        )
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ========== 3. è·å–å¶å­èŠ‚ç‚¹ç‰¹å¾ ==========
    try:
        # è·å–GBDTæ¨¡å‹å®é™…è®­ç»ƒçš„æ ‘æ•°é‡
        actual_n_estimators = gbdt_model.best_iteration_
        print(f"âœ… GBDTå®é™…æ ‘æ•°é‡: {actual_n_estimators}")
        
        # è·å–å¶å­èŠ‚ç‚¹ç´¢å¼•
        print("æ­£åœ¨è·å–å¶å­èŠ‚ç‚¹ç´¢å¼•...")
        gbdt_feats_train = gbdt_model.booster_.predict(x_train.values, pred_leaf=True)
        gbdt_feats_val = gbdt_model.booster_.predict(x_val.values, pred_leaf=True)
        
        # å¯¹å¶å­èŠ‚ç‚¹åš One-Hot ç¼–ç 
        gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(actual_n_estimators)]
        df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns=gbdt_feats_name)
        df_val_gbdt_feats = pd.DataFrame(gbdt_feats_val, columns=gbdt_feats_name)
        
        data_gbdt = pd.concat([df_train_gbdt_feats, df_val_gbdt_feats], ignore_index=True)
        
        for col in gbdt_feats_name:
            # ç¡®ä¿åˆ—æ•°æ®æ˜¯æ•´æ•°ç±»å‹
            data_gbdt[col] = data_gbdt[col].astype(int)
            onehot_feats = pd.get_dummies(data_gbdt[col], prefix=col)
            data_gbdt.drop([col], axis=1, inplace=True)
            data_gbdt = pd.concat([data_gbdt, onehot_feats], axis=1)
        
        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
        for col in data_gbdt.columns:
            if data_gbdt[col].dtype == 'bool':
                data_gbdt[col] = data_gbdt[col].astype(int)
            # æ£€æŸ¥æ˜¯å¦æœ‰å­—ç¬¦ä¸²ç±»å‹çš„åˆ—
            elif data_gbdt[col].dtype == 'object':
                print(f"âš ï¸  å‘ç°éæ•°å€¼åˆ—: {col}, ç±»å‹: {data_gbdt[col].dtype}")
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                data_gbdt[col] = pd.to_numeric(data_gbdt[col], errors='coerce').fillna(0)
        
        train_len = df_train_gbdt_feats.shape[0]
        train_lr = data_gbdt.iloc[:train_len, :].reset_index(drop=True)
        val_lr = data_gbdt.iloc[train_len:, :].reset_index(drop=True)
        
        # åˆ’åˆ†LRè®­ç»ƒ/éªŒè¯é›†ï¼ˆä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„éšæœºç§å­ï¼‰
        x_train_lr, x_val_lr, y_train_lr, y_val_lr = train_test_split(
            train_lr, y_train, test_size=0.3, random_state=2018, stratify=y_train
        )
        print("âœ… å¶å­èŠ‚ç‚¹ç‰¹å¾å¤„ç†å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å¶å­èŠ‚ç‚¹ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ========== 4. è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡ ==========
    print("\n" + "="*60)
    print("âš–ï¸  æ­£åœ¨è®¡ç®—æ¨¡å‹å…¬å¹³æ€§æŒ‡æ ‡...")
    print("="*60)
    
    # é€‰æ‹©ä¸€ä¸ªç±»åˆ«ç‰¹å¾ä½œä¸ºæ•æ„Ÿå±æ€§
    sensitive_attr = None
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨ç±»åˆ«ç‰¹å¾ä½œä¸ºæ•æ„Ÿå±æ€§
    if category_features:
        # é€‰æ‹©ä¸€ä¸ªå…·æœ‰æ›´å¤šæ ·åŒ–å€¼çš„ç±»åˆ«ç‰¹å¾ä½œä¸ºæ•æ„Ÿå±æ€§
        # ä¼˜å…ˆé€‰æ‹©éƒ¨é—¨ç›¸å…³çš„ç‰¹å¾ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½å…·æœ‰æ›´å¤šä¸åŒçš„å€¼
        department_features = [col for col in category_features if 'éƒ¨é—¨' in col]
        if department_features:
            sensitive_col = department_features[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªéƒ¨é—¨ç‰¹å¾
        else:
            sensitive_col = category_features[0]  # å¦åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ªç±»åˆ«ç‰¹å¾
        
        print(f"â„¹ï¸  ä½¿ç”¨ '{sensitive_col}' ä½œä¸ºæ•æ„Ÿå±æ€§è¿›è¡Œå…¬å¹³æ€§åˆ†æ")
        
        try:
            # ä»å·²ä¿å­˜çš„åŸå§‹æ•°æ®ä¸­è·å–æ•æ„Ÿå±æ€§åˆ—çš„å€¼
            if sensitive_col in original_x_val.columns:
                # è·å–æ•æ„Ÿå±æ€§å€¼
                sensitive_attr = original_x_val[sensitive_col]
                unique_values = sensitive_attr.unique()
                print(f"âœ… æ•æ„Ÿå±æ€§åˆ— '{sensitive_col}' å·²åŠ è½½ï¼ŒåŒ…å« {len(unique_values)} ä¸ªå”¯ä¸€å€¼: {unique_values}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§æ¥è¿›è¡Œå…¬å¹³æ€§åˆ†æ
                if len(unique_values) < 2:
                    print("âš ï¸  æ•æ„Ÿå±æ€§åˆ—çš„å”¯ä¸€å€¼è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„å…¬å¹³æ€§åˆ†æ")
                    sensitive_attr = None
            else:
                print(f"âš ï¸  æ•æ„Ÿå±æ€§åˆ— '{sensitive_col}' åœ¨åŸå§‹æ•°æ®ä¸­æœªæ‰¾åˆ°")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ•æ„Ÿå±æ€§æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç±»åˆ«ç‰¹å¾ï¼Œåˆ›å»ºä¸€ä¸ªäººå·¥çš„æ•æ„Ÿå±æ€§åˆ—ç”¨äºæ¼”ç¤º
    if sensitive_attr is None:
        print("â„¹ï¸  æœªæ‰¾åˆ°åˆé€‚çš„ç±»åˆ«ç‰¹å¾ä½œä¸ºæ•æ„Ÿå±æ€§ï¼Œåˆ›å»ºäººå·¥æ•æ„Ÿå±æ€§åˆ—ç”¨äºæ¼”ç¤º")
        try:
            # ä½¿ç”¨LRæ¨¡å‹çš„é¢„æµ‹ç»“æœåˆ›å»ºä¸€ä¸ªäººå·¥çš„æ•æ„Ÿå±æ€§
            # åŸºäºé¢„æµ‹æ¦‚ç‡çš„ä¸­ä½æ•°å°†æ ·æœ¬åˆ†ä¸ºä¸¤ç»„
            # ä¸ºäº†é¿å…ç‰¹å¾åç§°ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæˆ‘ä»¬å…ˆæ£€æŸ¥x_val_lrçš„ç‰¹å¾åç§°
            print(f"â„¹ï¸  x_val_lr ç‰¹å¾æ•°é‡: {x_val_lr.shape[1]}")
            print(f"â„¹ï¸  LRæ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡: {len(lr_model.feature_names_in_)}")
            
            # ç¡®ä¿ç‰¹å¾åç§°åŒ¹é…
            if set(x_val_lr.columns) == set(lr_model.feature_names_in_):
                # é‡æ–°æ’åˆ—åˆ—ä»¥åŒ¹é…LRæ¨¡å‹æœŸæœ›çš„é¡ºåº
                x_val_lr_aligned = x_val_lr[lr_model.feature_names_in_]
                y_val_pred_prob_temp = lr_model.predict_proba(x_val_lr_aligned)[:, 1]
                median_prob = np.median(y_val_pred_prob_temp)
                sensitive_attr = pd.Series([1 if prob >= median_prob else 0 for prob in y_val_pred_prob_temp])
                print(f"âœ… äººå·¥æ•æ„Ÿå±æ€§åˆ—å·²åˆ›å»ºï¼ŒåŒ…å« {len(sensitive_attr.unique())} ä¸ªå”¯ä¸€å€¼: {sensitive_attr.unique()}")
            else:
                print("âš ï¸  ç‰¹å¾åç§°ä¸åŒ¹é…ï¼Œæ— æ³•ä½¿ç”¨LRæ¨¡å‹é¢„æµ‹ç»“æœåˆ›å»ºæ•æ„Ÿå±æ€§")
                # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šåŸºäºæ ·æœ¬ç´¢å¼•åˆ›å»º
                sensitive_attr = pd.Series([i % 2 for i in range(len(x_val_lr))])
                print(f"âœ… äººå·¥æ•æ„Ÿå±æ€§åˆ—å·²åˆ›å»ºï¼ŒåŒ…å« {len(sensitive_attr.unique())} ä¸ªå”¯ä¸€å€¼: {sensitive_attr.unique()}")
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºäººå·¥æ•æ„Ÿå±æ€§æ—¶å‡ºé”™: {e}")
            # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šåŸºäºæ ·æœ¬ç´¢å¼•åˆ›å»º
            try:
                sensitive_attr = pd.Series([i % 2 for i in range(len(x_val_lr))])
                print(f"âœ… äººå·¥æ•æ„Ÿå±æ€§åˆ—å·²åˆ›å»ºï¼ŒåŒ…å« {len(sensitive_attr.unique())} ä¸ªå”¯ä¸€å€¼: {sensitive_attr.unique()}")
            except Exception as e2:
                print(f"âš ï¸  åˆ›å»ºäººå·¥æ•æ„Ÿå±æ€§æ—¶å‡ºé”™: {e2}")
    
    if sensitive_attr is not None and len(sensitive_attr) == len(y_val_lr):
        try:
            # ç¡®ä¿ç‰¹å¾åç§°åŒ¹é…åå†è¿›è¡Œé¢„æµ‹
            if set(x_val_lr.columns) == set(lr_model.feature_names_in_):
                # é‡æ–°æ’åˆ—åˆ—ä»¥åŒ¹é…LRæ¨¡å‹æœŸæœ›çš„é¡ºåº
                x_val_lr_aligned = x_val_lr[lr_model.feature_names_in_]
                # ä½¿ç”¨LRæ¨¡å‹è¿›è¡Œé¢„æµ‹
                y_val_pred_prob = lr_model.predict_proba(x_val_lr_aligned)[:, 1]
            else:
                print("âš ï¸  ç‰¹å¾åç§°ä¸åŒ¹é…ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
                y_val_pred_prob = None
                
            if y_val_pred_prob is not None:
                # è®¡ç®—å„ç§å…¬å¹³æ€§æŒ‡æ ‡
                fairness_metrics = BaseFairnessProcessor.calculate_fairness_metrics(
                    y_val_lr.values, 
                    y_val_pred_prob, 
                    sensitive_attr.values
                )
                
                if fairness_metrics is not None:
                    # è¾“å‡ºå…¬å¹³æ€§æŒ‡æ ‡ç»“æœ
                    print("\nğŸ“Š æ¨¡å‹å…¬å¹³æ€§æŒ‡æ ‡:")
                    for _, row in fairness_metrics.iterrows():
                        print(f"   {row['Metric']}: {row['Score']:.4f}")
                    
                    # ä¿å­˜å…¬å¹³æ€§æŒ‡æ ‡åˆ°CSVæ–‡ä»¶
                    fairness_metrics.to_csv('output/fairness_metrics.csv', index=False)
                    print("\nâœ… å…¬å¹³æ€§æŒ‡æ ‡å·²ä¿å­˜è‡³ output/fairness_metrics.csv")
                    
                    # è®¡ç®—å¹¶æ˜¾ç¤ºAUCä½œä¸ºå‚è€ƒ
                    val_auc = roc_auc_score(y_val_lr, y_val_pred_prob)
                    print(f"âœ… éªŒè¯é›† AUC: {val_auc:.4f}")
                else:
                    print("âš ï¸  å…¬å¹³æ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥")
            else:
                print("âš ï¸  æ— æ³•è¿›è¡Œé¢„æµ‹ï¼Œè·³è¿‡å…¬å¹³æ€§æŒ‡æ ‡è®¡ç®—")
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•æ„Ÿå±æ€§åˆ—ï¼Œæ— æ³•è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡")
        print("â„¹ï¸  ä¸ºäº†è¿›è¡Œå…¬å¹³æ€§åˆ†æï¼Œè¯·ç¡®ä¿æ•°æ®ä¸­åŒ…å«åˆé€‚çš„æ•æ„Ÿå±æ€§åˆ—ï¼ˆå¦‚æ€§åˆ«ã€å¹´é¾„ç»„ç­‰ï¼‰")

if __name__ == "__main__":
    calculate_fairness_metrics()
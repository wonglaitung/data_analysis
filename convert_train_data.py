import pandas as pd
import os
import glob
import re
from collections import defaultdict
import psutil
import gc

# ==============================
# 1. è·¯å¾„è®¾ç½®
# ==============================
input_dir = "data_train"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

def normalize_name(name):
    """
    æ ‡å‡†åŒ–åç§°ï¼Œå°†ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    """
    return re.sub(r'[^\w]', '_', name)

def read_config_file(file_path, required_columns):
    """
    è¯»å–é…ç½®æ–‡ä»¶çš„é€šç”¨å‡½æ•°
    
    å‚æ•°:
    file_path: é…ç½®æ–‡ä»¶è·¯å¾„
    required_columns: å¿…éœ€çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    DataFrame: è¯»å–çš„é…ç½®æ•°æ®
    """
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path, dtype=str)
            # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                print(f"é…ç½®æ–‡ä»¶ {file_path} ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
                return pd.DataFrame()
            return df
        except Exception as e:
            print(f"è¯»å–é…ç½®æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    return pd.DataFrame()

def get_primary_key_mapping():
    """
    è¯»å–config/primary_key.csvæ–‡ä»¶ï¼Œè·å–ä¸»é”®æ˜ å°„
    """
    config_dir = "config"
    primary_key_file = os.path.join(config_dir, "primary_key.csv")
    mapping = {}
    
    df_pk = read_config_file(primary_key_file, ['file_name', 'primary_key'])
    if not df_pk.empty:
        for _, row in df_pk.iterrows():
            file = str(row.get('file_name', '')).strip()
            sheet = str(row.get('sheet_name', '')).strip() if 'sheet_name' in row else ''
            pk = str(row.get('primary_key', '')).strip()
            if file and pk:
                mapping[(file, sheet)] = pk
    return mapping

def get_category_feature_mapping():
    """
    è¯»å–config/category_type.csvæ–‡ä»¶ï¼Œè·å–éœ€è¦å¼ºåˆ¶ä½œä¸ºç±»åˆ«ç‰¹å¾çš„å­—æ®µæ˜ å°„
    """
    config_dir = "config"
    category_type_file = os.path.join(config_dir, "category_type.csv")
    mapping = {}
    
    df_category = read_config_file(category_type_file, ['file_name', 'column_name', 'feature_type'])
    if not df_category.empty:
        for _, row in df_category.iterrows():
            file = str(row.get('file_name', '')).strip()
            column = str(row.get('column_name', '')).strip()
            feature_type = str(row.get('feature_type', '')).strip()
            if file and column and feature_type:
                # ä½¿ç”¨æ–‡ä»¶åå’Œåˆ—åä½œä¸ºé”®
                mapping[(file, column)] = feature_type
    return mapping

def auto_detect_key(df):
    primary_key_candidates = ['cusno', 'ci', 'å®¢æˆ·ç¼–å·', 'å®¢æˆ·å·']
    for col in df.columns:
        if any(candidate in col.lower() for candidate in primary_key_candidates):
            return col
    return None

def determine_feature_type(col, wide_df, category_feature_mapping, file_name=None, orig_col_name=None):
    """
    ç¡®å®šç‰¹å¾ç±»å‹
    
    å‚æ•°:
    col: åˆ—å
    wide_df: æ•°æ®æ¡†
    category_feature_mapping: ç±»åˆ«ç‰¹å¾æ˜ å°„
    file_name: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    orig_col_name: åŸå§‹åˆ—åï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
    feature_type: ç‰¹å¾ç±»å‹ ('continuous', 'category', 'text', 'other')
    """
    # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨category_type.csvä¸­è¢«æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾
    is_forced_category = False
    
    # å¦‚æœæä¾›äº†file_nameå’Œorig_col_nameï¼Œç›´æ¥æ£€æŸ¥æ˜¯å¦åœ¨category_feature_mappingä¸­
    if file_name and orig_col_name and (file_name, orig_col_name) in category_feature_mapping:
        is_forced_category = True
    
    # éå†category_feature_mappingï¼Œæ£€æŸ¥åˆ—åæ˜¯å¦åŒ¹é…
    if not is_forced_category:
        for (f_name, column_name), feature_type in category_feature_mapping.items():
            # ç”Ÿæˆå¯èƒ½çš„å‰ç¼€
            safe_prefix = normalize_name(f_name.replace('.xlsx', ''))
            
            # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆä¸å¸¦å‰ç¼€çš„åŸå§‹åˆ—åï¼‰
            if col == column_name and feature_type == 'category':
                is_forced_category = True
                break
            
            # æ£€æŸ¥æ˜¯å¦å¸¦å‰ç¼€åŒ¹é…
            if col.startswith(f"{safe_prefix}_{column_name.lower()}") and feature_type == 'category':
                is_forced_category = True
                break
            
            # ç‰¹æ®Šå¤„ç†ï¼šå¤„ç†åˆ—åä¸­åŒ…å«ä¸‹åˆ’çº¿çš„æƒ…å†µ
            if feature_type == 'category':
                # å°†åŸå§‹åˆ—åè½¬æ¢ä¸ºå°å†™å¹¶æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
                normalized_column_name = normalize_name(column_name.lower())
                if col.startswith(f"{safe_prefix}_{normalized_column_name}") or col.startswith(f"{safe_prefix}_{column_name.lower()}"):
                    is_forced_category = True
                    break
    
    if is_forced_category:
        # å¼ºåˆ¶ä½œä¸ºç±»åˆ«ç‰¹å¾
        return 'category'
    elif pd.api.types.is_numeric_dtype(wide_df[col]):
        return 'continuous'
    elif pd.api.types.is_string_dtype(wide_df[col]) or pd.api.types.is_object_dtype(wide_df[col]):
        if wide_df[col].nunique() <= 10:
            return 'category'
        else:
            return 'text'
    else:
        return 'other'

def process_all_excel_files():
    excel_files = glob.glob(os.path.join(input_dir, "*.xlsx"))
    print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
    all_data = defaultdict(list)
    all_primary_keys_set = set(['__primary_key__', 'id', 'cusno', 'ci', 'index'])

    primary_key_mapping = get_primary_key_mapping()
    category_feature_mapping = get_category_feature_mapping()

    for file_path in excel_files:
        file_name = os.path.basename(file_path)
        print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_name}")
        try:
            excel_data = pd.read_excel(file_path, sheet_name=None)
            for sheet_name, df in excel_data.items():
                # åˆ—åè½¬å°å†™å¹¶å»ç©ºæ ¼
                df.columns = [col.strip().lower() if isinstance(col, str) else col for col in df.columns]

                # === å…³é”®ï¼šä¸å†æ·»åŠ  source_file / sheet_name ===
                safe_prefix = normalize_name(file_name.replace('.xlsx', ''))
                new_columns = []
                for col in df.columns:
                    if col == '__primary_key__':
                        new_columns.append(col)
                    else:
                        new_columns.append(f"{safe_prefix}_{col}")
                df.columns = new_columns

                # ç¡®å®šä¸»é”®
                pk = None
                if (file_name, sheet_name) in primary_key_mapping:
                    pk = primary_key_mapping[(file_name, sheet_name)].strip()
                elif (file_name, '') in primary_key_mapping:
                    pk = primary_key_mapping[(file_name, '')].strip()
                elif auto_detect_key(df):
                    pk = auto_detect_key(df)
                else:
                    pk = 'index'
                    df[pk] = df.index.astype(str)

                pk_lower = pk.lower()
                if pk_lower in df.columns:
                    df['__primary_key__'] = df[pk_lower]
                else:
                    df['__primary_key__'] = df[pk]

                print(f"  ä½¿ç”¨ä¸»é”®: {pk}")
                all_primary_keys_set.add(pk_lower)
                all_data[file_name].append(df)

        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return all_data, [k.lower() for k in all_primary_keys_set], category_feature_mapping

def analyze_fields_and_dimensions(all_data, category_feature_mapping):
    """
    åˆ†æå­—æ®µå’Œç»´åº¦ï¼Œè€ƒè™‘ä»category_type.csvä¸­è¯»å–çš„ç±»åˆ«ç‰¹å¾
    """
    field_analysis = {}
    dimension_analysis = {}
    for file_name, dataframes in all_data.items():
        for df in dataframes:
            # è·å–è¯¥æ–‡ä»¶çš„å®‰å…¨å‰ç¼€
            safe_prefix = normalize_name(file_name.replace('.xlsx', ''))
            
            for col in df.columns:
                if col not in field_analysis:
                    field_analysis[col] = {
                        'data_types': set(),
                        'files': set(),
                        'sample_values': set()
                    }
                field_analysis[col]['data_types'].add(str(df[col].dtype))
                field_analysis[col]['files'].add(file_name)
                sample_vals = df[col].dropna().unique()[:5]
                field_analysis[col]['sample_values'].update(sample_vals)
            
            for col in df.columns:
                # === ä¸å†æ’é™¤ source_file/sheet_nameï¼ˆå› ä¸ºå®ƒä»¬ä¸å­˜åœ¨ï¼‰===
                # æå–åŸå§‹åˆ—åï¼ˆå»é™¤å‰ç¼€ï¼‰
                orig_col_name = col
                if col.startswith(safe_prefix + '_'):
                    orig_col_name = col[len(safe_prefix) + 1:]
                
                # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨category_type.csvä¸­è¢«æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾
                is_forced_category = False
                # æ£€æŸ¥æ˜¯å¦åœ¨category_feature_mappingä¸­
                if (file_name, orig_col_name) in category_feature_mapping:
                    is_forced_category = True
                
                # å¦‚æœæ˜¯å¼ºåˆ¶ç±»åˆ«ç‰¹å¾ï¼Œæˆ–è€…åŸæœ¬å°±æ˜¯å­—ç¬¦ä¸²/å¯¹è±¡ç±»å‹ä¸”ä¸æ˜¯ä¸»é”®ï¼Œåˆ™ä½œä¸ºç»´åº¦å¤„ç†
                if (is_forced_category or 
                    ((pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col])) 
                     and col != '__primary_key__')):
                    if col not in dimension_analysis:
                        dimension_analysis[col] = {
                            'files': set(),
                            'values': set()
                        }
                    dimension_analysis[col]['files'].add(file_name)
                    unique_vals = df[col].dropna().unique()
                    dimension_analysis[col]['values'].update(unique_vals[:100])
    return field_analysis, dimension_analysis

def get_topk_by_coverage(value_counts, coverage_threshold=0.95, max_top_k=50):
    total = value_counts.sum()
    if total == 0:
        return []
    cumulative = value_counts.cumsum() / total
    topk_idx = cumulative[cumulative <= coverage_threshold].index.tolist()
    if len(topk_idx) < len(value_counts):
        topk_idx.append(cumulative.index[len(topk_idx)])
    if len(topk_idx) > max_top_k:
        topk_idx = topk_idx[:max_top_k]
    return topk_idx

def create_wide_table_per_file(all_data, dimension_analysis, all_primary_keys, category_feature_mapping, coverage_threshold=0.95, max_top_k=50):
    file_wide_tables = {}

    for file_name, dataframes in all_data.items():
        print(f"\n=== å¤„ç†æ–‡ä»¶å®½è¡¨: {file_name} ===")
        file_wide_dfs = []
        category_features = []  # ç”¨äºå­˜å‚¨ç±»åˆ«å‹ç‰¹å¾

        safe_prefix = normalize_name(file_name.replace('.xlsx', ''))

        def strip_file_prefix(full_col):
            expected_start = safe_prefix + '_'
            if full_col.startswith(expected_start):
                return full_col[len(expected_start):]
            return full_col

        for df in dataframes:
            primary_key = '__primary_key__'
            if primary_key not in df.columns:
                df[primary_key] = df.index.astype(str)

            # ç¡®å®šæ•°å€¼å‹åˆ—å’Œç»´åº¦åˆ—
            numeric_cols = []
            dimension_cols = []
            
            for col in df.columns:
                if col == primary_key or col.lower() in all_primary_keys:
                    continue
                
                # æå–åŸå§‹åˆ—åï¼ˆå»é™¤å‰ç¼€ï¼‰
                orig_col_name = col
                if col.startswith(safe_prefix + '_'):
                    orig_col_name = col[len(safe_prefix) + 1:]
                
                # æ£€æŸ¥æ˜¯å¦åœ¨category_type.csvä¸­è¢«æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾
                is_forced_category = (file_name, orig_col_name) in category_feature_mapping
                
                # å¦‚æœæ˜¯å¼ºåˆ¶ç±»åˆ«ç‰¹å¾ï¼Œåˆ™åŠ å…¥ç»´åº¦åˆ—
                if is_forced_category:
                    dimension_cols.append(col)
                # å¦‚æœæ˜¯æ•°å€¼å‹ä¸”æœªè¢«å¼ºåˆ¶æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾ï¼Œåˆ™åŠ å…¥æ•°å€¼åˆ—
                elif pd.api.types.is_numeric_dtype(df[col]):
                    numeric_cols.append(col)
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²/å¯¹è±¡ç±»å‹ä¸”æœªè¢«å¼ºåˆ¶æŒ‡å®šä¸ºç±»åˆ«ç‰¹å¾ï¼Œåˆ™åŠ å…¥ç»´åº¦åˆ—
                elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):
                    dimension_cols.append(col)

            # ä¿å­˜åŸå§‹çš„ç±»åˆ«å‹ç‰¹å¾åˆ—
            for dim_col in dimension_cols:
                if dim_col in dimension_analysis:
                    value_counts = df[dim_col].value_counts()
                    topk_values = get_topk_by_coverage(value_counts, coverage_threshold=coverage_threshold, max_top_k=max_top_k)
                    df[dim_col] = df[dim_col].where(df[dim_col].isin(topk_values), other='other')
                    
                    # ä¿å­˜ç±»åˆ«å‹ç‰¹å¾åˆ—ï¼Œç”¨äºåç»­åˆå¹¶
                    category_features.append(dim_col)

                    for numeric_col in numeric_cols:
                        try:
                            max_rows = 50000
                            df_subset = df.iloc[:max_rows] if len(df) > max_rows else df

                            pivot = df_subset.pivot_table(
                                index=primary_key,
                                columns=dim_col,
                                values=numeric_col,
                                aggfunc='sum',
                                fill_value=0
                            )

                            orig_numeric = strip_file_prefix(numeric_col)
                            orig_dim = strip_file_prefix(dim_col)
                            pivot.columns = [
                                f"{safe_prefix}_{orig_numeric}_{orig_dim}_{col}"
                                for col in pivot.columns
                            ]
                            pivot = pivot.reset_index()

                            # === å…³é”®ï¼šä¸å†æ·»åŠ  source_file / dimension / value_field ===
                            file_wide_dfs.append(pivot)

                            process = psutil.Process(os.getpid())
                            memory_info = process.memory_info()
                            if memory_info.rss / psutil.virtual_memory().total > 0.7:
                                gc.collect()
                                print("  æ‰§è¡Œåƒåœ¾å›æ”¶")

                            del pivot, df_subset
                        except Exception as e:
                            print(f"  è­¦å‘Šï¼šé€è§†å¤±è´¥ ({dim_col}, {numeric_col}): {e}")
                else:
                    print(f"  è·³è¿‡æœªåˆ†æç»´åº¦: {dim_col}")

        if not file_wide_dfs and not category_features:
            print(f"  âš ï¸ æ–‡ä»¶ {file_name} æœªç”Ÿæˆä»»ä½•é€è§†è¡¨")
            continue

        final_df = None
        
        # å¦‚æœæœ‰é€è§†è¡¨æ•°æ®ï¼Œåˆ™å¤„ç†é€è§†è¡¨
        if file_wide_dfs:
            print(f"  åˆå¹¶ {len(file_wide_dfs)} ä¸ªé€è§†è¡¨...")
            merged_df = pd.concat(file_wide_dfs, axis=0, sort=False)
            del file_wide_dfs
            gc.collect()

            numeric_columns = [col for col in merged_df.columns if pd.api.types.is_numeric_dtype(merged_df[col])]
            other_columns = [col for col in merged_df.columns if col not in numeric_columns]

            max_agg_cols = 1000
            if len(numeric_columns) > max_agg_cols:
                numeric_columns = numeric_columns[:max_agg_cols]

            agg_dict = {}
            for col in numeric_columns:
                if col != '__primary_key__':
                    agg_dict[col] = 'sum'
            for col in other_columns:
                if col != '__primary_key__' and len(agg_dict) < max_agg_cols:
                    agg_dict[col] = 'first'

            final_df = merged_df.groupby('__primary_key__').agg(agg_dict).reset_index()
            del merged_df
            gc.collect()

            numeric_cols_final = [col for col in final_df.columns if pd.api.types.is_numeric_dtype(final_df[col])]
            other_cols_final = [col for col in final_df.columns if col not in numeric_cols_final]

            final_df[numeric_cols_final] = final_df[numeric_cols_final].fillna(0)
            for col in other_cols_final:
                if col != '__primary_key__':
                    final_df[col] = final_df[col].fillna('Unknown')
        else:
            # å¦‚æœæ²¡æœ‰é€è§†è¡¨æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªåªåŒ…å«ä¸»é”®çš„DataFrame
            all_dfs = dataframes
            if all_dfs:
                first_df = all_dfs[0]
                final_df = pd.DataFrame({'__primary_key__': first_df['__primary_key__'].unique()})
        
        # æ·»åŠ åŸå§‹çš„ç±»åˆ«å‹ç‰¹å¾åˆ—ï¼ˆè½¬æ¢ä¸ºæ•°å€¼å‹ï¼‰
        if category_features and dataframes:
            # è·å–æ‰€æœ‰æ•°æ®å¸§ä¸­çš„ç±»åˆ«ç‰¹å¾å¹¶åˆå¹¶
            all_category_data = []
            for df in dataframes:
                cols_to_include = ['__primary_key__'] + [col for col in category_features if col in df.columns]
                if cols_to_include:
                    # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œå»é‡å’Œèšåˆ
                    category_df = df[cols_to_include].copy()
                    all_category_data.append(category_df)
            
            if all_category_data:
                # åˆå¹¶æ‰€æœ‰ç±»åˆ«ç‰¹å¾æ•°æ®
                combined_category_df = pd.concat(all_category_data, axis=0, sort=False)
                
                # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œèšåˆï¼ˆå–ç¬¬ä¸€ä¸ªå€¼ï¼‰
                category_agg_dict = {}
                for col in combined_category_df.columns:
                    if col != '__primary_key__':
                        category_agg_dict[col] = 'first'
                
                if category_agg_dict:
                    unique_category_df = combined_category_df.groupby('__primary_key__').agg(category_agg_dict).reset_index()
                    
                    # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-Hotç¼–ç 
                    encoded_dfs = []
                    for col in category_agg_dict.keys():
                        if col in unique_category_df.columns:
                            # å¡«å……ç¼ºå¤±å€¼
                            unique_category_df[col] = unique_category_df[col].fillna('Unknown')
                            # One-Hotç¼–ç 
                            onehot_df = pd.get_dummies(unique_category_df[col], prefix=col)
                            # æ·»åŠ ä¸»é”®åˆ—
                            onehot_df['__primary_key__'] = unique_category_df['__primary_key__'].values
                            encoded_dfs.append(onehot_df)
                    
                    # åˆå¹¶æ‰€æœ‰One-Hotç¼–ç çš„ç‰¹å¾
                    if encoded_dfs:
                        combined_encoded_df = encoded_dfs[0]
                        for encoded_df in encoded_dfs[1:]:
                            combined_encoded_df = pd.merge(combined_encoded_df, encoded_df, on='__primary_key__', how='outer')
                        
                        # å°†ç¼–ç åçš„ç‰¹å¾åˆå¹¶åˆ°æœ€ç»ˆDataFrameä¸­
                        if final_df is not None:
                            final_df = pd.merge(final_df, combined_encoded_df, on='__primary_key__', how='left')
                        else:
                            final_df = combined_encoded_df

        if final_df is None:
            print(f"  âš ï¸ æ–‡ä»¶ {file_name} æ— æ³•ç”Ÿæˆå®½è¡¨")
            continue

        # å¡«å……ç±»åˆ«ç‰¹å¾çš„ç¼ºå¤±å€¼
        category_cols = [col for col in final_df.columns 
                        if (pd.api.types.is_string_dtype(final_df[col]) or pd.api.types.is_object_dtype(final_df[col]))
                        and col != '__primary_key__']
        for col in category_cols:
            final_df[col] = final_df[col].fillna('Unknown')

        if '__primary_key__' in final_df.columns:
            final_df.rename(columns={'__primary_key__': 'Id'}, inplace=True)

        # === ä¸å†éœ€è¦åˆ é™¤å…ƒå­—æ®µï¼Œå› ä¸ºä»æœªæ·»åŠ è¿‡ ===
        file_wide_tables[file_name] = final_df
        print(f"  âœ… å®Œæˆæ–‡ä»¶ {file_name}ï¼Œå®½è¡¨å½¢çŠ¶: {final_df.shape}")

    return file_wide_tables

def calculate_derived_features(wide_df):
    if wide_df.empty:
        return wide_df
    numeric_cols = [col for col in wide_df.columns if pd.api.types.is_numeric_dtype(wide_df[col])]
    exclude_cols = ['ci', 'cusno', 'index', 'Id']
    numeric_cols = [col for col in numeric_cols if col.lower() not in [x.lower() for x in exclude_cols]]
    max_feature_cols = 500
    if len(numeric_cols) > max_feature_cols:
        numeric_cols = numeric_cols[:max_feature_cols]
    new_features = {}
    if len(numeric_cols) > 0:
        new_features['total_sum'] = wide_df[numeric_cols].sum(axis=1)
    if len(numeric_cols) > 1:
        new_features['total_mean'] = wide_df[numeric_cols].mean(axis=1)
        new_features['total_std'] = wide_df[numeric_cols].std(axis=1)
        new_features['total_max'] = wide_df[numeric_cols].max(axis=1)
        new_features['total_min'] = wide_df[numeric_cols].min(axis=1)
    if new_features:
        new_features_df = pd.DataFrame(new_features, index=wide_df.index)
        wide_df = pd.concat([wide_df, new_features_df], axis=1)
    print(f"  è¡ç”Ÿç‰¹å¾è®¡ç®—å®Œæˆï¼Œå½“å‰å½¢çŠ¶: {wide_df.shape}")
    return wide_df

def generate_feature_dictionary(wide_df, category_feature_mapping):
    """
    ç”Ÿæˆç‰¹å¾å­—å…¸ï¼Œè€ƒè™‘ä»category_type.csvä¸­è¯»å–çš„ç±»åˆ«ç‰¹å¾
    """
    feature_dict = []
    for col in wide_df.columns:
        # ä½¿ç”¨å·¥å…·å‡½æ•°ç¡®å®šç‰¹å¾ç±»å‹
        feature_type = determine_feature_type(col, wide_df, category_feature_mapping)
            
        feature_dict.append({
            'feature_name': col,
            'feature_type': feature_type
        })
    return pd.DataFrame(feature_dict)

def main(coverage_threshold=0.95, max_top_k=50):
    # è·å–ç±»åˆ«ç‰¹å¾æ˜ å°„
    all_data, all_primary_keys, category_feature_mapping = process_all_excel_files()
    field_analysis, dimension_analysis = analyze_fields_and_dimensions(all_data, category_feature_mapping)
    print(f"\nåˆ†æäº† {len(field_analysis)} ä¸ªå­—æ®µ, {len(dimension_analysis)} ä¸ªç»´åº¦")

    file_wide_tables = create_wide_table_per_file(
        all_data,
        dimension_analysis,
        all_primary_keys,
        category_feature_mapping,
        coverage_threshold=coverage_threshold,
        max_top_k=max_top_k
    )

    if not file_wide_tables:
        print("âŒ æœªç”Ÿæˆä»»ä½•å®½è¡¨")
        return

    # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„ç‹¬ç«‹å®½è¡¨
    for file_name, wide_df in file_wide_tables.items():
        safe_name = normalize_name(file_name.replace('.xlsx', ''))
        output_csv = os.path.join(output_dir, f"wide_table_{safe_name}.csv")
        wide_df.to_csv(output_csv, index=False, encoding='utf-8')
        print(f"âœ… ä¿å­˜æ–‡ä»¶å®½è¡¨: {output_csv}")

        feature_dict_df = generate_feature_dictionary(wide_df, category_feature_mapping)
        dict_csv = os.path.join(output_dir, f"feature_dict_{safe_name}.csv")
        feature_dict_df.to_csv(dict_csv, index=False, encoding='utf-8')

    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶å®½è¡¨ï¼ˆç”¨äºå»ºæ¨¡ï¼‰
    print("\n=== åˆå¹¶æ‰€æœ‰æ–‡ä»¶å®½è¡¨ï¼ˆç”¨äºå»ºæ¨¡ï¼‰===")
    all_wide_dfs = list(file_wide_tables.values())
    if len(all_wide_dfs) == 1:
        global_wide = all_wide_dfs[0].copy()
    else:
        global_wide = all_wide_dfs[0].copy()
        for df in all_wide_dfs[1:]:
            global_wide = pd.merge(global_wide, df, on='Id', how='outer')

    global_wide = calculate_derived_features(global_wide)

    global_output = os.path.join(output_dir, "ml_wide_table_global.csv")
    global_wide.to_csv(global_output, index=False, encoding='utf-8')

    global_dict = generate_feature_dictionary(global_wide, category_feature_mapping)
    global_dict.to_csv(os.path.join(output_dir, "feature_dictionary_global.csv"), index=False, encoding='utf-8')

    config_dir = "config"
    os.makedirs(config_dir, exist_ok=True)
    config_file = os.path.join(config_dir, "features.csv")
    global_dict_filtered = global_dict[global_dict['feature_type'] != 'text']
    global_dict_filtered.to_csv(config_file, index=False, encoding='utf-8')

    print(f"\nâœ… å…¨å±€å®½è¡¨å·²ä¿å­˜: {global_output}")
    print(f"âœ… å…¨å±€å­—æ®µå­—å…¸: {os.path.join(output_dir, 'feature_dictionary_global.csv')}")
    print(f"âœ… å»ºæ¨¡ç”¨ç‰¹å¾åˆ—è¡¨: {config_file}")
    print(f"\nğŸ“Š å…¨å±€å®½è¡¨æœ€ç»ˆå½¢çŠ¶: {global_wide.shape[0]} è¡Œ, {global_wide.shape[1]} åˆ—")

if __name__ == "__main__":
    main(coverage_threshold=0.95, max_top_k=50)